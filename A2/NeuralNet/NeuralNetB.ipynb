{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-4893ff20ac27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtraining_data_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtesting_data_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# training_data_path = \"../data/devnagri_train.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "training_data_path = sys.argv[1]\n",
    "testing_data_path = sys.argv[2]\n",
    "output_path = sys.argv[3]\n",
    "\n",
    "# training_data_path = \"../data/devnagri_train.csv\"\n",
    "# testing_data_path = \"../data/devnagri_test_public.csv\"\n",
    "# output_path = \"../data/nn/b/cs1160328\"\n",
    "\n",
    "batch_size = 128\n",
    "n0 = 5\n",
    "activation = 'sigmoid'\n",
    "hidden_layers_sizes = [100,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return (x>0) * x\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def reluPrime(x):\n",
    "    return (x>0)+0\n",
    "\n",
    "def tanhPrime(x):\n",
    "    return 1 - np.power(x,2)\n",
    "\n",
    "def sigmoidPrime(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def exp_normalize(x):\n",
    "    b = np.amax(x,axis=1,keepdims = True)\n",
    "    y = np.exp(x - b)\n",
    "    return y / y.sum(axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,input_size,output_size,hidden_layers_sizes, activation):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        if(activation == 'relu'):\n",
    "            self.activation = relu\n",
    "            self.activationPrime = reluPrime\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation = tanh\n",
    "            self.activationPrime = tanhPrime\n",
    "        else:\n",
    "            self.activation = sigmoid\n",
    "            self.activationPrime = sigmoidPrime\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hiddent_layers_sizes = hidden_layers_sizes\n",
    "        \n",
    "        prev_layer_count = input_size\n",
    "        \n",
    "        for i in range(len(hidden_layers_sizes) + 1):\n",
    "            if i==len(hidden_layers_sizes):\n",
    "                self.weights.append(np.random.rand(prev_layer_count, output_size)/100)\n",
    "                self.biases.append(np.random.rand(1, output_size)/100)        \n",
    "            else:\n",
    "                hidden_layer_count = hidden_layers_sizes[i]\n",
    "                self.weights.append(np.random.rand(prev_layer_count, hidden_layer_count)/100)\n",
    "                self.biases.append(np.random.rand(1, hidden_layer_count)/100)\n",
    "                prev_layer_count = hidden_layer_count\n",
    "        \n",
    "    def train(self,inpX,inpY,batch_size,n0,max_iterations):\n",
    "        max_examples = inpX.shape[0]\n",
    "        max_possible_iterations = int(0.5 + max_examples / batch_size)\n",
    "        num_hidden_layers = len(self.weights) - 1\n",
    "               \n",
    "        count = 0\n",
    "            \n",
    "        lr = n0\n",
    "        totLoss = 0\n",
    "        prevAvgLoss = sys.float_info.max\n",
    "        epoch = 0\n",
    "        \n",
    "        for n in range(max_iterations):\n",
    "            # Forming Mini Batches\n",
    "            i_eff = n%max_possible_iterations\n",
    "            \n",
    "            # Updating Learning Rate\n",
    "            if (i_eff == 0 and n!=0):\n",
    "                avgLoss = totLoss/max_possible_iterations\n",
    "                if(avgLoss >= prevAvgLoss):\n",
    "                    count += 1\n",
    "                    lr = n0 / np.sqrt(count+1)\n",
    "#                 print(\"Epoch = \",epoch,\" Average Loss = \",avgLoss,\" New Learning Rate = \",lr)\n",
    "                epoch += 1\n",
    "                prevAvgLoss = avgLoss\n",
    "                totLoss = 0\n",
    "            \n",
    "            outputs = []\n",
    "            \n",
    "            if i_eff != max_possible_iterations - 1:\n",
    "                X = inpX[i_eff*batch_size: (i_eff+1)*batch_size]\n",
    "                Y = inpY[i_eff*batch_size: (i_eff+1)*batch_size]\n",
    "            else:\n",
    "                X = inpX[i_eff*batch_size:]\n",
    "                Y = inpY[i_eff*batch_size:]\n",
    "                \n",
    "#             # Neural Network Forward Propagation (Cross Entropy)\n",
    "#             outputs.append(X)\n",
    "#             prev_layer_output = X\n",
    "#             for i in range(num_hidden_layers + 1):\n",
    "#                 weight = self.weights[i]\n",
    "#                 bias = self.biases[i]\n",
    "#                 if i == num_hidden_layers:\n",
    "#                     prev_layer_output = exp_normalize(prev_layer_output.dot(weight) + bias)\n",
    "#                 else:\n",
    "#                     prev_layer_output = self.activation(prev_layer_output.dot(weight) + bias)\n",
    "                    \n",
    "#                 outputs.append(prev_layer_output)\n",
    "            \n",
    "#             # Backpropagation\n",
    "#             dWs = []\n",
    "#             dbs = []\n",
    "            \n",
    "#             for i in range(num_hidden_layers + 1,0,-1):\n",
    "#                 if i == num_hidden_layers + 1:\n",
    "#                     delta = outputs[i].copy()\n",
    "#                     delta[range(Y.shape[0]),Y] -= 1\n",
    "#                 else:\n",
    "#                     delta = delta.dot(self.weights[i].T) * self.activationPrime(outputs[i])\n",
    "#                 dW = (outputs[i-1].T).dot(delta)\n",
    "#                 dWs.append(dW)\n",
    "#                 dbs.append(np.sum(delta,axis=0,keepdims=True))\n",
    "#             if (n%100 == 0):\n",
    "#                 loss_ = np.sum(-1*np.log(outputs[-1][range(Y.shape[0]),Y] + 0.001)) / Y.shape[0]\n",
    "#                 labels_ = np.argmax(outputs[-1],axis = 1)\n",
    "#                 accuracy_ = 100 * np.sum(labels_ == Y)/Y.shape[0]\n",
    "#                 print(\"Iteration \",n,\"\\tLoss = \",loss_,\"\\tAccuracy = \",accuracy_,\"%\")\n",
    "#             dWs.reverse()\n",
    "#             dbs.reverse()\n",
    "\n",
    "#             # Gradient Descent Parameter Update\n",
    "#             for i in range(len(dWs)):\n",
    "#                 self.weights[i] += dWs[i].dot(-1 * lr)\n",
    "#                 self.biases[i] += dbs[i].dot(-1 * lr)\n",
    "            \n",
    "#             loss = np.sum(-1*np.log(outputs[-1][range(Y.shape[0]),Y] + 0.001)) / Y.shape[0]\n",
    "#             totLoss += loss\n",
    "\n",
    "             # Neural Network Forward Propagation (MSE)\n",
    "            outputs.append(X)\n",
    "            prev_layer_output = X\n",
    "            for i in range(num_hidden_layers + 1):\n",
    "                weight = self.weights[i]\n",
    "                bias = self.biases[i]\n",
    "                if i == num_hidden_layers:\n",
    "                    prev_layer_output = sigmoid(prev_layer_output.dot(weight) + bias)\n",
    "                else:\n",
    "                    prev_layer_output = self.activation(prev_layer_output.dot(weight) + bias)\n",
    "                outputs.append(prev_layer_output)\n",
    "            \n",
    "            # Backpropagation\n",
    "            dWs = []\n",
    "            dbs = []\n",
    "            \n",
    "            y_onehot = np.zeros((Y.shape[0],self.output_size))\n",
    "            y_onehot[range(Y.shape[0]),Y] = 1\n",
    "            \n",
    "            for i in range(num_hidden_layers + 1,0,-1):\n",
    "                if i == num_hidden_layers + 1:\n",
    "                    delta = (outputs[i] - y_onehot).dot(2/Y.shape[0]) * sigmoidPrime(outputs[i])\n",
    "                else:\n",
    "                    delta = delta.dot(self.weights[i].T) * self.activationPrime(outputs[i])\n",
    "                dW = (outputs[i-1].T).dot(delta)\n",
    "                dWs.append(dW)\n",
    "                dbs.append(np.sum(delta,axis=0,keepdims=True))\n",
    "\n",
    "#             if (n%100 == 0):\n",
    "#                 loss = np.sum(np.power(outputs[-1] - y_onehot,2) )/Y.shape[0]\n",
    "#                 labels = np.argmax(outputs[-1],axis = 1)\n",
    "#                 accuracy = 100 * np.sum(labels == Y)/Y.shape[0]\n",
    "#                 print(\"Iteration \",n,\"\\tLoss = \",loss,\"\\tAccuracy = \",accuracy,\"%\")\n",
    "                \n",
    "            dWs.reverse()\n",
    "            dbs.reverse()\n",
    "\n",
    "            # Gradient Descent Parameter Update\n",
    "            for i in range(len(dWs)):\n",
    "                self.weights[i] += dWs[i].dot(-1 * lr)\n",
    "                self.biases[i] += dbs[i].dot(-1 * lr)\n",
    "\n",
    "            loss = np.sum(np.power(outputs[-1] - y_onehot,2) )/Y.shape[0]\n",
    "            totLoss += loss\n",
    "            \n",
    "    def predict(self,X):\n",
    "        return self.forward_run(X)\n",
    "        \n",
    "    def forward_run(self,X):\n",
    "        prev_layer_output = X\n",
    "        num_hidden_layers = len(self.weights) - 1\n",
    "        for i in range(num_hidden_layers + 1):\n",
    "            weight = self.weights[i]\n",
    "            bias = self.biases[i]\n",
    "            if i == num_hidden_layers:\n",
    "                probabilities = exp_normalize(prev_layer_output.dot(weight) + bias)\n",
    "                labels = np.argmax(probabilities,axis = 1)\n",
    "                return labels\n",
    "            else:\n",
    "                prev_layer_output = self.activation(prev_layer_output.dot(weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,avg,std):\n",
    "    if avg is None:\n",
    "        input_data = np.loadtxt(open(path, \"rb\"), delimiter=\",\")\n",
    "        Y = input_data[:,0].copy()\n",
    "        X = input_data[:,1:].copy()\n",
    "        avg = np.average(X,axis=0)\n",
    "        X = X - avg\n",
    "        std = np.std(X,axis=0)\n",
    "        std[(std == 0)] = 1\n",
    "        X = X / std\n",
    "        return X,Y,avg,std\n",
    "    else:\n",
    "        input_data = np.loadtxt(open(path, \"rb\"), delimiter=\",\")\n",
    "        X = input_data[:,1:].copy()\n",
    "        X = (X - avg)/std\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpX,Y,avg,std = load_data(training_data_path,None,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 \tLoss =  3.782283867361179 \tAccuracy =  2.34375 %\n",
      "Iteration  100 \tLoss =  6.745831338841937 \tAccuracy =  2.34375 %\n",
      "Iteration  200 \tLoss =  6.600449216009279 \tAccuracy =  0.78125 %\n",
      "Iteration  300 \tLoss =  6.711249824247213 \tAccuracy =  2.34375 %\n",
      "Iteration  400 \tLoss =  6.848222235081371 \tAccuracy =  0.78125 %\n",
      "Iteration  500 \tLoss =  6.745831338841937 \tAccuracy =  2.34375 %\n",
      "Iteration  600 \tLoss =  6.691832870320664 \tAccuracy =  3.125 %\n",
      "Epoch =  0  Average Loss =  6.717183552379846  New Learning Rate =  5\n",
      "Iteration  700 \tLoss =  6.574674708045514 \tAccuracy =  2.34375 %\n",
      "Iteration  800 \tLoss =  6.7310345928550674 \tAccuracy =  2.34375 %\n",
      "Iteration  900 \tLoss =  6.807452713611032 \tAccuracy =  0.78125 %\n",
      "Iteration  1000 \tLoss =  6.799805985538095 \tAccuracy =  1.5625 %\n",
      "Iteration  1100 \tLoss =  6.799805973152872 \tAccuracy =  1.5625 %\n",
      "Iteration  1200 \tLoss =  6.853780632268569 \tAccuracy =  0.78125 %\n",
      "Epoch =  1  Average Loss =  6.726600477952544  New Learning Rate =  3.5355339059327373\n",
      "Iteration  1300 \tLoss =  6.689367217285625 \tAccuracy =  2.34375 %\n",
      "Iteration  1400 \tLoss =  6.761044138793903 \tAccuracy =  1.5625 %\n",
      "Iteration  1500 \tLoss =  6.7998059855366995 \tAccuracy =  1.5625 %\n",
      "Iteration  1600 \tLoss =  6.799805451416267 \tAccuracy =  1.5625 %\n",
      "Iteration  1700 \tLoss =  6.74583133828571 \tAccuracy =  2.34375 %\n",
      "Iteration  1800 \tLoss =  6.407560513014081 \tAccuracy =  3.125 %\n",
      "Epoch =  2  Average Loss =  6.697466611063046  New Learning Rate =  3.5355339059327373\n",
      "Iteration  1900 \tLoss =  6.745685734127173 \tAccuracy =  2.34375 %\n",
      "Iteration  2000 \tLoss =  6.7265170867195785 \tAccuracy =  0.78125 %\n",
      "Iteration  2100 \tLoss =  6.6901941609382565 \tAccuracy =  3.125 %\n",
      "Iteration  2200 \tLoss =  6.853780488711548 \tAccuracy =  0.78125 %\n",
      "Iteration  2300 \tLoss =  6.56800324818889 \tAccuracy =  1.5625 %\n",
      "Iteration  2400 \tLoss =  6.621284522451106 \tAccuracy =  1.5625 %\n",
      "Epoch =  3  Average Loss =  6.697154934674613  New Learning Rate =  3.5355339059327373\n",
      "Iteration  2500 \tLoss =  6.8535392364890715 \tAccuracy =  0.78125 %\n",
      "Iteration  2600 \tLoss =  6.530162284224964 \tAccuracy =  3.125 %\n",
      "Iteration  2700 \tLoss =  6.6660482395297755 \tAccuracy =  2.34375 %\n",
      "Iteration  2800 \tLoss =  6.799801941398556 \tAccuracy =  1.5625 %\n",
      "Iteration  2900 \tLoss =  6.6675054779372624 \tAccuracy =  1.5625 %\n",
      "Iteration  3000 \tLoss =  6.588750956863025 \tAccuracy =  3.125 %\n",
      "Epoch =  4  Average Loss =  6.709738849307733  New Learning Rate =  2.886751345948129\n",
      "Iteration  3100 \tLoss =  6.73424998029239 \tAccuracy =  2.34375 %\n",
      "Iteration  3200 \tLoss =  6.689518567357177 \tAccuracy =  0.0 %\n",
      "Iteration  3300 \tLoss =  6.691856681321431 \tAccuracy =  3.125 %\n",
      "Iteration  3400 \tLoss =  6.514533256048472 \tAccuracy =  3.125 %\n",
      "Iteration  3500 \tLoss =  6.745824021122973 \tAccuracy =  2.34375 %\n",
      "Iteration  3600 \tLoss =  6.765150325879803 \tAccuracy =  0.78125 %\n",
      "Epoch =  5  Average Loss =  6.669279131734275  New Learning Rate =  2.886751345948129\n",
      "Iteration  3700 \tLoss =  6.799800284456323 \tAccuracy =  1.5625 %\n",
      "Iteration  3800 \tLoss =  6.6056467397283445 \tAccuracy =  1.5625 %\n",
      "Iteration  3900 \tLoss =  6.529932751976372 \tAccuracy =  5.46875 %\n",
      "Iteration  4000 \tLoss =  6.799761295876655 \tAccuracy =  1.5625 %\n",
      "Iteration  4100 \tLoss =  6.722649341955915 \tAccuracy =  1.5625 %\n",
      "Iteration  4200 \tLoss =  6.800564865013573 \tAccuracy =  0.0 %\n",
      "Epoch =  6  Average Loss =  6.685019600099481  New Learning Rate =  2.5\n",
      "Iteration  4300 \tLoss =  6.710105295828855 \tAccuracy =  2.34375 %\n",
      "Iteration  4400 \tLoss =  6.558561991048936 \tAccuracy =  3.125 %\n",
      "Iteration  4500 \tLoss =  6.681181024568567 \tAccuracy =  3.125 %\n",
      "Iteration  4600 \tLoss =  6.680547264276112 \tAccuracy =  0.0 %\n",
      "Iteration  4700 \tLoss =  6.573822976814788 \tAccuracy =  4.6875 %\n",
      "Iteration  4800 \tLoss =  6.745830128086145 \tAccuracy =  2.34375 %\n",
      "Epoch =  7  Average Loss =  6.655060705234616  New Learning Rate =  2.5\n",
      "Iteration  4900 \tLoss =  6.4966775787450874 \tAccuracy =  2.34375 %\n",
      "Iteration  5000 \tLoss =  6.6402714327675 \tAccuracy =  3.90625 %\n",
      "Iteration  5100 \tLoss =  6.650095046425573 \tAccuracy =  2.34375 %\n",
      "Iteration  5200 \tLoss =  6.681271429357187 \tAccuracy =  2.34375 %\n",
      "Iteration  5300 \tLoss =  6.69172397618971 \tAccuracy =  3.125 %\n",
      "Iteration  5400 \tLoss =  6.6087739156735035 \tAccuracy =  2.34375 %\n",
      "Epoch =  8  Average Loss =  6.658814153030803  New Learning Rate =  2.23606797749979\n",
      "Iteration  5500 \tLoss =  6.546700962086305 \tAccuracy =  2.34375 %\n",
      "Iteration  5600 \tLoss =  6.657100273251581 \tAccuracy =  1.5625 %\n",
      "Iteration  5700 \tLoss =  6.694439674098945 \tAccuracy =  2.34375 %\n",
      "Iteration  5800 \tLoss =  6.476967989765823 \tAccuracy =  0.78125 %\n",
      "Iteration  5900 \tLoss =  6.823983483036236 \tAccuracy =  0.78125 %\n",
      "Iteration  6000 \tLoss =  6.563315390996922 \tAccuracy =  2.34375 %\n",
      "Iteration  6100 \tLoss =  6.660081442308879 \tAccuracy =  3.125 %\n",
      "Epoch =  9  Average Loss =  6.6252780115180885  New Learning Rate =  2.23606797749979\n",
      "Iteration  6200 \tLoss =  6.780561782233737 \tAccuracy =  1.5625 %\n",
      "Iteration  6300 \tLoss =  6.820449515510319 \tAccuracy =  0.78125 %\n",
      "Iteration  6400 \tLoss =  6.744999011137388 \tAccuracy =  2.34375 %\n",
      "Iteration  6500 \tLoss =  6.451628325111486 \tAccuracy =  1.5625 %\n",
      "Iteration  6600 \tLoss =  6.551792324200646 \tAccuracy =  3.90625 %\n",
      "Iteration  6700 \tLoss =  6.7042259982103545 \tAccuracy =  0.0 %\n",
      "Epoch =  10  Average Loss =  6.627056316302915  New Learning Rate =  2.041241452319315\n",
      "Iteration  6800 \tLoss =  6.424090939298762 \tAccuracy =  3.125 %\n",
      "Iteration  6900 \tLoss =  6.4657834313951374 \tAccuracy =  3.125 %\n",
      "Iteration  7000 \tLoss =  6.5624685545463795 \tAccuracy =  3.125 %\n",
      "Iteration  7100 \tLoss =  6.746566497353367 \tAccuracy =  1.5625 %\n",
      "Iteration  7200 \tLoss =  6.70687637898844 \tAccuracy =  2.34375 %\n",
      "Iteration  7300 \tLoss =  6.477691298053873 \tAccuracy =  3.125 %\n",
      "Epoch =  11  Average Loss =  6.603577184126726  New Learning Rate =  2.041241452319315\n",
      "Iteration  7400 \tLoss =  6.459499794825536 \tAccuracy =  1.5625 %\n",
      "Iteration  7500 \tLoss =  6.478014935363974 \tAccuracy =  1.5625 %\n",
      "Iteration  7600 \tLoss =  6.748722318861157 \tAccuracy =  0.78125 %\n",
      "Iteration  7700 \tLoss =  6.311140313790794 \tAccuracy =  0.78125 %\n",
      "Iteration  7800 \tLoss =  6.595286763539372 \tAccuracy =  1.5625 %\n",
      "Iteration  7900 \tLoss =  6.4514325867142235 \tAccuracy =  1.5625 %\n",
      "Epoch =  12  Average Loss =  6.601512951773323  New Learning Rate =  2.041241452319315\n",
      "Iteration  8000 \tLoss =  6.852774224587446 \tAccuracy =  0.78125 %\n",
      "Iteration  8100 \tLoss =  6.8487478811426605 \tAccuracy =  0.78125 %\n",
      "Iteration  8200 \tLoss =  6.499005419366824 \tAccuracy =  0.78125 %\n",
      "Iteration  8300 \tLoss =  6.3886912467150365 \tAccuracy =  3.125 %\n",
      "Iteration  8400 \tLoss =  6.59376983962299 \tAccuracy =  2.34375 %\n",
      "Iteration  8500 \tLoss =  6.5469924236890025 \tAccuracy =  2.34375 %\n",
      "Epoch =  13  Average Loss =  6.607027040009889  New Learning Rate =  1.889822365046136\n",
      "Iteration  8600 \tLoss =  6.618233979255016 \tAccuracy =  1.5625 %\n",
      "Iteration  8700 \tLoss =  6.341126815626851 \tAccuracy =  2.34375 %\n",
      "Iteration  8800 \tLoss =  6.557582234768236 \tAccuracy =  3.90625 %\n",
      "Iteration  8900 \tLoss =  6.637877182451927 \tAccuracy =  3.90625 %\n",
      "Iteration  9000 \tLoss =  6.797218942966971 \tAccuracy =  0.78125 %\n",
      "Iteration  9100 \tLoss =  6.743758874930477 \tAccuracy =  1.5625 %\n",
      "Epoch =  14  Average Loss =  6.5934920582243395  New Learning Rate =  1.889822365046136\n",
      "Iteration  9200 \tLoss =  6.744384407657318 \tAccuracy =  2.34375 %\n",
      "Iteration  9300 \tLoss =  6.6725388540090975 \tAccuracy =  3.125 %\n",
      "Iteration  9400 \tLoss =  6.6443662790853075 \tAccuracy =  1.5625 %\n",
      "Iteration  9500 \tLoss =  6.670554385640643 \tAccuracy =  3.125 %\n",
      "Iteration  9600 \tLoss =  6.581056952822367 \tAccuracy =  2.34375 %\n",
      "Iteration  9700 \tLoss =  6.642786733797227 \tAccuracy =  3.125 %\n",
      "Epoch =  15  Average Loss =  6.586643062249398  New Learning Rate =  1.889822365046136\n",
      "Iteration  9800 \tLoss =  6.883677463057403 \tAccuracy =  0.0 %\n",
      "Iteration  9900 \tLoss =  6.774757865751359 \tAccuracy =  1.5625 %\n",
      "Iteration  10000 \tLoss =  6.722218348085367 \tAccuracy =  1.5625 %\n",
      "Iteration  10100 \tLoss =  6.676412435044767 \tAccuracy =  3.125 %\n",
      "Iteration  10200 \tLoss =  6.794967731463537 \tAccuracy =  1.5625 %\n",
      "Iteration  10300 \tLoss =  6.437478034635396 \tAccuracy =  0.78125 %\n",
      "Epoch =  16  Average Loss =  6.58652973876837  New Learning Rate =  1.889822365046136\n",
      "Iteration  10400 \tLoss =  6.710062901765586 \tAccuracy =  0.78125 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  10500 \tLoss =  6.5336838457943 \tAccuracy =  3.125 %\n",
      "Iteration  10600 \tLoss =  6.303698370320086 \tAccuracy =  6.25 %\n",
      "Iteration  10700 \tLoss =  6.59125779199172 \tAccuracy =  3.90625 %\n",
      "Iteration  10800 \tLoss =  6.598391126089053 \tAccuracy =  1.5625 %\n",
      "Iteration  10900 \tLoss =  6.354071421847509 \tAccuracy =  1.5625 %\n",
      "Epoch =  17  Average Loss =  6.585468207314915  New Learning Rate =  1.889822365046136\n",
      "Iteration  11000 \tLoss =  6.690405741679525 \tAccuracy =  3.125 %\n",
      "Iteration  11100 \tLoss =  6.522232802547682 \tAccuracy =  1.5625 %\n",
      "Iteration  11200 \tLoss =  6.864131440199692 \tAccuracy =  0.0 %\n",
      "Iteration  11300 \tLoss =  6.728683618996761 \tAccuracy =  1.5625 %\n",
      "Iteration  11400 \tLoss =  6.737519165618606 \tAccuracy =  1.5625 %\n",
      "Iteration  11500 \tLoss =  6.695849172365364 \tAccuracy =  2.34375 %\n",
      "Iteration  11600 \tLoss =  6.343091710239351 \tAccuracy =  3.125 %\n",
      "Epoch =  18  Average Loss =  6.584652064074546  New Learning Rate =  1.889822365046136\n",
      "Iteration  11700 \tLoss =  6.674040005264967 \tAccuracy =  3.125 %\n",
      "Iteration  11800 \tLoss =  6.262560217655961 \tAccuracy =  3.125 %\n",
      "Iteration  11900 \tLoss =  6.672299412366693 \tAccuracy =  1.5625 %\n",
      "Iteration  12000 \tLoss =  6.600692230426958 \tAccuracy =  1.5625 %\n",
      "Iteration  12100 \tLoss =  6.459588060446112 \tAccuracy =  3.125 %\n",
      "Iteration  12200 \tLoss =  6.36052337694574 \tAccuracy =  3.125 %\n",
      "Epoch =  19  Average Loss =  6.590471803123377  New Learning Rate =  1.7677669529663687\n",
      "Iteration  12300 \tLoss =  6.389084744461686 \tAccuracy =  3.90625 %\n",
      "Iteration  12400 \tLoss =  6.6484848374071674 \tAccuracy =  1.5625 %\n",
      "Iteration  12500 \tLoss =  6.6827539701814995 \tAccuracy =  1.5625 %\n",
      "Iteration  12600 \tLoss =  6.708551726078372 \tAccuracy =  0.78125 %\n",
      "Iteration  12700 \tLoss =  6.72408384232426 \tAccuracy =  1.5625 %\n",
      "Iteration  12800 \tLoss =  6.58781231884899 \tAccuracy =  0.78125 %\n",
      "Epoch =  20  Average Loss =  6.577703223458163  New Learning Rate =  1.7677669529663687\n",
      "Iteration  12900 \tLoss =  6.716456525303428 \tAccuracy =  1.5625 %\n",
      "Iteration  13000 \tLoss =  6.430588569815151 \tAccuracy =  3.90625 %\n",
      "Iteration  13100 \tLoss =  6.654706350475582 \tAccuracy =  0.78125 %\n",
      "Iteration  13200 \tLoss =  6.767193058411722 \tAccuracy =  1.5625 %\n",
      "Iteration  13300 \tLoss =  6.6695670290760685 \tAccuracy =  1.5625 %\n",
      "Iteration  13400 \tLoss =  6.474579197430872 \tAccuracy =  0.78125 %\n",
      "Epoch =  21  Average Loss =  6.5611901402946  New Learning Rate =  1.7677669529663687\n",
      "Iteration  13500 \tLoss =  6.853748099960011 \tAccuracy =  0.78125 %\n",
      "Iteration  13600 \tLoss =  6.7067709877186275 \tAccuracy =  0.0 %\n",
      "Iteration  13700 \tLoss =  6.635430759066586 \tAccuracy =  1.5625 %\n",
      "Iteration  13800 \tLoss =  6.664522184983521 \tAccuracy =  1.5625 %\n",
      "Iteration  13900 \tLoss =  6.5730778583117235 \tAccuracy =  0.78125 %\n",
      "Iteration  14000 \tLoss =  6.074431696348039 \tAccuracy =  1.5625 %\n",
      "Epoch =  22  Average Loss =  6.549757523570609  New Learning Rate =  1.7677669529663687\n",
      "Iteration  14100 \tLoss =  6.575856184722381 \tAccuracy =  1.5625 %\n",
      "Iteration  14200 \tLoss =  6.5944639133590215 \tAccuracy =  0.78125 %\n",
      "Iteration  14300 \tLoss =  6.463915033523194 \tAccuracy =  3.90625 %\n",
      "Iteration  14400 \tLoss =  6.529159557646903 \tAccuracy =  2.34375 %\n",
      "Iteration  14500 \tLoss =  6.61877870787133 \tAccuracy =  3.125 %\n",
      "Iteration  14600 \tLoss =  6.777397689497267 \tAccuracy =  0.78125 %\n",
      "Epoch =  23  Average Loss =  6.5625550021264845  New Learning Rate =  1.6666666666666667\n",
      "Iteration  14700 \tLoss =  6.661264994488178 \tAccuracy =  1.5625 %\n",
      "Iteration  14800 \tLoss =  6.521587084989891 \tAccuracy =  2.34375 %\n",
      "Iteration  14900 \tLoss =  6.462974078765525 \tAccuracy =  0.78125 %\n",
      "Iteration  15000 \tLoss =  6.550829416271582 \tAccuracy =  2.34375 %\n",
      "Iteration  15100 \tLoss =  6.694720854725506 \tAccuracy =  2.34375 %\n",
      "Iteration  15200 \tLoss =  6.253941728500095 \tAccuracy =  1.5625 %\n",
      "Epoch =  24  Average Loss =  6.541193938349758  New Learning Rate =  1.6666666666666667\n",
      "Iteration  15300 \tLoss =  6.57295994108946 \tAccuracy =  2.34375 %\n",
      "Iteration  15400 \tLoss =  6.463930031388863 \tAccuracy =  1.5625 %\n",
      "Iteration  15500 \tLoss =  6.718212895295636 \tAccuracy =  0.78125 %\n",
      "Iteration  15600 \tLoss =  6.53336784474506 \tAccuracy =  2.34375 %\n",
      "Iteration  15700 \tLoss =  6.602025696744841 \tAccuracy =  3.125 %\n",
      "Iteration  15800 \tLoss =  6.608162031738469 \tAccuracy =  0.78125 %\n",
      "Epoch =  25  Average Loss =  6.517505885414801  New Learning Rate =  1.6666666666666667\n",
      "Iteration  15900 \tLoss =  6.53694474509656 \tAccuracy =  3.90625 %\n",
      "Iteration  16000 \tLoss =  6.55326498570468 \tAccuracy =  0.78125 %\n",
      "Iteration  16100 \tLoss =  6.901874331672635 \tAccuracy =  0.0 %\n",
      "Iteration  16200 \tLoss =  6.429540259316571 \tAccuracy =  3.125 %\n",
      "Iteration  16300 \tLoss =  6.709879021404564 \tAccuracy =  2.34375 %\n",
      "Iteration  16400 \tLoss =  6.411889292552377 \tAccuracy =  4.6875 %\n",
      "Epoch =  26  Average Loss =  6.549524629664674  New Learning Rate =  1.5811388300841895\n",
      "Iteration  16500 \tLoss =  6.670131224833275 \tAccuracy =  1.5625 %\n",
      "Iteration  16600 \tLoss =  6.654176238261099 \tAccuracy =  0.78125 %\n",
      "Iteration  16700 \tLoss =  6.3525461893290185 \tAccuracy =  3.90625 %\n",
      "Iteration  16800 \tLoss =  6.841957539395928 \tAccuracy =  0.78125 %\n",
      "Iteration  16900 \tLoss =  6.358740974421629 \tAccuracy =  5.46875 %\n",
      "Iteration  17000 \tLoss =  6.465058142001283 \tAccuracy =  1.5625 %\n",
      "Iteration  17100 \tLoss =  6.577769584015556 \tAccuracy =  3.125 %\n",
      "Epoch =  27  Average Loss =  6.477540615276632  New Learning Rate =  1.5811388300841895\n",
      "Iteration  17200 \tLoss =  6.426779988161194 \tAccuracy =  1.5625 %\n",
      "Iteration  17300 \tLoss =  6.122186840494023 \tAccuracy =  2.34375 %\n",
      "Iteration  17400 \tLoss =  6.34041079231347 \tAccuracy =  3.90625 %\n",
      "Iteration  17500 \tLoss =  6.419008205359395 \tAccuracy =  1.5625 %\n",
      "Iteration  17600 \tLoss =  6.377838954544715 \tAccuracy =  1.5625 %\n",
      "Iteration  17700 \tLoss =  6.679018573653355 \tAccuracy =  0.78125 %\n",
      "Epoch =  28  Average Loss =  6.490659263232472  New Learning Rate =  1.507556722888818\n",
      "Iteration  17800 \tLoss =  6.517971982157589 \tAccuracy =  2.34375 %\n",
      "Iteration  17900 \tLoss =  6.40846006045047 \tAccuracy =  3.125 %\n",
      "Iteration  18000 \tLoss =  6.548152354498231 \tAccuracy =  1.5625 %\n",
      "Iteration  18100 \tLoss =  6.701295052865918 \tAccuracy =  1.5625 %\n",
      "Iteration  18200 \tLoss =  6.177692274229344 \tAccuracy =  3.125 %\n",
      "Iteration  18300 \tLoss =  6.6454991675917725 \tAccuracy =  2.34375 %\n",
      "Epoch =  29  Average Loss =  6.480364008514038  New Learning Rate =  1.507556722888818\n",
      "Iteration  18400 \tLoss =  6.329344577845511 \tAccuracy =  3.90625 %\n",
      "Iteration  18500 \tLoss =  6.578413734715973 \tAccuracy =  3.125 %\n",
      "Iteration  18600 \tLoss =  6.685521790817861 \tAccuracy =  2.34375 %\n",
      "Iteration  18700 \tLoss =  6.40256223758068 \tAccuracy =  2.34375 %\n",
      "Iteration  18800 \tLoss =  6.58385446623768 \tAccuracy =  1.5625 %\n",
      "Iteration  18900 \tLoss =  6.768812263216839 \tAccuracy =  0.78125 %\n",
      "Epoch =  30  Average Loss =  6.470488391139135  New Learning Rate =  1.507556722888818\n",
      "Iteration  19000 \tLoss =  6.310994462234014 \tAccuracy =  2.34375 %\n",
      "Iteration  19100 \tLoss =  6.494154529577926 \tAccuracy =  1.5625 %\n",
      "Iteration  19200 \tLoss =  6.524722238419999 \tAccuracy =  0.78125 %\n",
      "Iteration  19300 \tLoss =  6.636154751828707 \tAccuracy =  1.5625 %\n",
      "Iteration  19400 \tLoss =  6.62954237515604 \tAccuracy =  0.78125 %\n",
      "Iteration  19500 \tLoss =  6.576497031157164 \tAccuracy =  1.5625 %\n",
      "Epoch =  31  Average Loss =  6.458547215439033  New Learning Rate =  1.507556722888818\n",
      "Iteration  19600 \tLoss =  6.493079017942856 \tAccuracy =  3.90625 %\n",
      "Iteration  19700 \tLoss =  6.485964092410534 \tAccuracy =  2.34375 %\n",
      "Iteration  19800 \tLoss =  6.54344794461866 \tAccuracy =  1.5625 %\n",
      "Iteration  19900 \tLoss =  6.796409711084321 \tAccuracy =  1.5625 %\n",
      "Iteration  20000 \tLoss =  6.511542020047344 \tAccuracy =  1.5625 %\n",
      "Iteration  20100 \tLoss =  6.033001626643829 \tAccuracy =  5.46875 %\n",
      "Epoch =  32  Average Loss =  6.497637706504311  New Learning Rate =  1.4433756729740645\n",
      "Iteration  20200 \tLoss =  6.38109392146232 \tAccuracy =  2.34375 %\n",
      "Iteration  20300 \tLoss =  6.313145082756353 \tAccuracy =  3.90625 %\n",
      "Iteration  20400 \tLoss =  6.47424651964425 \tAccuracy =  2.34375 %\n",
      "Iteration  20500 \tLoss =  6.441692477522049 \tAccuracy =  1.5625 %\n",
      "Iteration  20600 \tLoss =  6.259183024588498 \tAccuracy =  1.5625 %\n",
      "Iteration  20700 \tLoss =  6.290013405128295 \tAccuracy =  3.125 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  33  Average Loss =  6.44691413667335  New Learning Rate =  1.4433756729740645\n",
      "Iteration  20800 \tLoss =  6.272380887584987 \tAccuracy =  2.34375 %\n",
      "Iteration  20900 \tLoss =  6.597950214612192 \tAccuracy =  1.5625 %\n",
      "Iteration  21000 \tLoss =  6.458779471254484 \tAccuracy =  3.125 %\n",
      "Iteration  21100 \tLoss =  6.63868699721376 \tAccuracy =  1.5625 %\n",
      "Iteration  21200 \tLoss =  6.244568606197561 \tAccuracy =  3.125 %\n",
      "Iteration  21300 \tLoss =  6.241833190893663 \tAccuracy =  2.34375 %\n",
      "Epoch =  34  Average Loss =  6.4388410881393  New Learning Rate =  1.4433756729740645\n",
      "Iteration  21400 \tLoss =  6.3766122218349945 \tAccuracy =  0.0 %\n",
      "Iteration  21500 \tLoss =  6.654619333329661 \tAccuracy =  1.5625 %\n",
      "Iteration  21600 \tLoss =  6.753868159582628 \tAccuracy =  0.0 %\n",
      "Iteration  21700 \tLoss =  6.542843893995697 \tAccuracy =  0.78125 %\n",
      "Iteration  21800 \tLoss =  5.907336982532781 \tAccuracy =  2.34375 %\n",
      "Iteration  21900 \tLoss =  5.974303119315447 \tAccuracy =  5.46875 %\n",
      "Epoch =  35  Average Loss =  6.443116035386005  New Learning Rate =  1.386750490563073\n",
      "Iteration  22000 \tLoss =  6.273355752351673 \tAccuracy =  0.78125 %\n",
      "Iteration  22100 \tLoss =  6.479761643554875 \tAccuracy =  0.78125 %\n",
      "Iteration  22200 \tLoss =  6.145888764085905 \tAccuracy =  2.34375 %\n",
      "Iteration  22300 \tLoss =  5.990341445437246 \tAccuracy =  1.5625 %\n",
      "Iteration  22400 \tLoss =  6.371229070351181 \tAccuracy =  1.5625 %\n",
      "Iteration  22500 \tLoss =  6.292207050740794 \tAccuracy =  1.5625 %\n",
      "Iteration  22600 \tLoss =  6.229099478401219 \tAccuracy =  1.5625 %\n",
      "Epoch =  36  Average Loss =  6.4243184039895445  New Learning Rate =  1.386750490563073\n",
      "Iteration  22700 \tLoss =  6.451113803675389 \tAccuracy =  1.5625 %\n",
      "Iteration  22800 \tLoss =  6.451820146261155 \tAccuracy =  0.78125 %\n",
      "Iteration  22900 \tLoss =  6.059890078273547 \tAccuracy =  3.90625 %\n",
      "Iteration  23000 \tLoss =  6.620229297732475 \tAccuracy =  0.78125 %\n",
      "Iteration  23100 \tLoss =  6.572825691970483 \tAccuracy =  3.125 %\n",
      "Iteration  23200 \tLoss =  6.182432077048965 \tAccuracy =  0.78125 %\n",
      "Epoch =  37  Average Loss =  6.4278036090709625  New Learning Rate =  1.3363062095621219\n",
      "Iteration  23300 \tLoss =  6.45696543350898 \tAccuracy =  1.5625 %\n",
      "Iteration  23400 \tLoss =  6.5535368381040255 \tAccuracy =  1.5625 %\n",
      "Iteration  23500 \tLoss =  6.234875478677193 \tAccuracy =  2.34375 %\n",
      "Iteration  23600 \tLoss =  6.564444134657284 \tAccuracy =  3.125 %\n",
      "Iteration  23700 \tLoss =  6.696652233664104 \tAccuracy =  2.34375 %\n",
      "Iteration  23800 \tLoss =  6.234443750550231 \tAccuracy =  3.125 %\n",
      "Epoch =  38  Average Loss =  6.383198596905627  New Learning Rate =  1.3363062095621219\n",
      "Iteration  23900 \tLoss =  6.203500066140862 \tAccuracy =  1.5625 %\n",
      "Iteration  24000 \tLoss =  6.486277479361594 \tAccuracy =  2.34375 %\n",
      "Iteration  24100 \tLoss =  6.707931549943544 \tAccuracy =  0.0 %\n",
      "Iteration  24200 \tLoss =  6.124740329128617 \tAccuracy =  2.34375 %\n",
      "Iteration  24300 \tLoss =  6.625081150296016 \tAccuracy =  1.5625 %\n",
      "Iteration  24400 \tLoss =  6.134167833332272 \tAccuracy =  2.34375 %\n"
     ]
    }
   ],
   "source": [
    "X = inpX.copy()\n",
    "\n",
    "input_size = X.shape[1]\n",
    "output_size = int(np.amax(Y))+1\n",
    "num_examples = X.shape[0]\n",
    "max_iterations = int(40*(num_examples/batch_size))\n",
    "\n",
    "network = NeuralNetwork(input_size,output_size,hidden_layers_sizes,activation)\n",
    "network.train(X,Y.astype(int),batch_size,n0,max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.79667519181585\n"
     ]
    }
   ],
   "source": [
    "# predictions = network.predict(X.copy())\n",
    "# print(100 * np.sum(predictions == Y)/Y.shape[0])\n",
    "# print(np.average(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = load_data(testing_data_path,avg,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = network.predict(testX)\n",
    "np.savetxt(output_path,predictions,fmt=\"%i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
