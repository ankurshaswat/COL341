{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "# training_data_path = sys.argv[1]\n",
    "# testing_data_path = sys.argv[2]\n",
    "# output_path = sys.argv[3]\n",
    "# batch_size = int(sys.argv[4])\n",
    "# activation = sys.argv[5]\n",
    "# hidden_layers_sizes = []\n",
    "# for i in range(6,len(sys.argv)):\n",
    "#     hidden_layers_sizes.append(int(sys.argv[i]))\n",
    "    \n",
    "training_data_path = \"../data/devnagri_train.csv\"\n",
    "testing_data_path = \"../data/devnagri_test_public.csv\"\n",
    "output_path = \"../data/nn/b/cs1160328.txt\"\n",
    "batch_size = 128\n",
    "activation = 'relu'\n",
    "hidden_layers_sizes = [200]\n",
    "\n",
    "n0 = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return (x>0) * x\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def reluPrime(x):\n",
    "    return (x>0)+0\n",
    "\n",
    "def tanhPrime(x):\n",
    "    return 1 - np.power(x,2)\n",
    "\n",
    "def sigmoidPrime(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def exp_normalize(x):\n",
    "    b = np.amax(x,axis=1,keepdims = True)\n",
    "    y = np.exp(x - b)\n",
    "    return y / y.sum(axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,input_size,output_size,hidden_layers_sizes, activation):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        if(activation == 'relu'):\n",
    "            self.activation = relu\n",
    "            self.activationPrime = reluPrime\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation = tanh\n",
    "            self.activationPrime = tanhPrime\n",
    "        else:\n",
    "            self.activation = sigmoid\n",
    "            self.activationPrime = sigmoidPrime\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hiddent_layers_sizes = hidden_layers_sizes\n",
    "        \n",
    "        prev_layer_count = input_size\n",
    "        \n",
    "        for i in range(len(hidden_layers_sizes) + 1):\n",
    "            if i==len(hidden_layers_sizes):\n",
    "                self.weights.append(np.random.rand(prev_layer_count, output_size)/100)\n",
    "                self.biases.append(np.random.rand(1, output_size)/100)        \n",
    "            else:\n",
    "                hidden_layer_count = hidden_layers_sizes[i]\n",
    "                self.weights.append(np.random.rand(prev_layer_count, hidden_layer_count)/100)\n",
    "                self.biases.append(np.random.rand(1, hidden_layer_count)/100)\n",
    "                prev_layer_count = hidden_layer_count\n",
    "        \n",
    "    def train(self,inpX,inpY,batch_size,n0,max_iterations):\n",
    "        max_examples = inpX.shape[0]\n",
    "        max_possible_iterations = int(0.5 + max_examples / batch_size)\n",
    "        num_hidden_layers = len(self.weights) - 1\n",
    "        \n",
    "        for n in range(max_iterations):\n",
    "            # Forming Mini Batches\n",
    "            i_eff = n%max_possible_iterations\n",
    "            \n",
    "            outputs = []\n",
    "            \n",
    "            if i_eff != max_possible_iterations - 1:\n",
    "                X = inpX[i_eff*batch_size: (i_eff+1)*batch_size]\n",
    "                Y = inpY[i_eff*batch_size: (i_eff+1)*batch_size]\n",
    "            else:\n",
    "                X = inpX[i_eff*batch_size:]\n",
    "                Y = inpY[i_eff*batch_size:]\n",
    "            \n",
    "            # Updating Learning Rate\n",
    "            lr = n0 / np.sqrt(n+1) \n",
    "                \n",
    "            # Neural Network Forward Propagation\n",
    "            outputs.append(X)\n",
    "            prev_layer_output = X\n",
    "            for i in range(num_hidden_layers + 1):\n",
    "                weight = self.weights[i]\n",
    "                bias = self.biases[i]\n",
    "                if i == num_hidden_layers:\n",
    "                    prev_layer_output = exp_normalize(prev_layer_output.dot(weight) + bias)\n",
    "                else:\n",
    "                    prev_layer_output = self.activation(prev_layer_output.dot(weight) + bias)\n",
    "                outputs.append(prev_layer_output)\n",
    "            \n",
    "            # Backpropagation\n",
    "            dWs = []\n",
    "            dbs = []\n",
    "            \n",
    "            for i in range(num_hidden_layers + 1,0,-1):\n",
    "                if i == num_hidden_layers + 1:\n",
    "                    delta = outputs[i].copy()\n",
    "                    delta[range(Y.shape[0]),Y] -= 1\n",
    "                else:\n",
    "                    delta = delta.dot(self.weights[i].T) * self.activationPrime(outputs[i])\n",
    "                dW = (outputs[i-1].T).dot(delta)\n",
    "                dWs.append(dW)\n",
    "                dbs.append(np.sum(delta,axis=0,keepdims=True))\n",
    "                \n",
    "            if (n%100 == 0):\n",
    "                probabilities = outputs[-1]\n",
    "                loss = np.sum(-1*np.log(probabilities[range(Y.shape[0]),Y])) / Y.shape[0]\n",
    "                labels = np.argmax(outputs[-1],axis = 1)\n",
    "                accuracy = 100 * np.sum(labels == Y)/Y.shape[0]\n",
    "                print(\"Iteration \",n,\" ,Loss = \",loss,\" ,Accuracy = \",accuracy,\"%\")\n",
    "                \n",
    "            dWs.reverse()\n",
    "            dbs.reverse()\n",
    "\n",
    "            # Gradient Descent Parameter Update\n",
    "            for i in range(len(dWs)):\n",
    "                self.weights[i] += dWs[i].dot(-1 * lr)\n",
    "                self.biases[i] += dbs[i].dot(-1 * lr)\n",
    "\n",
    "    def predict(self,X):\n",
    "        return self.forward_run(X)\n",
    "        \n",
    "    def forward_run(self,X):\n",
    "        prev_layer_output = X\n",
    "        num_hidden_layers = len(self.weights) - 1\n",
    "        for i in range(num_hidden_layers + 1):\n",
    "            weight = self.weights[i]\n",
    "            bias = self.biases[i]\n",
    "            if i == num_hidden_layers:\n",
    "                probabilities = exp_normalize(prev_layer_output.dot(weight) + bias)\n",
    "                labels = np.argmax(probabilities,axis = 1)\n",
    "                return labels\n",
    "            else:\n",
    "                prev_layer_output = self.activation(prev_layer_output.dot(weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,avg,std):\n",
    "    if avg is None:\n",
    "        input_data = np.loadtxt(open(path, \"rb\"), delimiter=\",\")\n",
    "        Y = input_data[:,0].copy()\n",
    "        X = input_data[:,1:].copy()\n",
    "        avg = np.average(X,axis=0)\n",
    "        X = X - avg\n",
    "        std = np.std(X,axis=0)\n",
    "        std[(std == 0)] = 1\n",
    "        X = X / std\n",
    "        return X,Y,avg,std\n",
    "    else:\n",
    "        input_data = np.loadtxt(open(path, \"rb\"), delimiter=\",\")\n",
    "        X = input_data[:,1:].copy()\n",
    "        X = (X - avg)/std\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpX,Y,avg,std = load_data(training_data_path,None,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0  ,Loss =  3.8317014969139285  ,Accuracy =  0.0 %\n",
      "Iteration  100  ,Loss =  1.7088075370765061  ,Accuracy =  54.6875 %\n",
      "Iteration  200  ,Loss =  1.2941586624640817  ,Accuracy =  69.53125 %\n",
      "Iteration  300  ,Loss =  1.1486513990162552  ,Accuracy =  69.53125 %\n",
      "Iteration  400  ,Loss =  0.895254466733158  ,Accuracy =  78.125 %\n",
      "Iteration  500  ,Loss =  0.93088707206551  ,Accuracy =  72.65625 %\n",
      "Iteration  600  ,Loss =  0.9785850908233098  ,Accuracy =  75.0 %\n",
      "Iteration  700  ,Loss =  0.7400555974919976  ,Accuracy =  80.46875 %\n",
      "Iteration  800  ,Loss =  0.5688956905952327  ,Accuracy =  83.59375 %\n",
      "Iteration  900  ,Loss =  0.6146634781413804  ,Accuracy =  83.59375 %\n",
      "Iteration  1000  ,Loss =  0.6240115101126972  ,Accuracy =  83.59375 %\n",
      "Iteration  1100  ,Loss =  0.7217665907078903  ,Accuracy =  79.6875 %\n",
      "Iteration  1200  ,Loss =  0.706501395694505  ,Accuracy =  79.6875 %\n",
      "Iteration  1300  ,Loss =  0.5942898130527431  ,Accuracy =  89.84375 %\n",
      "Iteration  1400  ,Loss =  0.7089700976085631  ,Accuracy =  80.46875 %\n",
      "Iteration  1500  ,Loss =  0.5340424131142558  ,Accuracy =  87.5 %\n",
      "Iteration  1600  ,Loss =  0.4897566522942622  ,Accuracy =  88.28125 %\n",
      "Iteration  1700  ,Loss =  0.5588626018642868  ,Accuracy =  83.59375 %\n",
      "Iteration  1800  ,Loss =  0.5217928095552958  ,Accuracy =  86.71875 %\n",
      "Iteration  1900  ,Loss =  0.5566556983934763  ,Accuracy =  86.71875 %\n",
      "Iteration  2000  ,Loss =  0.6109792095231172  ,Accuracy =  85.15625 %\n",
      "Iteration  2100  ,Loss =  0.398426904972939  ,Accuracy =  88.28125 %\n",
      "Iteration  2200  ,Loss =  0.5230174798270985  ,Accuracy =  83.59375 %\n",
      "Iteration  2300  ,Loss =  0.437863775967504  ,Accuracy =  88.28125 %\n",
      "Iteration  2400  ,Loss =  0.49223349192430255  ,Accuracy =  87.5 %\n",
      "Iteration  2500  ,Loss =  0.32177116593344934  ,Accuracy =  92.1875 %\n",
      "Iteration  2600  ,Loss =  0.48964590324831875  ,Accuracy =  82.8125 %\n",
      "Iteration  2700  ,Loss =  0.2794765508583499  ,Accuracy =  94.53125 %\n",
      "Iteration  2800  ,Loss =  0.7241064173298289  ,Accuracy =  83.59375 %\n",
      "Iteration  2900  ,Loss =  0.4575757884200697  ,Accuracy =  87.5 %\n",
      "Iteration  3000  ,Loss =  0.4104052453935787  ,Accuracy =  89.84375 %\n",
      "Iteration  3100  ,Loss =  0.35736080265342407  ,Accuracy =  90.625 %\n",
      "Iteration  3200  ,Loss =  0.34638130938226186  ,Accuracy =  93.75 %\n",
      "Iteration  3300  ,Loss =  0.45874345790284665  ,Accuracy =  88.28125 %\n",
      "Iteration  3400  ,Loss =  0.49852401691144754  ,Accuracy =  82.03125 %\n",
      "Iteration  3500  ,Loss =  0.4139324168580196  ,Accuracy =  89.0625 %\n",
      "Iteration  3600  ,Loss =  0.5218671593048014  ,Accuracy =  86.71875 %\n",
      "Iteration  3700  ,Loss =  0.38621092374575394  ,Accuracy =  89.0625 %\n",
      "Iteration  3800  ,Loss =  0.40481965452261576  ,Accuracy =  90.625 %\n",
      "Iteration  3900  ,Loss =  0.36148256920316074  ,Accuracy =  89.84375 %\n",
      "Iteration  4000  ,Loss =  0.3850455078003318  ,Accuracy =  90.625 %\n",
      "Iteration  4100  ,Loss =  0.34665493578166606  ,Accuracy =  87.5 %\n",
      "Iteration  4200  ,Loss =  0.3091601918437702  ,Accuracy =  92.1875 %\n",
      "Iteration  4300  ,Loss =  0.3176535521756578  ,Accuracy =  92.1875 %\n",
      "Iteration  4400  ,Loss =  0.27039375528447046  ,Accuracy =  93.75 %\n",
      "Iteration  4500  ,Loss =  0.3037854095960861  ,Accuracy =  92.1875 %\n",
      "Iteration  4600  ,Loss =  0.46390574323535927  ,Accuracy =  89.0625 %\n",
      "Iteration  4700  ,Loss =  0.29687068028346586  ,Accuracy =  92.96875 %\n",
      "Iteration  4800  ,Loss =  0.3753384780058551  ,Accuracy =  90.625 %\n",
      "Iteration  4900  ,Loss =  0.37601205662461223  ,Accuracy =  91.40625 %\n",
      "Iteration  5000  ,Loss =  0.4582576595347873  ,Accuracy =  85.9375 %\n",
      "Iteration  5100  ,Loss =  0.3554620589559835  ,Accuracy =  89.84375 %\n",
      "Iteration  5200  ,Loss =  0.25555914578685635  ,Accuracy =  96.09375 %\n",
      "Iteration  5300  ,Loss =  0.3778661848983076  ,Accuracy =  88.28125 %\n",
      "Iteration  5400  ,Loss =  0.2618486206079212  ,Accuracy =  94.53125 %\n",
      "Iteration  5500  ,Loss =  0.280914357098547  ,Accuracy =  96.09375 %\n",
      "Iteration  5600  ,Loss =  0.3377553686096971  ,Accuracy =  91.40625 %\n",
      "Iteration  5700  ,Loss =  0.33742724613301806  ,Accuracy =  89.0625 %\n",
      "Iteration  5800  ,Loss =  0.3009727039172637  ,Accuracy =  92.1875 %\n",
      "Iteration  5900  ,Loss =  0.23163475664436664  ,Accuracy =  92.96875 %\n",
      "Iteration  6000  ,Loss =  0.355664388489383  ,Accuracy =  88.28125 %\n",
      "Iteration  6100  ,Loss =  0.30249713332798855  ,Accuracy =  93.75 %\n",
      "Iteration  6200  ,Loss =  0.3717942715689213  ,Accuracy =  91.40625 %\n",
      "Iteration  6300  ,Loss =  0.2986512259506693  ,Accuracy =  92.1875 %\n",
      "Iteration  6400  ,Loss =  0.3642222750314399  ,Accuracy =  89.84375 %\n",
      "Iteration  6500  ,Loss =  0.23259564833862123  ,Accuracy =  95.3125 %\n",
      "Iteration  6600  ,Loss =  0.35305775669070616  ,Accuracy =  90.625 %\n",
      "Iteration  6700  ,Loss =  0.23037178144702727  ,Accuracy =  95.3125 %\n",
      "Iteration  6800  ,Loss =  0.2861876962346275  ,Accuracy =  89.84375 %\n",
      "Iteration  6900  ,Loss =  0.28423298356055104  ,Accuracy =  89.84375 %\n",
      "Iteration  7000  ,Loss =  0.3146593864454731  ,Accuracy =  94.53125 %\n",
      "Iteration  7100  ,Loss =  0.2611419537170595  ,Accuracy =  93.75 %\n",
      "Iteration  7200  ,Loss =  0.3228774552943787  ,Accuracy =  92.96875 %\n",
      "Iteration  7300  ,Loss =  0.2133894111770851  ,Accuracy =  96.09375 %\n",
      "Iteration  7400  ,Loss =  0.4188643316170068  ,Accuracy =  89.84375 %\n",
      "Iteration  7500  ,Loss =  0.23748317517141593  ,Accuracy =  93.75 %\n",
      "Iteration  7600  ,Loss =  0.22845100609653643  ,Accuracy =  92.96875 %\n",
      "Iteration  7700  ,Loss =  0.32565796908781197  ,Accuracy =  92.1875 %\n",
      "Iteration  7800  ,Loss =  0.45909946548207226  ,Accuracy =  84.375 %\n",
      "Iteration  7900  ,Loss =  0.3523405973332453  ,Accuracy =  92.1875 %\n",
      "Iteration  8000  ,Loss =  0.28049064561443127  ,Accuracy =  92.96875 %\n",
      "Iteration  8100  ,Loss =  0.2926891604986321  ,Accuracy =  93.75 %\n",
      "Iteration  8200  ,Loss =  0.23250827295947177  ,Accuracy =  93.75 %\n",
      "Iteration  8300  ,Loss =  0.2592528795011687  ,Accuracy =  94.53125 %\n",
      "Iteration  8400  ,Loss =  0.1451381539655714  ,Accuracy =  97.65625 %\n",
      "Iteration  8500  ,Loss =  0.2512809083059436  ,Accuracy =  93.75 %\n",
      "Iteration  8600  ,Loss =  0.23410846610600752  ,Accuracy =  95.3125 %\n",
      "Iteration  8700  ,Loss =  0.2262548228360045  ,Accuracy =  94.53125 %\n",
      "Iteration  8800  ,Loss =  0.2752175819226812  ,Accuracy =  95.3125 %\n",
      "Iteration  8900  ,Loss =  0.23252839246691429  ,Accuracy =  96.09375 %\n",
      "Iteration  9000  ,Loss =  0.22608972506610836  ,Accuracy =  93.75 %\n",
      "Iteration  9100  ,Loss =  0.3091113629443686  ,Accuracy =  92.96875 %\n",
      "Iteration  9200  ,Loss =  0.18409232105136544  ,Accuracy =  95.3125 %\n",
      "Iteration  9300  ,Loss =  0.20097996929590473  ,Accuracy =  95.3125 %\n",
      "Iteration  9400  ,Loss =  0.20236647673286368  ,Accuracy =  94.53125 %\n",
      "Iteration  9500  ,Loss =  0.1745381131690174  ,Accuracy =  96.09375 %\n",
      "Iteration  9600  ,Loss =  0.3118540378931173  ,Accuracy =  92.1875 %\n",
      "Iteration  9700  ,Loss =  0.29822105419900774  ,Accuracy =  92.1875 %\n",
      "Iteration  9800  ,Loss =  0.1435937184501626  ,Accuracy =  98.4375 %\n",
      "Iteration  9900  ,Loss =  0.18431017902292895  ,Accuracy =  95.3125 %\n",
      "Iteration  10000  ,Loss =  0.22153456763924714  ,Accuracy =  93.75 %\n",
      "Iteration  10100  ,Loss =  0.29474008530751417  ,Accuracy =  92.1875 %\n",
      "Iteration  10200  ,Loss =  0.27351118993037427  ,Accuracy =  93.75 %\n",
      "Iteration  10300  ,Loss =  0.35504396937706456  ,Accuracy =  90.625 %\n",
      "Iteration  10400  ,Loss =  0.2726224978828271  ,Accuracy =  92.96875 %\n",
      "Iteration  10500  ,Loss =  0.20999266394400742  ,Accuracy =  96.09375 %\n",
      "Iteration  10600  ,Loss =  0.2303000797055889  ,Accuracy =  96.09375 %\n",
      "Iteration  10700  ,Loss =  0.2402272667984564  ,Accuracy =  93.75 %\n",
      "Iteration  10800  ,Loss =  0.31803681714870513  ,Accuracy =  96.09375 %\n",
      "Iteration  10900  ,Loss =  0.2536218431289399  ,Accuracy =  94.53125 %\n",
      "Iteration  11000  ,Loss =  0.18641942771398434  ,Accuracy =  96.875 %\n",
      "Iteration  11100  ,Loss =  0.22385762181208385  ,Accuracy =  94.53125 %\n",
      "Iteration  11200  ,Loss =  0.23643337317353033  ,Accuracy =  92.1875 %\n",
      "Iteration  11300  ,Loss =  0.3839458083679256  ,Accuracy =  89.84375 %\n",
      "Iteration  11400  ,Loss =  0.16508900367185747  ,Accuracy =  96.09375 %\n",
      "Iteration  11500  ,Loss =  0.19345369600032525  ,Accuracy =  96.09375 %\n",
      "Iteration  11600  ,Loss =  0.2279169554962285  ,Accuracy =  95.3125 %\n",
      "Iteration  11700  ,Loss =  0.2555139800468682  ,Accuracy =  92.96875 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  11800  ,Loss =  0.2207025791533423  ,Accuracy =  96.09375 %\n",
      "Iteration  11900  ,Loss =  0.2261633004560764  ,Accuracy =  94.53125 %\n",
      "Iteration  12000  ,Loss =  0.19893212156898799  ,Accuracy =  96.09375 %\n",
      "Iteration  12100  ,Loss =  0.2708385605270903  ,Accuracy =  93.75 %\n",
      "Iteration  12200  ,Loss =  0.24657388602657018  ,Accuracy =  91.40625 %\n",
      "Iteration  12300  ,Loss =  0.42185500663790476  ,Accuracy =  91.40625 %\n",
      "Iteration  12400  ,Loss =  0.25577859669939884  ,Accuracy =  93.75 %\n",
      "Iteration  12500  ,Loss =  0.1402055396571771  ,Accuracy =  96.875 %\n",
      "Iteration  12600  ,Loss =  0.19733588668792051  ,Accuracy =  94.53125 %\n",
      "Iteration  12700  ,Loss =  0.23445521972210562  ,Accuracy =  95.3125 %\n",
      "Iteration  12800  ,Loss =  0.2659165436912198  ,Accuracy =  93.75 %\n",
      "Iteration  12900  ,Loss =  0.2987084940127783  ,Accuracy =  92.96875 %\n",
      "Iteration  13000  ,Loss =  0.22681368739378818  ,Accuracy =  93.75 %\n",
      "Iteration  13100  ,Loss =  0.24408965320231948  ,Accuracy =  94.53125 %\n",
      "Iteration  13200  ,Loss =  0.18380107378444618  ,Accuracy =  96.09375 %\n",
      "Iteration  13300  ,Loss =  0.13301133226075726  ,Accuracy =  96.875 %\n",
      "Iteration  13400  ,Loss =  0.24219646302711342  ,Accuracy =  93.75 %\n",
      "Iteration  13500  ,Loss =  0.32153958898134793  ,Accuracy =  95.3125 %\n",
      "Iteration  13600  ,Loss =  0.15587354437597586  ,Accuracy =  98.4375 %\n",
      "Iteration  13700  ,Loss =  0.21913194841718217  ,Accuracy =  95.3125 %\n",
      "Iteration  13800  ,Loss =  0.26803620633421876  ,Accuracy =  92.1875 %\n",
      "Iteration  13900  ,Loss =  0.14308787374408644  ,Accuracy =  96.09375 %\n",
      "Iteration  14000  ,Loss =  0.2749370469270755  ,Accuracy =  92.1875 %\n",
      "Iteration  14100  ,Loss =  0.20662765559220878  ,Accuracy =  92.96875 %\n",
      "Iteration  14200  ,Loss =  0.21872408210048594  ,Accuracy =  92.1875 %\n",
      "Iteration  14300  ,Loss =  0.15822639471553535  ,Accuracy =  97.65625 %\n",
      "Iteration  14400  ,Loss =  0.15672781139944297  ,Accuracy =  95.3125 %\n",
      "Iteration  14500  ,Loss =  0.20076115744787634  ,Accuracy =  94.53125 %\n",
      "Iteration  14600  ,Loss =  0.26736503632476305  ,Accuracy =  92.1875 %\n",
      "Iteration  14700  ,Loss =  0.2817439068403699  ,Accuracy =  96.875 %\n",
      "Iteration  14800  ,Loss =  0.29878857175742635  ,Accuracy =  92.1875 %\n",
      "Iteration  14900  ,Loss =  0.1978402548867247  ,Accuracy =  95.3125 %\n",
      "Iteration  15000  ,Loss =  0.17002250652230871  ,Accuracy =  96.09375 %\n",
      "Iteration  15100  ,Loss =  0.26127601619399055  ,Accuracy =  92.1875 %\n",
      "Iteration  15200  ,Loss =  0.1564929704061025  ,Accuracy =  96.875 %\n",
      "Iteration  15300  ,Loss =  0.3201909498365018  ,Accuracy =  92.1875 %\n",
      "Iteration  15400  ,Loss =  0.17368674982514842  ,Accuracy =  96.09375 %\n",
      "Iteration  15500  ,Loss =  0.21829178350005318  ,Accuracy =  95.3125 %\n",
      "Iteration  15600  ,Loss =  0.16473788359672445  ,Accuracy =  96.09375 %\n",
      "Iteration  15700  ,Loss =  0.16959206151652065  ,Accuracy =  93.75 %\n",
      "Iteration  15800  ,Loss =  0.10194464429470533  ,Accuracy =  98.4375 %\n",
      "Iteration  15900  ,Loss =  0.16760644470434075  ,Accuracy =  96.875 %\n",
      "Iteration  16000  ,Loss =  0.1657002176983739  ,Accuracy =  96.875 %\n",
      "Iteration  16100  ,Loss =  0.23009278842986927  ,Accuracy =  93.75 %\n",
      "Iteration  16200  ,Loss =  0.24241715492329519  ,Accuracy =  94.53125 %\n",
      "Iteration  16300  ,Loss =  0.1727678976139147  ,Accuracy =  96.875 %\n",
      "Iteration  16400  ,Loss =  0.12152652540028343  ,Accuracy =  98.4375 %\n",
      "Iteration  16500  ,Loss =  0.1354537308731961  ,Accuracy =  96.875 %\n",
      "Iteration  16600  ,Loss =  0.16120772575814676  ,Accuracy =  96.09375 %\n",
      "Iteration  16700  ,Loss =  0.22086733834561148  ,Accuracy =  93.75 %\n",
      "Iteration  16800  ,Loss =  0.18590351661138635  ,Accuracy =  95.3125 %\n",
      "Iteration  16900  ,Loss =  0.23848972667987228  ,Accuracy =  96.09375 %\n",
      "Iteration  17000  ,Loss =  0.18266709379751242  ,Accuracy =  94.53125 %\n",
      "Iteration  17100  ,Loss =  0.19340994648748433  ,Accuracy =  95.3125 %\n",
      "Iteration  17200  ,Loss =  0.1654529816631661  ,Accuracy =  94.53125 %\n",
      "Iteration  17300  ,Loss =  0.09392622180212201  ,Accuracy =  97.65625 %\n",
      "Iteration  17400  ,Loss =  0.16548537348934952  ,Accuracy =  97.65625 %\n",
      "Iteration  17500  ,Loss =  0.2034208784266817  ,Accuracy =  94.53125 %\n",
      "Iteration  17600  ,Loss =  0.18809409375457997  ,Accuracy =  97.65625 %\n",
      "Iteration  17700  ,Loss =  0.10881173125053523  ,Accuracy =  97.65625 %\n",
      "Iteration  17800  ,Loss =  0.19016915561713565  ,Accuracy =  95.3125 %\n",
      "Iteration  17900  ,Loss =  0.2454155021872788  ,Accuracy =  93.75 %\n",
      "Iteration  18000  ,Loss =  0.08772044689395299  ,Accuracy =  98.4375 %\n",
      "Iteration  18100  ,Loss =  0.16311932072364402  ,Accuracy =  95.3125 %\n",
      "Iteration  18200  ,Loss =  0.16412079209834374  ,Accuracy =  96.09375 %\n",
      "Iteration  18300  ,Loss =  0.16384405117791997  ,Accuracy =  95.3125 %\n",
      "Iteration  18400  ,Loss =  0.17230207019198343  ,Accuracy =  96.875 %\n",
      "Iteration  18500  ,Loss =  0.13068953710389736  ,Accuracy =  97.65625 %\n",
      "Iteration  18600  ,Loss =  0.10099807536492522  ,Accuracy =  100.0 %\n",
      "Iteration  18700  ,Loss =  0.1265471206410249  ,Accuracy =  95.3125 %\n",
      "Iteration  18800  ,Loss =  0.14051729795029672  ,Accuracy =  96.09375 %\n",
      "Iteration  18900  ,Loss =  0.18542055693945578  ,Accuracy =  96.09375 %\n",
      "Iteration  19000  ,Loss =  0.2384405594803936  ,Accuracy =  94.53125 %\n",
      "Iteration  19100  ,Loss =  0.14900255951767918  ,Accuracy =  95.3125 %\n",
      "Iteration  19200  ,Loss =  0.1303168952251903  ,Accuracy =  96.875 %\n",
      "Iteration  19300  ,Loss =  0.15203777452294184  ,Accuracy =  96.09375 %\n",
      "Iteration  19400  ,Loss =  0.21185593395945143  ,Accuracy =  96.09375 %\n",
      "Iteration  19500  ,Loss =  0.171948783924126  ,Accuracy =  95.3125 %\n",
      "Iteration  19600  ,Loss =  0.14854542495584014  ,Accuracy =  96.875 %\n",
      "Iteration  19700  ,Loss =  0.11410002203321443  ,Accuracy =  95.3125 %\n",
      "Iteration  19800  ,Loss =  0.1435567965787353  ,Accuracy =  97.65625 %\n",
      "Iteration  19900  ,Loss =  0.19579627597383173  ,Accuracy =  94.53125 %\n",
      "Iteration  20000  ,Loss =  0.11806526586569607  ,Accuracy =  97.65625 %\n",
      "Iteration  20100  ,Loss =  0.1507965557663074  ,Accuracy =  97.65625 %\n",
      "Iteration  20200  ,Loss =  0.13034118447037538  ,Accuracy =  98.4375 %\n",
      "Iteration  20300  ,Loss =  0.11572324025384476  ,Accuracy =  97.65625 %\n",
      "Iteration  20400  ,Loss =  0.23939390084057396  ,Accuracy =  92.96875 %\n",
      "Iteration  20500  ,Loss =  0.2953126738991274  ,Accuracy =  89.84375 %\n",
      "Iteration  20600  ,Loss =  0.2072857639051676  ,Accuracy =  96.09375 %\n",
      "Iteration  20700  ,Loss =  0.17100390863226342  ,Accuracy =  97.65625 %\n",
      "Iteration  20800  ,Loss =  0.07640790688658776  ,Accuracy =  99.21875 %\n",
      "Iteration  20900  ,Loss =  0.16916737134589008  ,Accuracy =  96.875 %\n",
      "Iteration  21000  ,Loss =  0.17788939991390967  ,Accuracy =  94.53125 %\n",
      "Iteration  21100  ,Loss =  0.19128503631588725  ,Accuracy =  95.3125 %\n",
      "Iteration  21200  ,Loss =  0.15351606239605142  ,Accuracy =  96.875 %\n",
      "Iteration  21300  ,Loss =  0.11188061668506982  ,Accuracy =  97.65625 %\n",
      "Iteration  21400  ,Loss =  0.15101622208685653  ,Accuracy =  96.875 %\n",
      "Iteration  21500  ,Loss =  0.18808985333944034  ,Accuracy =  93.75 %\n",
      "Iteration  21600  ,Loss =  0.1602923594574216  ,Accuracy =  95.3125 %\n",
      "Iteration  21700  ,Loss =  0.1306726878542125  ,Accuracy =  96.09375 %\n",
      "Iteration  21800  ,Loss =  0.14245070324488895  ,Accuracy =  95.3125 %\n",
      "Iteration  21900  ,Loss =  0.14130916960381068  ,Accuracy =  96.09375 %\n",
      "Iteration  22000  ,Loss =  0.18197259295807372  ,Accuracy =  96.875 %\n",
      "Iteration  22100  ,Loss =  0.1574749267942185  ,Accuracy =  96.875 %\n",
      "Iteration  22200  ,Loss =  0.12410622753209706  ,Accuracy =  96.09375 %\n",
      "Iteration  22300  ,Loss =  0.09140499993426429  ,Accuracy =  97.65625 %\n",
      "Iteration  22400  ,Loss =  0.2568811060328704  ,Accuracy =  92.1875 %\n",
      "Iteration  22500  ,Loss =  0.20337867195892465  ,Accuracy =  95.3125 %\n",
      "Iteration  22600  ,Loss =  0.14853267050762192  ,Accuracy =  95.3125 %\n",
      "Iteration  22700  ,Loss =  0.18832747976811262  ,Accuracy =  93.75 %\n",
      "Iteration  22800  ,Loss =  0.09283821191162805  ,Accuracy =  99.21875 %\n",
      "Iteration  22900  ,Loss =  0.17184375473723856  ,Accuracy =  96.09375 %\n",
      "Iteration  23000  ,Loss =  0.1830396383020378  ,Accuracy =  96.09375 %\n",
      "Iteration  23100  ,Loss =  0.21234365576753844  ,Accuracy =  95.3125 %\n",
      "Iteration  23200  ,Loss =  0.16976092438262638  ,Accuracy =  96.875 %\n",
      "Iteration  23300  ,Loss =  0.1526083156805917  ,Accuracy =  94.53125 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  23400  ,Loss =  0.10727268224142844  ,Accuracy =  98.4375 %\n",
      "Iteration  23500  ,Loss =  0.20870787841079408  ,Accuracy =  94.53125 %\n",
      "Iteration  23600  ,Loss =  0.12351537030597429  ,Accuracy =  96.09375 %\n",
      "Iteration  23700  ,Loss =  0.15918679419087156  ,Accuracy =  96.09375 %\n",
      "Iteration  23800  ,Loss =  0.22527906181869267  ,Accuracy =  93.75 %\n",
      "Iteration  23900  ,Loss =  0.140011271398698  ,Accuracy =  96.875 %\n",
      "Iteration  24000  ,Loss =  0.2156645879463161  ,Accuracy =  95.3125 %\n",
      "Iteration  24100  ,Loss =  0.09859288654405096  ,Accuracy =  96.875 %\n",
      "Iteration  24200  ,Loss =  0.15381842359436582  ,Accuracy =  97.65625 %\n",
      "Iteration  24300  ,Loss =  0.14837208517838263  ,Accuracy =  95.3125 %\n",
      "Iteration  24400  ,Loss =  0.13930672597916094  ,Accuracy =  96.875 %\n"
     ]
    }
   ],
   "source": [
    "X = inpX.copy()\n",
    "\n",
    "input_size = X.shape[1]\n",
    "output_size = int(np.amax(Y))+1\n",
    "num_examples = X.shape[0]\n",
    "max_iterations = int(40*(num_examples/batch_size))\n",
    "\n",
    "network = NeuralNetwork(input_size,output_size,hidden_layers_sizes,activation)\n",
    "network.train(X,Y.astype(int),batch_size,n0,max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.64066496163683\n"
     ]
    }
   ],
   "source": [
    "predictions = network.predict(X.copy())\n",
    "print(100 * np.sum(predictions == Y)/Y.shape[0])\n",
    "# print(np.average(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = load_data(testing_data_path,avg,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = network.predict(testX)\n",
    "np.savetxt(output_path,predictions,fmt=\"%i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
