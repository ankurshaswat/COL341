{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from scipy.special import expit as sigmoid\n",
    "from scipy.fftpack import dctn\n",
    "\n",
    "training_data_path = sys.argv[1]\n",
    "testing_data_path = sys.argv[2]\n",
    "output_path = sys.argv[3]\n",
    "\n",
    "# training_data_path = \"../data/devnagri_train.csv\"\n",
    "# testing_data_path = \"../data/devnagri_test_public.csv\"\n",
    "# output_path = \"../data/nn/b/cs1160328.txt\"\n",
    "\n",
    "batch_size = 128\n",
    "n0 = 0.01\n",
    "activation = 'relu'\n",
    "hidden_layers_sizes = [200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return (x>0) * x\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def reluPrime(x):\n",
    "    return (x>0)+0\n",
    "\n",
    "def tanhPrime(x):\n",
    "    return 1 - np.power(x,2)\n",
    "\n",
    "def sigmoidPrime(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def exp_normalize(x):\n",
    "    b = np.amax(x,axis=1,keepdims = True)\n",
    "    y = np.exp(x - b)\n",
    "    return y / y.sum(axis=1,keepdims=True)\n",
    "\n",
    "def fft(X):\n",
    "    for i in range(X.shape[0]):\n",
    "        image = np.reshape(X[i,:],(32,32))\n",
    "        fft = np.fft.fft2(image)\n",
    "        X[i,:] = np.reshape(fft,(1,1024))\n",
    "    return X\n",
    "\n",
    "def dct(X):\n",
    "    for i in range(X.shape[0]):\n",
    "        image = np.reshape(X[i,:],(32,32))\n",
    "        dct = dctn(image)\n",
    "        X[i,:] = np.reshape(dct,(1,1024))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,input_size,output_size,hidden_layers_sizes, activation):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        if(activation == 'relu'):\n",
    "            self.activation = relu\n",
    "            self.activationPrime = reluPrime\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation = tanh\n",
    "            self.activationPrime = tanhPrime\n",
    "        else:\n",
    "            self.activation = sigmoid\n",
    "            self.activationPrime = sigmoidPrime\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hiddent_layers_sizes = hidden_layers_sizes\n",
    "        \n",
    "        prev_layer_count = input_size\n",
    "        \n",
    "        for i in range(len(hidden_layers_sizes) + 1):\n",
    "            if i==len(hidden_layers_sizes):\n",
    "                self.weights.append(np.random.rand(prev_layer_count, output_size)/100)\n",
    "                self.biases.append(np.random.rand(1, output_size)/100)        \n",
    "            else:\n",
    "                hidden_layer_count = hidden_layers_sizes[i]\n",
    "                self.weights.append(np.random.rand(prev_layer_count, hidden_layer_count)/100)\n",
    "                self.biases.append(np.random.rand(1, hidden_layer_count)/100)\n",
    "                prev_layer_count = hidden_layer_count\n",
    "        \n",
    "    def train(self,inpX,inpY,batch_size,n0,max_iterations):\n",
    "        max_examples = inpX.shape[0]\n",
    "        max_possible_iterations = int(0.5 + max_examples / batch_size)\n",
    "        num_hidden_layers = len(self.weights) - 1\n",
    "               \n",
    "        count = 0\n",
    "            \n",
    "        lr = n0\n",
    "        totLoss = 0\n",
    "        prevAvgLoss = sys.float_info.max\n",
    "        epoch = 0\n",
    "        \n",
    "        for n in range(max_iterations):\n",
    "            # Forming Mini Batches\n",
    "            i_eff = n%max_possible_iterations\n",
    "            \n",
    "            # Updating Learning Rate\n",
    "            if (i_eff == 0 and n!=0):\n",
    "                avgLoss = totLoss/max_possible_iterations\n",
    "                if(avgLoss >= prevAvgLoss):\n",
    "                    count += 1\n",
    "                    lr = n0 / np.sqrt(count+1)\n",
    "                print(\"Epoch = \",epoch,\" Average Loss = \",avgLoss,\" New Learning Rate = \",lr)\n",
    "                epoch += 1\n",
    "                prevAvgLoss = avgLoss\n",
    "                totLoss = 0\n",
    "            \n",
    "            outputs = []\n",
    "            \n",
    "            if i_eff != max_possible_iterations - 1:\n",
    "                X = inpX[i_eff*batch_size: (i_eff+1)*batch_size]\n",
    "                Y = inpY[i_eff*batch_size: (i_eff+1)*batch_size]\n",
    "            else:\n",
    "                X = inpX[i_eff*batch_size:]\n",
    "                Y = inpY[i_eff*batch_size:]\n",
    "                \n",
    "#             # Neural Network Forward Propagation (Cross Entropy)\n",
    "#             outputs.append(X)\n",
    "#             prev_layer_output = X\n",
    "#             for i in range(num_hidden_layers + 1):\n",
    "#                 weight = self.weights[i]\n",
    "#                 bias = self.biases[i]\n",
    "#                 if i == num_hidden_layers:\n",
    "#                     prev_layer_output = exp_normalize(prev_layer_output.dot(weight) + bias)\n",
    "#                 else:\n",
    "#                     prev_layer_output = self.activation(prev_layer_output.dot(weight) + bias)\n",
    "                    \n",
    "#                 outputs.append(prev_layer_output)\n",
    "            \n",
    "#             # Backpropagation\n",
    "#             dWs = []\n",
    "#             dbs = []\n",
    "            \n",
    "#             for i in range(num_hidden_layers + 1,0,-1):\n",
    "#                 if i == num_hidden_layers + 1:\n",
    "#                     delta = outputs[i].copy()\n",
    "#                     delta[range(Y.shape[0]),Y] -= 1\n",
    "#                 else:\n",
    "#                     delta = delta.dot(self.weights[i].T) * self.activationPrime(outputs[i])\n",
    "#                 dW = (outputs[i-1].T).dot(delta)\n",
    "#                 dWs.append(dW)\n",
    "#                 dbs.append(np.sum(delta,axis=0,keepdims=True))\n",
    "#             if (n%100 == 0):\n",
    "#                 loss_ = np.sum(-1*np.log(outputs[-1][range(Y.shape[0]),Y] + 0.001)) / Y.shape[0]\n",
    "#                 labels_ = np.argmax(outputs[-1],axis = 1)\n",
    "#                 accuracy_ = 100 * np.sum(labels_ == Y)/Y.shape[0]\n",
    "#                 print(\"Iteration \",n,\"\\tCE Loss = \",loss_,\"\\tAccuracy = \",accuracy_,\"%\")\n",
    "#             dWs.reverse()\n",
    "#             dbs.reverse()\n",
    "\n",
    "#             # Gradient Descent Parameter Update\n",
    "#             for i in range(len(dWs)):\n",
    "#                 self.weights[i] += dWs[i].dot(-1 * lr)\n",
    "#                 self.biases[i] += dbs[i].dot(-1 * lr)\n",
    "            \n",
    "#             loss = np.sum(-1*np.log(outputs[-1][range(Y.shape[0]),Y] + 0.001)) / Y.shape[0]\n",
    "#             totLoss += loss\n",
    "\n",
    "             # Neural Network Forward Propagation (MSE)\n",
    "            outputs.append(X)\n",
    "            prev_layer_output = X\n",
    "            for i in range(num_hidden_layers + 1):\n",
    "                weight = self.weights[i]\n",
    "                bias = self.biases[i]\n",
    "                if i == num_hidden_layers:\n",
    "                    prev_layer_output = sigmoid(prev_layer_output.dot(weight) + bias)\n",
    "                else:\n",
    "                    prev_layer_output = self.activation(prev_layer_output.dot(weight) + bias)\n",
    "                outputs.append(prev_layer_output)\n",
    "            \n",
    "            # Backpropagation\n",
    "            dWs = []\n",
    "            dbs = []\n",
    "            \n",
    "            y_onehot = np.zeros((Y.shape[0],self.output_size))\n",
    "            y_onehot[range(Y.shape[0]),Y] = 1\n",
    "            \n",
    "            for i in range(num_hidden_layers + 1,0,-1):\n",
    "                if i == num_hidden_layers + 1:\n",
    "                    delta = (outputs[i] - y_onehot).dot(2/Y.shape[0]) * sigmoidPrime(outputs[i])\n",
    "                else:\n",
    "                    delta = delta.dot(self.weights[i].T) * self.activationPrime(outputs[i])\n",
    "                dW = (outputs[i-1].T).dot(delta)\n",
    "                dWs.append(dW)\n",
    "                dbs.append(np.sum(delta,axis=0,keepdims=True))\n",
    "\n",
    "            if (n%100 == 0):\n",
    "                loss = np.sum(np.power(outputs[-1] - y_onehot,2) )/Y.shape[0]\n",
    "                labels = np.argmax(outputs[-1],axis = 1)\n",
    "                accuracy = 100 * np.sum(labels == Y)/Y.shape[0]\n",
    "                print(\"Iteration \",n,\"\\tMSE Loss = \",loss,\"\\tAccuracy = \",accuracy,\"%\")\n",
    "                \n",
    "            dWs.reverse()\n",
    "            dbs.reverse()\n",
    "\n",
    "            # Gradient Descent Parameter Update\n",
    "            for i in range(len(dWs)):\n",
    "                self.weights[i] += dWs[i].dot(-1 * lr)\n",
    "                self.biases[i] += dbs[i].dot(-1 * lr)\n",
    "\n",
    "            loss = np.sum(np.power(outputs[-1] - y_onehot,2) )/Y.shape[0]\n",
    "            totLoss += loss\n",
    "            \n",
    "    def predict(self,X):\n",
    "        return self.forward_run(X)\n",
    "        \n",
    "    def forward_run(self,X):\n",
    "        prev_layer_output = X\n",
    "        num_hidden_layers = len(self.weights) - 1\n",
    "        for i in range(num_hidden_layers + 1):\n",
    "            weight = self.weights[i]\n",
    "            bias = self.biases[i]\n",
    "            if i == num_hidden_layers:\n",
    "                probabilities = exp_normalize(prev_layer_output.dot(weight) + bias)\n",
    "                labels = np.argmax(probabilities,axis = 1)\n",
    "                return labels\n",
    "            else:\n",
    "                prev_layer_output = self.activation(prev_layer_output.dot(weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,avg,std):\n",
    "    if avg is None:\n",
    "        input_data = np.loadtxt(open(path, \"rb\"), delimiter=\",\")\n",
    "        Y = input_data[:,0].copy()\n",
    "        X = input_data[:,1:].copy()\n",
    "        avg = np.average(X,axis=0)\n",
    "        X = X - avg\n",
    "        std = np.std(X,axis=0)\n",
    "        std[(std == 0)] = 1\n",
    "        X = X / std\n",
    "        return X,Y,avg,std\n",
    "    else:\n",
    "        input_data = np.loadtxt(open(path, \"rb\"), delimiter=\",\")\n",
    "        X = input_data[:,1:].copy()\n",
    "        X = (X - avg)/std\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpX,Y,avg,std = load_data(training_data_path,None,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.637978807091713e-12\n",
      "6.048139766789973e-10\n",
      "Iteration  0 \tMSE Loss =  35.18026208546236 \tAccuracy =  3.125 %\n",
      "Iteration  100 \tMSE Loss =  0.9904652300968961 \tAccuracy =  9.375 %\n",
      "Iteration  200 \tMSE Loss =  0.9660095532643762 \tAccuracy =  9.375 %\n",
      "Iteration  300 \tMSE Loss =  0.9418715750823361 \tAccuracy =  15.625 %\n",
      "Iteration  400 \tMSE Loss =  0.8664990720727512 \tAccuracy =  21.875 %\n",
      "Iteration  500 \tMSE Loss =  0.8805841604357059 \tAccuracy =  22.65625 %\n",
      "Iteration  600 \tMSE Loss =  0.8158299945986209 \tAccuracy =  26.5625 %\n",
      "Epoch =  0  Average Loss =  1.0659298038492888  New Learning Rate =  0.01\n",
      "Iteration  700 \tMSE Loss =  0.8525702436544783 \tAccuracy =  25.78125 %\n",
      "Iteration  800 \tMSE Loss =  0.7161159984971204 \tAccuracy =  40.625 %\n",
      "Iteration  900 \tMSE Loss =  0.6923964973078903 \tAccuracy =  43.75 %\n",
      "Iteration  1000 \tMSE Loss =  0.732105764222156 \tAccuracy =  40.625 %\n",
      "Iteration  1100 \tMSE Loss =  0.726699320863179 \tAccuracy =  39.0625 %\n",
      "Iteration  1200 \tMSE Loss =  0.6228903019999272 \tAccuracy =  52.34375 %\n",
      "Epoch =  1  Average Loss =  0.7604052045522727  New Learning Rate =  0.01\n",
      "Iteration  1300 \tMSE Loss =  0.6829735967262118 \tAccuracy =  45.3125 %\n",
      "Iteration  1400 \tMSE Loss =  0.6763557727976842 \tAccuracy =  48.4375 %\n",
      "Iteration  1500 \tMSE Loss =  0.6311224142906126 \tAccuracy =  45.3125 %\n",
      "Iteration  1600 \tMSE Loss =  0.5146214103694327 \tAccuracy =  61.71875 %\n",
      "Iteration  1700 \tMSE Loss =  0.6194948643359476 \tAccuracy =  53.90625 %\n",
      "Iteration  1800 \tMSE Loss =  0.5682359181994922 \tAccuracy =  57.8125 %\n",
      "Epoch =  2  Average Loss =  0.6214456670277541  New Learning Rate =  0.01\n",
      "Iteration  1900 \tMSE Loss =  0.6281509893949864 \tAccuracy =  52.34375 %\n",
      "Iteration  2000 \tMSE Loss =  0.5061989069782877 \tAccuracy =  61.71875 %\n",
      "Iteration  2100 \tMSE Loss =  0.4845126692075033 \tAccuracy =  60.15625 %\n",
      "Iteration  2200 \tMSE Loss =  0.4952157900579257 \tAccuracy =  63.28125 %\n",
      "Iteration  2300 \tMSE Loss =  0.4488828301559673 \tAccuracy =  64.84375 %\n",
      "Iteration  2400 \tMSE Loss =  0.47837536987571433 \tAccuracy =  62.5 %\n",
      "Epoch =  3  Average Loss =  0.49113665083057306  New Learning Rate =  0.01\n",
      "Iteration  2500 \tMSE Loss =  0.33379552738263835 \tAccuracy =  77.34375 %\n",
      "Iteration  2600 \tMSE Loss =  0.4216402318695063 \tAccuracy =  74.21875 %\n",
      "Iteration  2700 \tMSE Loss =  0.38106674726405454 \tAccuracy =  70.3125 %\n",
      "Iteration  2800 \tMSE Loss =  0.38333634516894466 \tAccuracy =  71.09375 %\n",
      "Iteration  2900 \tMSE Loss =  0.45758878145401516 \tAccuracy =  67.1875 %\n",
      "Iteration  3000 \tMSE Loss =  0.3696424537489271 \tAccuracy =  73.4375 %\n",
      "Epoch =  4  Average Loss =  0.41883689029860927  New Learning Rate =  0.01\n",
      "Iteration  3100 \tMSE Loss =  0.34081661466954927 \tAccuracy =  78.90625 %\n",
      "Iteration  3200 \tMSE Loss =  0.3240066282891674 \tAccuracy =  75.78125 %\n",
      "Iteration  3300 \tMSE Loss =  0.3910732411610641 \tAccuracy =  71.875 %\n",
      "Iteration  3400 \tMSE Loss =  0.38620230960958124 \tAccuracy =  68.75 %\n",
      "Iteration  3500 \tMSE Loss =  0.3108595182644941 \tAccuracy =  78.125 %\n",
      "Iteration  3600 \tMSE Loss =  0.3525641754577905 \tAccuracy =  76.5625 %\n",
      "Epoch =  5  Average Loss =  0.3775407462369841  New Learning Rate =  0.01\n",
      "Iteration  3700 \tMSE Loss =  0.4276820680186619 \tAccuracy =  70.3125 %\n",
      "Iteration  3800 \tMSE Loss =  0.3418975378071659 \tAccuracy =  75.78125 %\n",
      "Iteration  3900 \tMSE Loss =  0.37753147817081356 \tAccuracy =  74.21875 %\n",
      "Iteration  4000 \tMSE Loss =  0.38973678514378834 \tAccuracy =  73.4375 %\n",
      "Iteration  4100 \tMSE Loss =  0.332946648918105 \tAccuracy =  76.5625 %\n",
      "Iteration  4200 \tMSE Loss =  0.34252159373241853 \tAccuracy =  74.21875 %\n",
      "Epoch =  6  Average Loss =  0.34588863859811  New Learning Rate =  0.01\n",
      "Iteration  4300 \tMSE Loss =  0.33212977575471603 \tAccuracy =  76.5625 %\n",
      "Iteration  4400 \tMSE Loss =  0.29404391564800836 \tAccuracy =  81.25 %\n",
      "Iteration  4500 \tMSE Loss =  0.29247824697118174 \tAccuracy =  79.6875 %\n",
      "Iteration  4600 \tMSE Loss =  0.34215299266949517 \tAccuracy =  73.4375 %\n",
      "Iteration  4700 \tMSE Loss =  0.37007076433158925 \tAccuracy =  72.65625 %\n",
      "Iteration  4800 \tMSE Loss =  0.3157372833705125 \tAccuracy =  78.125 %\n",
      "Epoch =  7  Average Loss =  0.3216981189718235  New Learning Rate =  0.01\n",
      "Iteration  4900 \tMSE Loss =  0.30653259634156654 \tAccuracy =  80.46875 %\n",
      "Iteration  5000 \tMSE Loss =  0.39934727580611873 \tAccuracy =  72.65625 %\n",
      "Iteration  5100 \tMSE Loss =  0.39183890408061467 \tAccuracy =  68.75 %\n",
      "Iteration  5200 \tMSE Loss =  0.23620386728986298 \tAccuracy =  81.25 %\n",
      "Iteration  5300 \tMSE Loss =  0.3103530479161962 \tAccuracy =  76.5625 %\n",
      "Iteration  5400 \tMSE Loss =  0.2908828377065391 \tAccuracy =  80.46875 %\n",
      "Epoch =  8  Average Loss =  0.30504450611188555  New Learning Rate =  0.01\n",
      "Iteration  5500 \tMSE Loss =  0.26519341148066944 \tAccuracy =  82.03125 %\n",
      "Iteration  5600 \tMSE Loss =  0.28334452972526414 \tAccuracy =  82.03125 %\n",
      "Iteration  5700 \tMSE Loss =  0.31597161565239135 \tAccuracy =  77.34375 %\n",
      "Iteration  5800 \tMSE Loss =  0.26495983746839386 \tAccuracy =  81.25 %\n",
      "Iteration  5900 \tMSE Loss =  0.2695108239473759 \tAccuracy =  80.46875 %\n",
      "Iteration  6000 \tMSE Loss =  0.29029196193412715 \tAccuracy =  78.90625 %\n",
      "Iteration  6100 \tMSE Loss =  0.2390297402217787 \tAccuracy =  82.8125 %\n",
      "Epoch =  9  Average Loss =  0.2920197792393223  New Learning Rate =  0.01\n",
      "Iteration  6200 \tMSE Loss =  0.3167226205017575 \tAccuracy =  79.6875 %\n",
      "Iteration  6300 \tMSE Loss =  0.24824806328610655 \tAccuracy =  82.8125 %\n",
      "Iteration  6400 \tMSE Loss =  0.2764846106509865 \tAccuracy =  80.46875 %\n",
      "Iteration  6500 \tMSE Loss =  0.21670122052489144 \tAccuracy =  84.375 %\n",
      "Iteration  6600 \tMSE Loss =  0.34146934658388534 \tAccuracy =  72.65625 %\n",
      "Iteration  6700 \tMSE Loss =  0.2582501758663609 \tAccuracy =  82.03125 %\n",
      "Epoch =  10  Average Loss =  0.2814873342529379  New Learning Rate =  0.01\n",
      "Iteration  6800 \tMSE Loss =  0.33937020941825785 \tAccuracy =  75.78125 %\n",
      "Iteration  6900 \tMSE Loss =  0.27720164998548136 \tAccuracy =  82.03125 %\n",
      "Iteration  7000 \tMSE Loss =  0.2785846281238528 \tAccuracy =  79.6875 %\n",
      "Iteration  7100 \tMSE Loss =  0.29050405363096965 \tAccuracy =  81.25 %\n",
      "Iteration  7200 \tMSE Loss =  0.2673085646302902 \tAccuracy =  81.25 %\n",
      "Iteration  7300 \tMSE Loss =  0.19464952714992986 \tAccuracy =  85.9375 %\n",
      "Epoch =  11  Average Loss =  0.2724406657355009  New Learning Rate =  0.01\n",
      "Iteration  7400 \tMSE Loss =  0.3029997441725345 \tAccuracy =  81.25 %\n",
      "Iteration  7500 \tMSE Loss =  0.2657423699795385 \tAccuracy =  80.46875 %\n",
      "Iteration  7600 \tMSE Loss =  0.24342621582635374 \tAccuracy =  84.375 %\n",
      "Iteration  7700 \tMSE Loss =  0.28172487334921154 \tAccuracy =  75.0 %\n",
      "Iteration  7800 \tMSE Loss =  0.3705155680790623 \tAccuracy =  69.53125 %\n",
      "Iteration  7900 \tMSE Loss =  0.296833967701458 \tAccuracy =  79.6875 %\n",
      "Epoch =  12  Average Loss =  0.2650819972228507  New Learning Rate =  0.01\n",
      "Iteration  8000 \tMSE Loss =  0.18437511970751427 \tAccuracy =  89.0625 %\n",
      "Iteration  8100 \tMSE Loss =  0.29564996025956297 \tAccuracy =  78.125 %\n",
      "Iteration  8200 \tMSE Loss =  0.24061399826948465 \tAccuracy =  78.90625 %\n",
      "Iteration  8300 \tMSE Loss =  0.23617257952216408 \tAccuracy =  81.25 %\n",
      "Iteration  8400 \tMSE Loss =  0.23945908211320507 \tAccuracy =  82.03125 %\n",
      "Iteration  8500 \tMSE Loss =  0.26281686858446224 \tAccuracy =  79.6875 %\n",
      "Epoch =  13  Average Loss =  0.259609101916115  New Learning Rate =  0.01\n",
      "Iteration  8600 \tMSE Loss =  0.23811712736900553 \tAccuracy =  85.9375 %\n",
      "Iteration  8700 \tMSE Loss =  0.26235476622108883 \tAccuracy =  78.125 %\n",
      "Iteration  8800 \tMSE Loss =  0.2509635374518428 \tAccuracy =  82.8125 %\n",
      "Iteration  8900 \tMSE Loss =  0.25884742069613786 \tAccuracy =  78.125 %\n",
      "Iteration  9000 \tMSE Loss =  0.2259325505359664 \tAccuracy =  83.59375 %\n",
      "Iteration  9100 \tMSE Loss =  0.3263090624623425 \tAccuracy =  78.90625 %\n",
      "Epoch =  14  Average Loss =  0.25438651802370577  New Learning Rate =  0.01\n",
      "Iteration  9200 \tMSE Loss =  0.2101632145092462 \tAccuracy =  82.8125 %\n",
      "Iteration  9300 \tMSE Loss =  0.22657844299845661 \tAccuracy =  81.25 %\n",
      "Iteration  9400 \tMSE Loss =  0.23317037883980252 \tAccuracy =  82.8125 %\n",
      "Iteration  9500 \tMSE Loss =  0.22415659254828585 \tAccuracy =  83.59375 %\n",
      "Iteration  9600 \tMSE Loss =  0.21876431481536196 \tAccuracy =  83.59375 %\n",
      "Iteration  9700 \tMSE Loss =  0.3178538367860346 \tAccuracy =  75.78125 %\n",
      "Epoch =  15  Average Loss =  0.24900134376007946  New Learning Rate =  0.01\n",
      "Iteration  9800 \tMSE Loss =  0.14788796970534326 \tAccuracy =  90.625 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  9900 \tMSE Loss =  0.1890706370258891 \tAccuracy =  86.71875 %\n",
      "Iteration  10000 \tMSE Loss =  0.2584372831358984 \tAccuracy =  83.59375 %\n",
      "Iteration  10100 \tMSE Loss =  0.23770271502321272 \tAccuracy =  85.9375 %\n",
      "Iteration  10200 \tMSE Loss =  0.2026681277472574 \tAccuracy =  84.375 %\n",
      "Iteration  10300 \tMSE Loss =  0.24610242039931077 \tAccuracy =  82.8125 %\n",
      "Epoch =  16  Average Loss =  0.24409834069770373  New Learning Rate =  0.01\n",
      "Iteration  10400 \tMSE Loss =  0.3351515861048274 \tAccuracy =  74.21875 %\n",
      "Iteration  10500 \tMSE Loss =  0.26063606285271307 \tAccuracy =  82.8125 %\n",
      "Iteration  10600 \tMSE Loss =  0.28033755750260747 \tAccuracy =  78.125 %\n",
      "Iteration  10700 \tMSE Loss =  0.16988231826453215 \tAccuracy =  87.5 %\n",
      "Iteration  10800 \tMSE Loss =  0.18984429229854785 \tAccuracy =  85.15625 %\n",
      "Iteration  10900 \tMSE Loss =  0.221668712766736 \tAccuracy =  85.9375 %\n",
      "Epoch =  17  Average Loss =  0.2397974360257414  New Learning Rate =  0.01\n",
      "Iteration  11000 \tMSE Loss =  0.222995886550115 \tAccuracy =  82.03125 %\n",
      "Iteration  11100 \tMSE Loss =  0.28584250153583257 \tAccuracy =  77.34375 %\n",
      "Iteration  11200 \tMSE Loss =  0.17974645008528972 \tAccuracy =  84.375 %\n",
      "Iteration  11300 \tMSE Loss =  0.26830668521468665 \tAccuracy =  78.90625 %\n",
      "Iteration  11400 \tMSE Loss =  0.20622220065613678 \tAccuracy =  85.9375 %\n",
      "Iteration  11500 \tMSE Loss =  0.25637197506487913 \tAccuracy =  82.03125 %\n",
      "Iteration  11600 \tMSE Loss =  0.22538369721992244 \tAccuracy =  83.59375 %\n",
      "Epoch =  18  Average Loss =  0.23538125978562113  New Learning Rate =  0.01\n",
      "Iteration  11700 \tMSE Loss =  0.23887712077886314 \tAccuracy =  82.8125 %\n",
      "Iteration  11800 \tMSE Loss =  0.21239330297422931 \tAccuracy =  83.59375 %\n",
      "Iteration  11900 \tMSE Loss =  0.2617549049056834 \tAccuracy =  81.25 %\n",
      "Iteration  12000 \tMSE Loss =  0.2109412368668205 \tAccuracy =  85.9375 %\n",
      "Iteration  12100 \tMSE Loss =  0.24131955881565653 \tAccuracy =  79.6875 %\n",
      "Iteration  12200 \tMSE Loss =  0.3578735092443468 \tAccuracy =  72.65625 %\n",
      "Epoch =  19  Average Loss =  0.23194351135992866  New Learning Rate =  0.01\n",
      "Iteration  12300 \tMSE Loss =  0.2510058630504969 \tAccuracy =  82.8125 %\n",
      "Iteration  12400 \tMSE Loss =  0.23112079540732128 \tAccuracy =  81.25 %\n",
      "Iteration  12500 \tMSE Loss =  0.19053394049577976 \tAccuracy =  84.375 %\n",
      "Iteration  12600 \tMSE Loss =  0.19706291733795425 \tAccuracy =  84.375 %\n",
      "Iteration  12700 \tMSE Loss =  0.23643780666030212 \tAccuracy =  81.25 %\n",
      "Iteration  12800 \tMSE Loss =  0.16941537011212338 \tAccuracy =  88.28125 %\n",
      "Epoch =  20  Average Loss =  0.23045760159551815  New Learning Rate =  0.01\n",
      "Iteration  12900 \tMSE Loss =  0.20845593198849566 \tAccuracy =  85.9375 %\n",
      "Iteration  13000 \tMSE Loss =  0.1750871998257597 \tAccuracy =  87.5 %\n",
      "Iteration  13100 \tMSE Loss =  0.20421684320149966 \tAccuracy =  82.8125 %\n",
      "Iteration  13200 \tMSE Loss =  0.2155744642750705 \tAccuracy =  82.8125 %\n",
      "Iteration  13300 \tMSE Loss =  0.22693823853164502 \tAccuracy =  82.03125 %\n",
      "Iteration  13400 \tMSE Loss =  0.2460983681966504 \tAccuracy =  82.03125 %\n",
      "Epoch =  21  Average Loss =  0.22775117225874017  New Learning Rate =  0.01\n",
      "Iteration  13500 \tMSE Loss =  0.2668797420207274 \tAccuracy =  79.6875 %\n",
      "Iteration  13600 \tMSE Loss =  0.24286093701998096 \tAccuracy =  82.8125 %\n",
      "Iteration  13700 \tMSE Loss =  0.1987469026093633 \tAccuracy =  85.15625 %\n",
      "Iteration  13800 \tMSE Loss =  0.21590286053386454 \tAccuracy =  83.59375 %\n",
      "Iteration  13900 \tMSE Loss =  0.16378946330094601 \tAccuracy =  86.71875 %\n",
      "Iteration  14000 \tMSE Loss =  0.2227700348594775 \tAccuracy =  83.59375 %\n",
      "Epoch =  22  Average Loss =  0.22456326897302545  New Learning Rate =  0.01\n",
      "Iteration  14100 \tMSE Loss =  0.22611430809008815 \tAccuracy =  84.375 %\n",
      "Iteration  14200 \tMSE Loss =  0.3209759932786837 \tAccuracy =  77.34375 %\n",
      "Iteration  14300 \tMSE Loss =  0.21344899974802628 \tAccuracy =  82.8125 %\n",
      "Iteration  14400 \tMSE Loss =  0.17978331148238347 \tAccuracy =  89.84375 %\n",
      "Iteration  14500 \tMSE Loss =  0.27449601898933773 \tAccuracy =  80.46875 %\n",
      "Iteration  14600 \tMSE Loss =  0.22840200220374896 \tAccuracy =  83.59375 %\n",
      "Epoch =  23  Average Loss =  0.22205286183773754  New Learning Rate =  0.01\n",
      "Iteration  14700 \tMSE Loss =  0.20745178749708737 \tAccuracy =  86.71875 %\n",
      "Iteration  14800 \tMSE Loss =  0.2888869987899878 \tAccuracy =  76.5625 %\n",
      "Iteration  14900 \tMSE Loss =  0.20764193833044547 \tAccuracy =  85.15625 %\n",
      "Iteration  15000 \tMSE Loss =  0.1965350380916206 \tAccuracy =  87.5 %\n",
      "Iteration  15100 \tMSE Loss =  0.20001958774662515 \tAccuracy =  82.8125 %\n",
      "Iteration  15200 \tMSE Loss =  0.1877117780492437 \tAccuracy =  84.375 %\n",
      "Epoch =  24  Average Loss =  0.22033039187632797  New Learning Rate =  0.01\n",
      "Iteration  15300 \tMSE Loss =  0.23566695743492883 \tAccuracy =  79.6875 %\n",
      "Iteration  15400 \tMSE Loss =  0.2797571271303526 \tAccuracy =  78.125 %\n",
      "Iteration  15500 \tMSE Loss =  0.21347003744411 \tAccuracy =  83.59375 %\n",
      "Iteration  15600 \tMSE Loss =  0.28164303668681745 \tAccuracy =  76.5625 %\n",
      "Iteration  15700 \tMSE Loss =  0.18312383479414174 \tAccuracy =  87.5 %\n",
      "Iteration  15800 \tMSE Loss =  0.16701147930271368 \tAccuracy =  89.84375 %\n",
      "Epoch =  25  Average Loss =  0.21833017225297627  New Learning Rate =  0.01\n",
      "Iteration  15900 \tMSE Loss =  0.22341512869075308 \tAccuracy =  83.59375 %\n",
      "Iteration  16000 \tMSE Loss =  0.19968573974353548 \tAccuracy =  85.15625 %\n",
      "Iteration  16100 \tMSE Loss =  0.1929123152764476 \tAccuracy =  90.625 %\n",
      "Iteration  16200 \tMSE Loss =  0.20660766347682283 \tAccuracy =  85.15625 %\n",
      "Iteration  16300 \tMSE Loss =  0.23182992152992415 \tAccuracy =  82.03125 %\n",
      "Iteration  16400 \tMSE Loss =  0.22271324081623106 \tAccuracy =  82.8125 %\n",
      "Epoch =  26  Average Loss =  0.21706169775439535  New Learning Rate =  0.01\n",
      "Iteration  16500 \tMSE Loss =  0.19488589636869136 \tAccuracy =  88.28125 %\n",
      "Iteration  16600 \tMSE Loss =  0.2388642918156691 \tAccuracy =  82.8125 %\n",
      "Iteration  16700 \tMSE Loss =  0.2775750337466969 \tAccuracy =  78.90625 %\n",
      "Iteration  16800 \tMSE Loss =  0.20868882060515215 \tAccuracy =  85.15625 %\n",
      "Iteration  16900 \tMSE Loss =  0.22144689866245473 \tAccuracy =  85.9375 %\n",
      "Iteration  17000 \tMSE Loss =  0.16189068681793736 \tAccuracy =  89.0625 %\n",
      "Iteration  17100 \tMSE Loss =  0.2154828863995783 \tAccuracy =  82.03125 %\n",
      "Epoch =  27  Average Loss =  0.2126791178677229  New Learning Rate =  0.01\n",
      "Iteration  17200 \tMSE Loss =  0.19455408350082296 \tAccuracy =  87.5 %\n",
      "Iteration  17300 \tMSE Loss =  0.16782705378240548 \tAccuracy =  86.71875 %\n",
      "Iteration  17400 \tMSE Loss =  0.18770952388899564 \tAccuracy =  85.9375 %\n",
      "Iteration  17500 \tMSE Loss =  0.23424623725908314 \tAccuracy =  83.59375 %\n",
      "Iteration  17600 \tMSE Loss =  0.256641162782421 \tAccuracy =  82.03125 %\n",
      "Iteration  17700 \tMSE Loss =  0.18242357530544379 \tAccuracy =  85.15625 %\n",
      "Epoch =  28  Average Loss =  0.2130827681938778  New Learning Rate =  0.0070710678118654745\n",
      "Iteration  17800 \tMSE Loss =  0.19322957538230168 \tAccuracy =  83.59375 %\n",
      "Iteration  17900 \tMSE Loss =  0.2174877986081603 \tAccuracy =  80.46875 %\n",
      "Iteration  18000 \tMSE Loss =  0.19215900476367503 \tAccuracy =  85.15625 %\n",
      "Iteration  18100 \tMSE Loss =  0.21795285244639329 \tAccuracy =  82.03125 %\n",
      "Iteration  18200 \tMSE Loss =  0.19903772448708307 \tAccuracy =  82.03125 %\n",
      "Iteration  18300 \tMSE Loss =  0.184268012571499 \tAccuracy =  85.15625 %\n",
      "Epoch =  29  Average Loss =  0.19691660904266395  New Learning Rate =  0.0070710678118654745\n",
      "Iteration  18400 \tMSE Loss =  0.215743203060991 \tAccuracy =  82.03125 %\n",
      "Iteration  18500 \tMSE Loss =  0.18275922195410238 \tAccuracy =  85.15625 %\n",
      "Iteration  18600 \tMSE Loss =  0.1738628025747165 \tAccuracy =  86.71875 %\n",
      "Iteration  18700 \tMSE Loss =  0.19224574204410927 \tAccuracy =  85.15625 %\n",
      "Iteration  18800 \tMSE Loss =  0.18200717054387094 \tAccuracy =  86.71875 %\n",
      "Iteration  18900 \tMSE Loss =  0.1505471646526258 \tAccuracy =  87.5 %\n",
      "Epoch =  30  Average Loss =  0.19131626400425888  New Learning Rate =  0.0070710678118654745\n",
      "Iteration  19000 \tMSE Loss =  0.20304283608818113 \tAccuracy =  85.15625 %\n",
      "Iteration  19100 \tMSE Loss =  0.14310143737697478 \tAccuracy =  88.28125 %\n",
      "Iteration  19200 \tMSE Loss =  0.1326846735132891 \tAccuracy =  91.40625 %\n",
      "Iteration  19300 \tMSE Loss =  0.18470979142555433 \tAccuracy =  82.8125 %\n",
      "Iteration  19400 \tMSE Loss =  0.2259397909786679 \tAccuracy =  82.03125 %\n",
      "Iteration  19500 \tMSE Loss =  0.19429948543204245 \tAccuracy =  83.59375 %\n",
      "Epoch =  31  Average Loss =  0.18716411806025715  New Learning Rate =  0.0070710678118654745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  19600 \tMSE Loss =  0.21511596244991524 \tAccuracy =  82.8125 %\n",
      "Iteration  19700 \tMSE Loss =  0.1850705823420265 \tAccuracy =  85.15625 %\n",
      "Iteration  19800 \tMSE Loss =  0.16075510185342468 \tAccuracy =  86.71875 %\n",
      "Iteration  19900 \tMSE Loss =  0.23245338617097605 \tAccuracy =  80.46875 %\n",
      "Iteration  20000 \tMSE Loss =  0.12335081700937764 \tAccuracy =  89.84375 %\n",
      "Iteration  20100 \tMSE Loss =  0.15302669753966253 \tAccuracy =  86.71875 %\n",
      "Epoch =  32  Average Loss =  0.18331790802649245  New Learning Rate =  0.0070710678118654745\n",
      "Iteration  20200 \tMSE Loss =  0.1953764159441327 \tAccuracy =  85.9375 %\n",
      "Iteration  20300 \tMSE Loss =  0.14960570817858115 \tAccuracy =  89.0625 %\n",
      "Iteration  20400 \tMSE Loss =  0.20390393104304627 \tAccuracy =  82.03125 %\n",
      "Iteration  20500 \tMSE Loss =  0.1730751883948521 \tAccuracy =  85.9375 %\n",
      "Iteration  20600 \tMSE Loss =  0.2576095819017131 \tAccuracy =  78.125 %\n",
      "Iteration  20700 \tMSE Loss =  0.1571849490711074 \tAccuracy =  86.71875 %\n",
      "Epoch =  33  Average Loss =  0.18234149464773827  New Learning Rate =  0.0070710678118654745\n",
      "Iteration  20800 \tMSE Loss =  0.14674789540923225 \tAccuracy =  88.28125 %\n",
      "Iteration  20900 \tMSE Loss =  0.1795031297182659 \tAccuracy =  85.15625 %\n",
      "Iteration  21000 \tMSE Loss =  0.25199594144951715 \tAccuracy =  81.25 %\n",
      "Iteration  21100 \tMSE Loss =  0.1763422851717518 \tAccuracy =  85.9375 %\n",
      "Iteration  21200 \tMSE Loss =  0.11349718844449669 \tAccuracy =  92.1875 %\n",
      "Iteration  21300 \tMSE Loss =  0.16632832415177695 \tAccuracy =  85.9375 %\n",
      "Epoch =  34  Average Loss =  0.18048759025614028  New Learning Rate =  0.0070710678118654745\n",
      "Iteration  21400 \tMSE Loss =  0.21074704817491502 \tAccuracy =  82.03125 %\n",
      "Iteration  21500 \tMSE Loss =  0.18020916791157857 \tAccuracy =  83.59375 %\n",
      "Iteration  21600 \tMSE Loss =  0.26180868956364545 \tAccuracy =  77.34375 %\n",
      "Iteration  21700 \tMSE Loss =  0.12206098948991426 \tAccuracy =  91.40625 %\n",
      "Iteration  21800 \tMSE Loss =  0.13766418938327024 \tAccuracy =  90.625 %\n",
      "Iteration  21900 \tMSE Loss =  0.178357727406028 \tAccuracy =  86.71875 %\n",
      "Epoch =  35  Average Loss =  0.17931819301292198  New Learning Rate =  0.0070710678118654745\n",
      "Iteration  22000 \tMSE Loss =  0.1348994782035493 \tAccuracy =  90.625 %\n",
      "Iteration  22100 \tMSE Loss =  0.1934646239103671 \tAccuracy =  85.15625 %\n",
      "Iteration  22200 \tMSE Loss =  0.1871352047252351 \tAccuracy =  85.15625 %\n",
      "Iteration  22300 \tMSE Loss =  0.2158450651236877 \tAccuracy =  82.03125 %\n",
      "Iteration  22400 \tMSE Loss =  0.22197298633210105 \tAccuracy =  82.03125 %\n",
      "Iteration  22500 \tMSE Loss =  0.15987668579314868 \tAccuracy =  87.5 %\n",
      "Iteration  22600 \tMSE Loss =  0.23681933420746412 \tAccuracy =  80.46875 %\n",
      "Epoch =  36  Average Loss =  0.17639474495959756  New Learning Rate =  0.0070710678118654745\n",
      "Iteration  22700 \tMSE Loss =  0.22696404204214612 \tAccuracy =  80.46875 %\n",
      "Iteration  22800 \tMSE Loss =  0.20661935754865457 \tAccuracy =  82.03125 %\n",
      "Iteration  22900 \tMSE Loss =  0.11908382322496441 \tAccuracy =  90.625 %\n",
      "Iteration  23000 \tMSE Loss =  0.25996942430425285 \tAccuracy =  78.90625 %\n",
      "Iteration  23100 \tMSE Loss =  0.18134787174144787 \tAccuracy =  85.9375 %\n",
      "Iteration  23200 \tMSE Loss =  0.18695831299840077 \tAccuracy =  85.15625 %\n",
      "Epoch =  37  Average Loss =  0.1744452879680451  New Learning Rate =  0.0070710678118654745\n",
      "Iteration  23300 \tMSE Loss =  0.17032280342940448 \tAccuracy =  83.59375 %\n",
      "Iteration  23400 \tMSE Loss =  0.1787799503400042 \tAccuracy =  84.375 %\n",
      "Iteration  23500 \tMSE Loss =  0.20074310115916505 \tAccuracy =  81.25 %\n",
      "Iteration  23600 \tMSE Loss =  0.1249101125930461 \tAccuracy =  88.28125 %\n",
      "Iteration  23700 \tMSE Loss =  0.19639628395625935 \tAccuracy =  85.9375 %\n",
      "Iteration  23800 \tMSE Loss =  0.20459634565161733 \tAccuracy =  82.8125 %\n",
      "Epoch =  38  Average Loss =  0.17378838563051652  New Learning Rate =  0.0070710678118654745\n",
      "Iteration  23900 \tMSE Loss =  0.1963068879494429 \tAccuracy =  82.8125 %\n",
      "Iteration  24000 \tMSE Loss =  0.2011159114189272 \tAccuracy =  83.59375 %\n",
      "Iteration  24100 \tMSE Loss =  0.1449768875351577 \tAccuracy =  87.5 %\n",
      "Iteration  24200 \tMSE Loss =  0.2139834730282864 \tAccuracy =  79.6875 %\n",
      "Iteration  24300 \tMSE Loss =  0.209320814121648 \tAccuracy =  82.8125 %\n",
      "Iteration  24400 \tMSE Loss =  0.14349256788173992 \tAccuracy =  87.5 %\n"
     ]
    }
   ],
   "source": [
    "X = inpX.copy()\n",
    "# print(np.sum(X))\n",
    "# X = dct(X)\n",
    "# print(np.sum(X))\n",
    "\n",
    "input_size = X.shape[1]\n",
    "output_size = int(np.amax(Y))+1\n",
    "num_examples = X.shape[0]\n",
    "max_iterations = int(40*(num_examples/batch_size))\n",
    "\n",
    "network = NeuralNetwork(input_size,output_size,hidden_layers_sizes,activation)\n",
    "network.train(X,Y.astype(int),batch_size,n0,max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.54219948849105\n"
     ]
    }
   ],
   "source": [
    "predictions = network.predict(X.copy())\n",
    "print(100 * np.sum(predictions == Y)/Y.shape[0])\n",
    "# print(np.average(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = load_data(testing_data_path,avg,std)\n",
    "# testX = dct(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = network.predict(testX)\n",
    "np.savetxt(output_path,predictions,fmt=\"%i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
