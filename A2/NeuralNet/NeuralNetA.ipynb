{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "# training_data_path = sys.argv[1]\n",
    "# testing_data_path = sys.argv[2]\n",
    "# output_path = sys.argv[3]\n",
    "# batch_size = int(sys.argv[4])\n",
    "# n0 = sys.argv[5]\n",
    "# activation = sys.argv[6]\n",
    "# hidden_layers_sizes = []\n",
    "# for i in range(7,len(sys.argv)):\n",
    "#     hidden_layers_sizes.append(int(sys.argv[i]))\n",
    "\n",
    "training_data_path = \"../data/devnagri_train.csv\"\n",
    "testing_data_path = \"../data/devnagri_test_public.csv\"\n",
    "output_path = \"../data/nn/a/cs1160328.txt\"\n",
    "batch_size = 128\n",
    "n0 = 2\n",
    "activation = 'sigmoid'\n",
    "hidden_layers_sizes = [50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return (x>0) * x\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def reluPrime(x):\n",
    "    return (x>0)+0\n",
    "\n",
    "def tanhPrime(x):\n",
    "    return 1 - np.power(x,2)\n",
    "\n",
    "def sigmoidPrime(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def exp_normalize(x):\n",
    "    b = np.amax(x,axis=1,keepdims = True)\n",
    "    y = np.exp(x - b)\n",
    "    return y / y.sum(axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,input_size,output_size,hidden_layers_sizes, activation):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        if(activation == 'relu'):\n",
    "            self.activation = relu\n",
    "            self.activationPrime = reluPrime\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation = tanh\n",
    "            self.activationPrime = tanhPrime\n",
    "        else:\n",
    "            self.activation = sigmoid\n",
    "            self.activationPrime = sigmoidPrime\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hiddent_layers_sizes = hidden_layers_sizes\n",
    "        \n",
    "        prev_layer_count = input_size\n",
    "        \n",
    "        for i in range(len(hidden_layers_sizes) + 1):\n",
    "            if i==len(hidden_layers_sizes):\n",
    "                self.weights.append(np.random.rand(prev_layer_count, output_size)/100)\n",
    "                self.biases.append(np.random.rand(1, output_size)/100)        \n",
    "            else:\n",
    "                hidden_layer_count = hidden_layers_sizes[i]\n",
    "                self.weights.append(np.random.rand(prev_layer_count, hidden_layer_count)/100)\n",
    "                self.biases.append(np.random.rand(1, hidden_layer_count)/100)\n",
    "                prev_layer_count = hidden_layer_count\n",
    "        \n",
    "    def train(self,inpX,inpY,batch_size,n0,max_iterations):\n",
    "        max_examples = inpX.shape[0]\n",
    "        max_possible_iterations = int(0.5 + max_examples / batch_size)\n",
    "        num_hidden_layers = len(self.weights) - 1\n",
    "        \n",
    "        count = 0\n",
    "            \n",
    "        lr = n0\n",
    "        totLoss = 0\n",
    "        prevAvgLoss = sys.float_info.max\n",
    "        epoch = 0\n",
    "        \n",
    "        for n in range(max_iterations):\n",
    "            # Forming Mini Batches\n",
    "            i_eff = n%max_possible_iterations\n",
    "            \n",
    "            # Updating Learning Rate\n",
    "            if (i_eff == 0 and n!=0):\n",
    "                avgLoss = totLoss/max_possible_iterations\n",
    "                if(avgLoss >= prevAvgLoss):\n",
    "                    count += 1\n",
    "                    lr = n0 / np.sqrt(count+1)\n",
    "                print(\"Epoch = \",epoch,\" Average Loss = \",avgLoss,\" New Learning Rate = \",lr)\n",
    "                epoch += 1\n",
    "                prevAvgLoss = avgLoss\n",
    "                totLoss = 0\n",
    "#             lr = n0 / np.sqrt(n+1) \n",
    "            \n",
    "            \n",
    "            outputs = []\n",
    "            \n",
    "            if i_eff != max_possible_iterations - 1:\n",
    "                X = inpX[i_eff*batch_size: (i_eff+1)*batch_size]\n",
    "                Y = inpY[i_eff*batch_size: (i_eff+1)*batch_size]\n",
    "            else:\n",
    "                X = inpX[i_eff*batch_size:]\n",
    "                Y = inpY[i_eff*batch_size:]\n",
    "                \n",
    "            # Neural Network Forward Propagation\n",
    "            outputs.append(X)\n",
    "            prev_layer_output = X\n",
    "            for i in range(num_hidden_layers + 1):\n",
    "                weight = self.weights[i]\n",
    "                bias = self.biases[i]\n",
    "                if i == num_hidden_layers:\n",
    "                    prev_layer_output = sigmoid(prev_layer_output.dot(weight) + bias)\n",
    "                else:\n",
    "                    prev_layer_output = self.activation(prev_layer_output.dot(weight) + bias)\n",
    "                outputs.append(prev_layer_output)\n",
    "            \n",
    "            # Backpropagation\n",
    "            dWs = []\n",
    "            dbs = []\n",
    "            \n",
    "            y_onehot = np.zeros((Y.shape[0],self.output_size))\n",
    "            y_onehot[range(Y.shape[0]),Y] = 1\n",
    "            \n",
    "            for i in range(num_hidden_layers + 1,0,-1):\n",
    "                if i == num_hidden_layers + 1:\n",
    "                    delta = (outputs[i] - y_onehot).dot(2/Y.shape[0]) * sigmoidPrime(outputs[i])\n",
    "                else:\n",
    "                    delta = delta.dot(self.weights[i].T) * self.activationPrime(outputs[i])\n",
    "                dW = (outputs[i-1].T).dot(delta)\n",
    "                dWs.append(dW)\n",
    "                dbs.append(np.sum(delta,axis=0,keepdims=True))\n",
    "                \n",
    "#             if (n%100 == 0):\n",
    "#                 loss = np.sum(np.power(outputs[-1] - y_onehot,2) )/Y.shape[0]\n",
    "#                 labels = np.argmax(outputs[-1],axis = 1)\n",
    "#                 accuracy = 100 * np.sum(labels == Y)/Y.shape[0]\n",
    "#                 print(\"Iteration \",n,\"\\tLoss = \",loss,\"\\tAccuracy = \",accuracy,\"%\")\n",
    "                \n",
    "            dWs.reverse()\n",
    "            dbs.reverse()\n",
    "\n",
    "            # Gradient Descent Parameter Update\n",
    "            for i in range(len(dWs)):\n",
    "                self.weights[i] += dWs[i].dot(-1 * lr)\n",
    "                self.biases[i] += dbs[i].dot(-1 * lr)\n",
    "\n",
    "            loss = np.sum(np.power(outputs[-1] - y_onehot,2) )/Y.shape[0]\n",
    "            totLoss += loss\n",
    "                \n",
    "    def predict(self,X):\n",
    "        return self.forward_run(X)\n",
    "        \n",
    "    def forward_run(self,X):\n",
    "        prev_layer_output = X\n",
    "        num_hidden_layers = len(self.weights) - 1\n",
    "        for i in range(num_hidden_layers + 1):\n",
    "            weight = self.weights[i]\n",
    "            bias = self.biases[i]\n",
    "            if i == num_hidden_layers:\n",
    "                probabilities = sigmoid(prev_layer_output.dot(weight) + bias)\n",
    "                labels = np.argmax(probabilities,axis = 1)\n",
    "                return labels\n",
    "            else:\n",
    "                prev_layer_output = self.activation(prev_layer_output.dot(weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,avg,std):\n",
    "    if avg is None:\n",
    "        input_data = np.loadtxt(open(path, \"rb\"), delimiter=\",\")\n",
    "        Y = input_data[:,0].copy()\n",
    "        X = input_data[:,1:].copy()\n",
    "        avg = np.average(X,axis=0)\n",
    "        X = X - avg\n",
    "        std = np.std(X,axis=0)\n",
    "        std[(std == 0)] = 1\n",
    "        X = X / std\n",
    "        return X,Y,avg,std\n",
    "    else:\n",
    "        input_data = np.loadtxt(open(path, \"rb\"), delimiter=\",\")\n",
    "        X = input_data[:,1:].copy()\n",
    "        X = (X - avg)/std\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpX,Y,avg,std = load_data(training_data_path,None,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  0  Average Loss =  1.0194231108129779  New Learning Rate =  100\n",
      "Epoch =  1  Average Loss =  0.999999999984083  New Learning Rate =  100\n",
      "Epoch =  2  Average Loss =  0.999999999984083  New Learning Rate =  70.71067811865474\n",
      "Epoch =  3  Average Loss =  0.999999999984083  New Learning Rate =  57.73502691896258\n",
      "Epoch =  4  Average Loss =  0.999999999984083  New Learning Rate =  50.0\n",
      "Epoch =  5  Average Loss =  0.999999999984083  New Learning Rate =  44.72135954999579\n",
      "Epoch =  6  Average Loss =  0.999999999984083  New Learning Rate =  40.824829046386306\n",
      "Epoch =  7  Average Loss =  0.999999999984083  New Learning Rate =  37.79644730092272\n",
      "Epoch =  8  Average Loss =  0.999999999984083  New Learning Rate =  35.35533905932737\n",
      "Epoch =  9  Average Loss =  0.999999999984083  New Learning Rate =  33.333333333333336\n",
      "Epoch =  10  Average Loss =  0.999999999984083  New Learning Rate =  31.622776601683793\n",
      "Epoch =  11  Average Loss =  0.999999999984083  New Learning Rate =  30.15113445777636\n",
      "Epoch =  12  Average Loss =  0.999999999984083  New Learning Rate =  28.86751345948129\n",
      "Epoch =  13  Average Loss =  0.999999999984083  New Learning Rate =  27.735009811261456\n",
      "Epoch =  14  Average Loss =  0.999999999984083  New Learning Rate =  26.726124191242437\n",
      "Epoch =  15  Average Loss =  0.999999999984083  New Learning Rate =  25.81988897471611\n",
      "Epoch =  16  Average Loss =  0.999999999984083  New Learning Rate =  25.0\n",
      "Epoch =  17  Average Loss =  0.999999999984083  New Learning Rate =  24.253562503633297\n",
      "Epoch =  18  Average Loss =  0.999999999984083  New Learning Rate =  23.570226039551585\n",
      "Epoch =  19  Average Loss =  0.999999999984083  New Learning Rate =  22.941573387056174\n",
      "Epoch =  20  Average Loss =  0.999999999984083  New Learning Rate =  22.360679774997894\n",
      "Epoch =  21  Average Loss =  0.999999999984083  New Learning Rate =  21.82178902359924\n",
      "Epoch =  22  Average Loss =  0.999999999984083  New Learning Rate =  21.320071635561042\n",
      "Epoch =  23  Average Loss =  0.999999999984083  New Learning Rate =  20.85144140570748\n",
      "Epoch =  24  Average Loss =  0.999999999984083  New Learning Rate =  20.412414523193153\n",
      "Epoch =  25  Average Loss =  0.999999999984083  New Learning Rate =  20.0\n",
      "Epoch =  26  Average Loss =  0.999999999984083  New Learning Rate =  19.611613513818405\n",
      "Epoch =  27  Average Loss =  0.999999999984083  New Learning Rate =  19.245008972987524\n",
      "Epoch =  28  Average Loss =  0.999999999984083  New Learning Rate =  18.89822365046136\n",
      "Epoch =  29  Average Loss =  0.999999999984083  New Learning Rate =  18.56953381770519\n",
      "Epoch =  30  Average Loss =  0.999999999984083  New Learning Rate =  18.257418583505537\n",
      "Epoch =  31  Average Loss =  0.999999999984083  New Learning Rate =  17.960530202677493\n",
      "Epoch =  32  Average Loss =  0.999999999984083  New Learning Rate =  17.677669529663685\n",
      "Epoch =  33  Average Loss =  0.999999999984083  New Learning Rate =  17.407765595569785\n",
      "Epoch =  34  Average Loss =  0.999999999984083  New Learning Rate =  17.149858514250884\n",
      "Epoch =  35  Average Loss =  0.999999999984083  New Learning Rate =  16.903085094570333\n",
      "Epoch =  36  Average Loss =  0.999999999984083  New Learning Rate =  16.666666666666668\n",
      "Epoch =  37  Average Loss =  0.999999999984083  New Learning Rate =  16.43989873053573\n",
      "Epoch =  38  Average Loss =  0.999999999984083  New Learning Rate =  16.222142113076256\n"
     ]
    }
   ],
   "source": [
    "X = inpX.copy()\n",
    "\n",
    "input_size = X.shape[1]\n",
    "output_size = int(np.amax(Y))+1\n",
    "num_examples = X.shape[0]\n",
    "max_iterations = int(40*(num_examples/batch_size))\n",
    "\n",
    "network = NeuralNetwork(input_size,output_size,hidden_layers_sizes,activation)\n",
    "network.train(X,Y.astype(int),batch_size,n0,max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraccy on Training Data =  41.452685421994886\n",
      "Average of predictions on Training Data =  19.716649616368287\n"
     ]
    }
   ],
   "source": [
    "predictions = network.predict(X.copy())\n",
    "print(\"Accuraccy on Training Data = \",100 * np.sum(predictions == Y)/Y.shape[0])\n",
    "print(\"Average of predictions on Training Data = \",np.average(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = load_data(testing_data_path,avg,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = network.predict(testX)\n",
    "np.savetxt(output_path,predictions,fmt=\"%i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
