{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "training_data_path = \"../data/devnagri_train.csv\"\n",
    "testing_data_path = \"../data/devnagri_test_public.csv\"\n",
    "output_path = \"../data/out.txt\"\n",
    "output_size = 46\n",
    "hidden_layers_sizes = [ 50]\n",
    "activation = 'sigmoid'\n",
    "input_size = -1\n",
    "batch_size = 32\n",
    "n0 = 0.001\n",
    "max_iterations = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return (x>0) * x\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-1 * x))\n",
    "\n",
    "def reluPrime(x):\n",
    "    return (x>0)+0\n",
    "\n",
    "def tanhPrime(x):\n",
    "    return 1 - np.power(x,2)\n",
    "\n",
    "def sigmoidPrime(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def exp_normalize(x):\n",
    "    b = np.amax(x,axis=1,keepdims = True)\n",
    "    y = np.exp(x - b)\n",
    "    return y / y.sum(axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,input_size,output_size,hidden_layers_sizes, activation):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        if(activation == 'relu'):\n",
    "            self.activation = relu\n",
    "            self.activationPrime = reluPrime\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation = tanh\n",
    "            self.activationPrime = tanhPrime\n",
    "        else:\n",
    "            self.activation = sigmoid\n",
    "            self.activationPrime = sigmoidPrime\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hiddent_layers_sizes = hidden_layers_sizes\n",
    "        \n",
    "        prev_layer_count = input_size\n",
    "        \n",
    "        for i in range(len(hidden_layers_sizes) + 1):\n",
    "            if i==len(hidden_layers_sizes):\n",
    "                self.weights.append(np.random.rand(prev_layer_count, output_size)/100)\n",
    "                self.biases.append(np.random.rand(1, output_size)/100)        \n",
    "            else:\n",
    "                hidden_layer_count = hidden_layers_sizes[i]\n",
    "                self.weights.append(np.random.rand(prev_layer_count, hidden_layer_count)/100)\n",
    "                self.biases.append(np.random.rand(1, hidden_layer_count)/100)\n",
    "                prev_layer_count = hidden_layer_count\n",
    "        \n",
    "    def train(self,inpX,inpY,batch_size,n0,max_iterations):\n",
    "        max_examples = inpX.shape[0]\n",
    "        max_possible_iterations = int(0.5 + max_examples / batch_size)\n",
    "        num_hidden_layers = len(self.weights) - 1\n",
    "        \n",
    "        for n in range(max_iterations):\n",
    "            # Forming Mini Batches\n",
    "            i_eff = n%max_possible_iterations\n",
    "            \n",
    "            outputs = []\n",
    "            \n",
    "            if i_eff != max_possible_iterations - 1:\n",
    "                X = inpX[i_eff*batch_size: (i_eff+1)*batch_size]\n",
    "                Y = inpY[i_eff*batch_size: (i_eff+1)*batch_size]\n",
    "            else:\n",
    "                X = inpX[i_eff*batch_size:]\n",
    "                Y = inpY[i_eff*batch_size:]\n",
    "            \n",
    "            # Updating Learning Rate\n",
    "            lr = n0 / np.sqrt(n+1) \n",
    "                \n",
    "            # Neural Network Forward Propagation\n",
    "            outputs.append(X)\n",
    "            prev_layer_output = X\n",
    "            for i in range(num_hidden_layers + 1):\n",
    "                weight = self.weights[i]\n",
    "                bias = self.biases[i]\n",
    "                if i == num_hidden_layers:\n",
    "                    prev_layer_output = sigmoid(prev_layer_output.dot(weight) + bias)\n",
    "                else:\n",
    "                    prev_layer_output = self.activation(prev_layer_output.dot(weight) + bias)\n",
    "                outputs.append(prev_layer_output)\n",
    "            \n",
    "            # Backpropagation\n",
    "            dWs = []\n",
    "            dbs = []\n",
    "            \n",
    "            y_onehot = np.zeros((Y.shape[0],self.output_size))\n",
    "            y_onehot[range(Y.shape[0]),Y] = 1\n",
    "            \n",
    "            for i in range(num_hidden_layers + 1,0,-1):\n",
    "                if i == num_hidden_layers + 1:\n",
    "                    delta = (outputs[i] - y_onehot).dot(2/Y.shape[0]) * sigmoidPrime(outputs[i])\n",
    "                else:\n",
    "                    delta = delta.dot(self.weights[i].T) * self.activationPrime(outputs[i])\n",
    "                dW = (outputs[i-1].T).dot(delta)\n",
    "                dWs.append(dW)\n",
    "                dbs.append(np.sum(delta,axis=0,keepdims=True))\n",
    "                \n",
    "            if (n%100 == 0):\n",
    "                loss = np.sum(np.power(outputs[-1] - y_onehot,2) )/Y.shape[0]\n",
    "                labels = np.argmax(outputs[-1],axis = 1)\n",
    "                accuracy = 100 * np.sum(labels == Y)/Y.shape[0]\n",
    "                print(\"Iteration \",n,\"\\tLoss = \",loss,\"\\tAccuracy = \",accuracy,\"%\")\n",
    "                \n",
    "            dWs.reverse()\n",
    "            dbs.reverse()\n",
    "\n",
    "            # Gradient Descent Parameter Update\n",
    "            for i in range(len(dWs)):\n",
    "                self.weights[i] += dWs[i].dot(-1 * lr)\n",
    "                self.biases[i] += dbs[i].dot(-1 * lr)\n",
    "\n",
    "    def predict(self,X):\n",
    "        return self.forward_run(X)\n",
    "        \n",
    "    def forward_run(self,X):\n",
    "        prev_layer_output = X\n",
    "        num_hidden_layers = len(self.weights) - 1\n",
    "        for i in range(num_hidden_layers + 1):\n",
    "            weight = self.weights[i]\n",
    "            bias = self.biases[i]\n",
    "            if i == num_hidden_layers:\n",
    "                probabilities = sigmoid(prev_layer_output.dot(weight) + bias)\n",
    "                labels = np.argmax(probabilities,axis = 1)\n",
    "                return labels\n",
    "            else:\n",
    "                prev_layer_output = self.activation(prev_layer_output.dot(weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,avg,std):\n",
    "    if avg is None:\n",
    "        input_data = np.loadtxt(open(path, \"rb\"), delimiter=\",\")\n",
    "        Y = input_data[:,0].copy()\n",
    "        X = input_data[:,1:].copy()\n",
    "        avg = np.average(X,axis=0)\n",
    "        X = X - avg\n",
    "        std = np.std(X,axis=0)\n",
    "        std[(std == 0)] = 1\n",
    "        X = X / std\n",
    "        return X,Y,avg,std\n",
    "    else:\n",
    "        input_data = np.loadtxt(open(path, \"rb\"), delimiter=\",\")\n",
    "        X = input_data[:,1:].copy()\n",
    "        X = (X - avg)/std\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpX,Y,avg,std = load_data(training_data_path,None,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 \tLoss =  13.052155417035252 \tAccuracy =  0.0 %\n",
      "Iteration  100 \tLoss =  12.291752180886219 \tAccuracy =  6.25 %\n",
      "Iteration  200 \tLoss =  11.940904700552725 \tAccuracy =  3.125 %\n",
      "Iteration  300 \tLoss =  11.738440489795808 \tAccuracy =  0.0 %\n",
      "Iteration  400 \tLoss =  11.52352440508204 \tAccuracy =  0.0 %\n",
      "Iteration  500 \tLoss =  11.361664815758777 \tAccuracy =  0.0 %\n",
      "Iteration  600 \tLoss =  11.208653478672904 \tAccuracy =  3.125 %\n",
      "Iteration  700 \tLoss =  11.06072975065171 \tAccuracy =  3.125 %\n",
      "Iteration  800 \tLoss =  10.971640980379405 \tAccuracy =  0.0 %\n",
      "Iteration  900 \tLoss =  10.865334320170124 \tAccuracy =  3.125 %\n",
      "Iteration  1000 \tLoss =  10.80541397488632 \tAccuracy =  0.0 %\n",
      "Iteration  1100 \tLoss =  10.529237415230677 \tAccuracy =  0.0 %\n",
      "Iteration  1200 \tLoss =  10.430819930038659 \tAccuracy =  0.0 %\n",
      "Iteration  1300 \tLoss =  10.545250147597244 \tAccuracy =  0.0 %\n",
      "Iteration  1400 \tLoss =  10.299883073837652 \tAccuracy =  0.0 %\n",
      "Iteration  1500 \tLoss =  10.216055690039742 \tAccuracy =  0.0 %\n",
      "Iteration  1600 \tLoss =  10.110050626924403 \tAccuracy =  0.0 %\n",
      "Iteration  1700 \tLoss =  10.072246216273747 \tAccuracy =  3.125 %\n",
      "Iteration  1800 \tLoss =  10.096749433109796 \tAccuracy =  0.0 %\n",
      "Iteration  1900 \tLoss =  10.074889470346482 \tAccuracy =  3.125 %\n",
      "Iteration  2000 \tLoss =  9.80785313729565 \tAccuracy =  3.125 %\n",
      "Iteration  2100 \tLoss =  9.829973641835373 \tAccuracy =  3.125 %\n",
      "Iteration  2200 \tLoss =  9.868482597532271 \tAccuracy =  6.25 %\n",
      "Iteration  2300 \tLoss =  9.674514403586741 \tAccuracy =  6.25 %\n",
      "Iteration  2400 \tLoss =  9.625559036174694 \tAccuracy =  0.0 %\n",
      "Iteration  2500 \tLoss =  9.692842248500462 \tAccuracy =  0.0 %\n",
      "Iteration  2600 \tLoss =  9.653852930632315 \tAccuracy =  0.0 %\n",
      "Iteration  2700 \tLoss =  9.441572280245351 \tAccuracy =  0.0 %\n",
      "Iteration  2800 \tLoss =  9.425728435630083 \tAccuracy =  3.125 %\n",
      "Iteration  2900 \tLoss =  9.379004270709688 \tAccuracy =  3.125 %\n",
      "Iteration  3000 \tLoss =  9.615665300720552 \tAccuracy =  0.0 %\n",
      "Iteration  3100 \tLoss =  9.157571785791623 \tAccuracy =  3.125 %\n",
      "Iteration  3200 \tLoss =  9.011432419211966 \tAccuracy =  3.125 %\n",
      "Iteration  3300 \tLoss =  9.104059322939774 \tAccuracy =  0.0 %\n",
      "Iteration  3400 \tLoss =  9.025209030253151 \tAccuracy =  3.125 %\n",
      "Iteration  3500 \tLoss =  9.092077406317038 \tAccuracy =  0.0 %\n",
      "Iteration  3600 \tLoss =  8.88844183898443 \tAccuracy =  3.125 %\n",
      "Iteration  3700 \tLoss =  8.91800509120328 \tAccuracy =  3.125 %\n",
      "Iteration  3800 \tLoss =  9.051800638825458 \tAccuracy =  0.0 %\n",
      "Iteration  3900 \tLoss =  8.964675191050866 \tAccuracy =  6.25 %\n",
      "Iteration  4000 \tLoss =  8.635001733768682 \tAccuracy =  3.125 %\n",
      "Iteration  4100 \tLoss =  8.852138846152016 \tAccuracy =  0.0 %\n",
      "Iteration  4200 \tLoss =  8.831252973430036 \tAccuracy =  9.375 %\n",
      "Iteration  4300 \tLoss =  8.98987224678336 \tAccuracy =  3.125 %\n",
      "Iteration  4400 \tLoss =  8.711235829815223 \tAccuracy =  0.0 %\n",
      "Iteration  4500 \tLoss =  8.846369232400647 \tAccuracy =  0.0 %\n",
      "Iteration  4600 \tLoss =  8.606521963340612 \tAccuracy =  6.25 %\n",
      "Iteration  4700 \tLoss =  8.730746294033455 \tAccuracy =  0.0 %\n",
      "Iteration  4800 \tLoss =  8.861600874476146 \tAccuracy =  3.125 %\n",
      "Iteration  4900 \tLoss =  9.007172387492798 \tAccuracy =  6.25 %\n",
      "Iteration  5000 \tLoss =  8.300221555099593 \tAccuracy =  3.125 %\n",
      "Iteration  5100 \tLoss =  8.414026256610562 \tAccuracy =  3.125 %\n",
      "Iteration  5200 \tLoss =  8.35358968245847 \tAccuracy =  3.125 %\n",
      "Iteration  5300 \tLoss =  8.312560061581124 \tAccuracy =  0.0 %\n",
      "Iteration  5400 \tLoss =  8.438284669697541 \tAccuracy =  6.25 %\n",
      "Iteration  5500 \tLoss =  8.026134812510517 \tAccuracy =  3.125 %\n",
      "Iteration  5600 \tLoss =  8.564048662663586 \tAccuracy =  3.125 %\n",
      "Iteration  5700 \tLoss =  8.478348706350333 \tAccuracy =  0.0 %\n",
      "Iteration  5800 \tLoss =  8.23872962492431 \tAccuracy =  0.0 %\n",
      "Iteration  5900 \tLoss =  8.615296733078878 \tAccuracy =  0.0 %\n",
      "Iteration  6000 \tLoss =  8.201669361638006 \tAccuracy =  0.0 %\n",
      "Iteration  6100 \tLoss =  8.453532907375118 \tAccuracy =  3.125 %\n",
      "Iteration  6200 \tLoss =  8.25432258285463 \tAccuracy =  0.0 %\n",
      "Iteration  6300 \tLoss =  8.122055356179658 \tAccuracy =  3.125 %\n",
      "Iteration  6400 \tLoss =  8.503595791909724 \tAccuracy =  0.0 %\n",
      "Iteration  6500 \tLoss =  8.10243887316502 \tAccuracy =  3.125 %\n",
      "Iteration  6600 \tLoss =  8.08542016259187 \tAccuracy =  6.25 %\n",
      "Iteration  6700 \tLoss =  8.280479707663913 \tAccuracy =  0.0 %\n",
      "Iteration  6800 \tLoss =  7.912623703790367 \tAccuracy =  0.0 %\n",
      "Iteration  6900 \tLoss =  7.81166299819251 \tAccuracy =  0.0 %\n",
      "Iteration  7000 \tLoss =  7.867322785634271 \tAccuracy =  0.0 %\n",
      "Iteration  7100 \tLoss =  7.968855587596676 \tAccuracy =  0.0 %\n",
      "Iteration  7200 \tLoss =  8.355377679138382 \tAccuracy =  0.0 %\n",
      "Iteration  7300 \tLoss =  7.840687049634024 \tAccuracy =  0.0 %\n",
      "Iteration  7400 \tLoss =  7.966301425356821 \tAccuracy =  0.0 %\n",
      "Iteration  7500 \tLoss =  7.695561609268898 \tAccuracy =  0.0 %\n",
      "Iteration  7600 \tLoss =  7.885301167535262 \tAccuracy =  3.125 %\n",
      "Iteration  7700 \tLoss =  8.050143035821224 \tAccuracy =  3.125 %\n",
      "Iteration  7800 \tLoss =  8.1860429241584 \tAccuracy =  0.0 %\n",
      "Iteration  7900 \tLoss =  7.842064288134958 \tAccuracy =  0.0 %\n",
      "Iteration  8000 \tLoss =  7.9920874276789515 \tAccuracy =  0.0 %\n",
      "Iteration  8100 \tLoss =  7.915097097064209 \tAccuracy =  3.125 %\n",
      "Iteration  8200 \tLoss =  7.878190965082773 \tAccuracy =  6.25 %\n",
      "Iteration  8300 \tLoss =  7.907938439601563 \tAccuracy =  3.125 %\n",
      "Iteration  8400 \tLoss =  7.797289774014883 \tAccuracy =  3.125 %\n",
      "Iteration  8500 \tLoss =  7.903680697298489 \tAccuracy =  3.125 %\n",
      "Iteration  8600 \tLoss =  7.500015471886705 \tAccuracy =  3.125 %\n",
      "Iteration  8700 \tLoss =  7.749633480330269 \tAccuracy =  0.0 %\n",
      "Iteration  8800 \tLoss =  7.91989393733507 \tAccuracy =  0.0 %\n",
      "Iteration  8900 \tLoss =  7.544619174212809 \tAccuracy =  0.0 %\n",
      "Iteration  9000 \tLoss =  7.56422201629934 \tAccuracy =  0.0 %\n",
      "Iteration  9100 \tLoss =  7.88246079766486 \tAccuracy =  0.0 %\n",
      "Iteration  9200 \tLoss =  7.290430865088748 \tAccuracy =  9.375 %\n",
      "Iteration  9300 \tLoss =  7.428080342928101 \tAccuracy =  6.25 %\n",
      "Iteration  9400 \tLoss =  7.444878949193979 \tAccuracy =  0.0 %\n",
      "Iteration  9500 \tLoss =  7.353540032417632 \tAccuracy =  3.125 %\n",
      "Iteration  9600 \tLoss =  7.543633681824791 \tAccuracy =  0.0 %\n",
      "Iteration  9700 \tLoss =  7.3749864090927755 \tAccuracy =  0.0 %\n",
      "Iteration  9800 \tLoss =  7.592698705722682 \tAccuracy =  0.0 %\n",
      "Iteration  9900 \tLoss =  7.54983130015769 \tAccuracy =  3.125 %\n"
     ]
    }
   ],
   "source": [
    "X = inpX.copy()\n",
    "input_size = X.shape[1]\n",
    "network = NeuralNetwork(input_size,output_size,hidden_layers_sizes,activation)\n",
    "network.train(X,Y.astype(int),batch_size,n0,max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = network.predict(X.copy())\n",
    "print(100 * np.sum(predictions == Y)/Y.shape[0])\n",
    "print(np.average(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = load_data(testing_data_path,avg,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = network.predict(testX)\n",
    "np.savetxt(output_path,predictions,fmt=\"%i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
