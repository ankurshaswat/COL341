{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "training_data_path = sys.argv[1]\n",
    "testing_data_path = sys.argv[2]\n",
    "output_path = sys.argv[3]\n",
    "batch_size = int(sys.argv[4])\n",
    "n0 = float(sys.argv[5])\n",
    "activation = sys.argv[6]\n",
    "hidden_layers_sizes = []\n",
    "for i in range(7,len(sys.argv)):\n",
    "    hidden_layers_sizes.append(int(sys.argv[i]))\n",
    "\n",
    "# training_data_path = \"../data/devnagri_train.csv\"\n",
    "# testing_data_path = \"../data/devnagri_test_public.csv\"\n",
    "# output_path = \"../data/nn/a/cs1160328.txt\"\n",
    "# batch_size = 512\n",
    "# n0 = 0.01\n",
    "# activation = 'sigmoid'\n",
    "# hidden_layers_sizes = [100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return (x>0) * x\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def reluPrime(x):\n",
    "    return (x>0)+0\n",
    "\n",
    "def tanhPrime(x):\n",
    "    return 1 - np.power(x,2)\n",
    "\n",
    "def sigmoidPrime(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def exp_normalize(x):\n",
    "    b = np.amax(x,axis=1,keepdims = True)\n",
    "    y = np.exp(x - b)\n",
    "    return y / y.sum(axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,input_size,output_size,hidden_layers_sizes, activation):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        if(activation == 'relu'):\n",
    "            self.activation = relu\n",
    "            self.activationPrime = reluPrime\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation = tanh\n",
    "            self.activationPrime = tanhPrime\n",
    "        else:\n",
    "            self.activation = sigmoid\n",
    "            self.activationPrime = sigmoidPrime\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hiddent_layers_sizes = hidden_layers_sizes\n",
    "        \n",
    "        prev_layer_count = input_size\n",
    "        \n",
    "        for i in range(len(hidden_layers_sizes) + 1):\n",
    "            if i==len(hidden_layers_sizes):\n",
    "                self.weights.append(np.random.rand(prev_layer_count, output_size)/100)\n",
    "                self.biases.append(np.random.rand(1, output_size)/100)        \n",
    "            else:\n",
    "                hidden_layer_count = hidden_layers_sizes[i]\n",
    "                self.weights.append(np.random.rand(prev_layer_count, hidden_layer_count)/100)\n",
    "                self.biases.append(np.random.rand(1, hidden_layer_count)/100)\n",
    "                prev_layer_count = hidden_layer_count\n",
    "        \n",
    "    def train(self,inpX,inpY,batch_size,n0,max_iterations):\n",
    "        max_examples = inpX.shape[0]\n",
    "        max_possible_iterations = int(0.5 + max_examples / batch_size)\n",
    "        num_hidden_layers = len(self.weights) - 1\n",
    "        \n",
    "        count = 0\n",
    "            \n",
    "        lr = n0\n",
    "        totLoss = 0\n",
    "        prevAvgLoss = sys.float_info.max\n",
    "        epoch = 0\n",
    "        \n",
    "        for n in range(max_iterations):\n",
    "            # Forming Mini Batches\n",
    "            i_eff = n%max_possible_iterations\n",
    "            \n",
    "            # Updating Learning Rate\n",
    "            if (i_eff == 0 and n!=0):\n",
    "                avgLoss = totLoss/max_possible_iterations\n",
    "                \n",
    "                if(np.absolute(avgLoss - prevAvgLoss) < 0.0001 * prevAvgLoss):\n",
    "                    stopCount += 1\n",
    "                    if stopCount > 1:\n",
    "                        break\n",
    "                else:\n",
    "                    stopCount = 0\n",
    "                if(avgLoss >= prevAvgLoss):\n",
    "                    count += 1\n",
    "                    lr = n0 / np.sqrt(count+1)\n",
    "                print(\"Epoch = \",epoch,\" Average Loss = \",avgLoss,\" New Learning Rate = \",lr)\n",
    "                epoch += 1\n",
    "                prevAvgLoss = avgLoss\n",
    "                totLoss = 0\n",
    "            \n",
    "            outputs = []\n",
    "            \n",
    "            if i_eff != max_possible_iterations - 1:\n",
    "                X = inpX[i_eff*batch_size: (i_eff+1)*batch_size]\n",
    "                Y = inpY[i_eff*batch_size: (i_eff+1)*batch_size]\n",
    "            else:\n",
    "                X = inpX[i_eff*batch_size:]\n",
    "                Y = inpY[i_eff*batch_size:]\n",
    "                \n",
    "            # Neural Network Forward Propagation\n",
    "            outputs.append(X)\n",
    "            prev_layer_output = X\n",
    "            for i in range(num_hidden_layers + 1):\n",
    "                weight = self.weights[i]\n",
    "                bias = self.biases[i]\n",
    "                if i == num_hidden_layers:\n",
    "                    prev_layer_output = sigmoid(prev_layer_output.dot(weight) + bias)\n",
    "                else:\n",
    "                    prev_layer_output = self.activation(prev_layer_output.dot(weight) + bias)\n",
    "                outputs.append(prev_layer_output)\n",
    "            \n",
    "            # Backpropagation\n",
    "            dWs = []\n",
    "            dbs = []\n",
    "            \n",
    "            y_onehot = np.zeros((Y.shape[0],self.output_size))\n",
    "            y_onehot[range(Y.shape[0]),Y] = 1\n",
    "            \n",
    "            for i in range(num_hidden_layers + 1,0,-1):\n",
    "                if i == num_hidden_layers + 1:\n",
    "                    delta = (outputs[i] - y_onehot).dot(2/Y.shape[0]) * sigmoidPrime(outputs[i])\n",
    "                else:\n",
    "                    delta = delta.dot(self.weights[i].T) * self.activationPrime(outputs[i])\n",
    "                dW = (outputs[i-1].T).dot(delta)\n",
    "                dWs.append(dW)\n",
    "                dbs.append(np.sum(delta,axis=0,keepdims=True))\n",
    "\n",
    "            if (n%100 == 0):\n",
    "                loss_ = np.sum(np.power(outputs[-1] - y_onehot,2) )/Y.shape[0]\n",
    "                labels_ = np.argmax(outputs[-1],axis = 1)\n",
    "                accuracy_ = 100 * np.sum(labels_ == Y)/Y.shape[0]\n",
    "                print(\"Iteration \",n,\"\\tLoss = \",loss_,\"\\tAccuracy = \",accuracy_,\"%\")\n",
    "                \n",
    "            dWs.reverse()\n",
    "            dbs.reverse()\n",
    "\n",
    "            # Gradient Descent Parameter Update\n",
    "            for i in range(len(dWs)):\n",
    "                self.weights[i] += dWs[i].dot(-1 * lr)\n",
    "                self.biases[i] += dbs[i].dot(-1 * lr)\n",
    "\n",
    "            loss = np.sum(np.power(outputs[-1] - y_onehot,2) )/Y.shape[0]\n",
    "            totLoss += loss\n",
    "                \n",
    "    def predict(self,X):\n",
    "        return self.forward_run(X)\n",
    "        \n",
    "    def forward_run(self,X):\n",
    "        prev_layer_output = X\n",
    "        num_hidden_layers = len(self.weights) - 1\n",
    "        for i in range(num_hidden_layers + 1):\n",
    "            weight = self.weights[i]\n",
    "            bias = self.biases[i]\n",
    "            if i == num_hidden_layers:\n",
    "                probabilities = sigmoid(prev_layer_output.dot(weight) + bias)\n",
    "                labels = np.argmax(probabilities,axis = 1)\n",
    "                return labels\n",
    "            else:\n",
    "                prev_layer_output = self.activation(prev_layer_output.dot(weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,avg,std):\n",
    "    if avg is None:\n",
    "        input_data = np.loadtxt(open(path, \"rb\"), delimiter=\",\")\n",
    "        Y = input_data[:,0].copy()\n",
    "        X = input_data[:,1:].copy()\n",
    "        avg = np.average(X,axis=0)\n",
    "        X = X - avg\n",
    "        std = np.std(X,axis=0)\n",
    "        std[(std == 0)] = 1\n",
    "        X = X / std\n",
    "        return X,Y,avg,std\n",
    "    else:\n",
    "        input_data = np.loadtxt(open(path, \"rb\"), delimiter=\",\")\n",
    "        X = input_data[:,1:].copy()\n",
    "        X = (X - avg)/std\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpX,Y,avg,std = load_data(training_data_path,None,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 \tLoss =  32.14825318333342 \tAccuracy =  2.34375 %\n",
      "Iteration  100 \tLoss =  32.53796718342765 \tAccuracy =  1.3671875 %\n",
      "Epoch =  0  Average Loss =  32.03512367346666  New Learning Rate =  0.01\n",
      "Iteration  200 \tLoss =  30.906488228664266 \tAccuracy =  1.5625 %\n",
      "Iteration  300 \tLoss =  30.2690481252686 \tAccuracy =  3.125 %\n",
      "Epoch =  1  Average Loss =  30.96630908246529  New Learning Rate =  0.01\n",
      "Iteration  400 \tLoss =  29.26547757561292 \tAccuracy =  1.5625 %\n",
      "Epoch =  2  Average Loss =  30.000979333473232  New Learning Rate =  0.01\n",
      "Iteration  500 \tLoss =  29.876958827903483 \tAccuracy =  1.171875 %\n",
      "Iteration  600 \tLoss =  29.36768140903809 \tAccuracy =  1.5625 %\n",
      "Epoch =  3  Average Loss =  29.157723298814044  New Learning Rate =  0.01\n",
      "Iteration  700 \tLoss =  27.47424872461346 \tAccuracy =  1.953125 %\n",
      "Epoch =  4  Average Loss =  28.439324679264715  New Learning Rate =  0.01\n",
      "Iteration  800 \tLoss =  27.121728266458874 \tAccuracy =  1.953125 %\n",
      "Iteration  900 \tLoss =  27.557161457289208 \tAccuracy =  2.34375 %\n",
      "Epoch =  5  Average Loss =  27.83712512664878  New Learning Rate =  0.01\n",
      "Iteration  1000 \tLoss =  25.817963369761834 \tAccuracy =  2.34375 %\n",
      "Epoch =  6  Average Loss =  27.336395094281322  New Learning Rate =  0.01\n",
      "Iteration  1100 \tLoss =  25.486986369423438 \tAccuracy =  1.5625 %\n",
      "Iteration  1200 \tLoss =  27.463130236777587 \tAccuracy =  2.34375 %\n",
      "Epoch =  7  Average Loss =  26.920671370499317  New Learning Rate =  0.01\n",
      "Iteration  1300 \tLoss =  26.03908607517313 \tAccuracy =  2.5390625 %\n",
      "Epoch =  8  Average Loss =  26.57439702979938  New Learning Rate =  0.01\n",
      "Iteration  1400 \tLoss =  26.893592301228413 \tAccuracy =  2.5390625 %\n",
      "Iteration  1500 \tLoss =  26.43278081589881 \tAccuracy =  2.5390625 %\n",
      "Epoch =  9  Average Loss =  26.284118987445755  New Learning Rate =  0.01\n",
      "Iteration  1600 \tLoss =  25.544100864963333 \tAccuracy =  1.953125 %\n",
      "Epoch =  10  Average Loss =  26.038769769835064  New Learning Rate =  0.01\n",
      "Iteration  1700 \tLoss =  25.61198693656795 \tAccuracy =  3.3203125 %\n",
      "Iteration  1800 \tLoss =  26.3818960502181 \tAccuracy =  0.9765625 %\n",
      "Epoch =  11  Average Loss =  25.829489766784945  New Learning Rate =  0.01\n",
      "Iteration  1900 \tLoss =  26.901203360626475 \tAccuracy =  1.953125 %\n",
      "Epoch =  12  Average Loss =  25.649284053422083  New Learning Rate =  0.01\n",
      "Iteration  2000 \tLoss =  24.53980436133301 \tAccuracy =  1.7578125 %\n",
      "Iteration  2100 \tLoss =  23.843319123999756 \tAccuracy =  2.734375 %\n",
      "Epoch =  13  Average Loss =  25.492658994568288  New Learning Rate =  0.01\n",
      "Iteration  2200 \tLoss =  25.8863609174252 \tAccuracy =  2.5390625 %\n",
      "Epoch =  14  Average Loss =  25.355298867521043  New Learning Rate =  0.01\n",
      "Iteration  2300 \tLoss =  24.065898784514463 \tAccuracy =  1.7578125 %\n",
      "Iteration  2400 \tLoss =  25.805139013452816 \tAccuracy =  2.5390625 %\n",
      "Epoch =  15  Average Loss =  25.233802350727018  New Learning Rate =  0.01\n",
      "Iteration  2500 \tLoss =  23.77890282338452 \tAccuracy =  1.953125 %\n",
      "Iteration  2600 \tLoss =  27.888625254876324 \tAccuracy =  1.5957446808510638 %\n",
      "Epoch =  16  Average Loss =  25.125475843780162  New Learning Rate =  0.01\n",
      "Iteration  2700 \tLoss =  25.494016799224696 \tAccuracy =  1.953125 %\n",
      "Epoch =  17  Average Loss =  25.02817330530103  New Learning Rate =  0.01\n",
      "Iteration  2800 \tLoss =  27.004200858956494 \tAccuracy =  2.1484375 %\n",
      "Iteration  2900 \tLoss =  24.264428348973166 \tAccuracy =  1.7578125 %\n",
      "Epoch =  18  Average Loss =  24.940173312527353  New Learning Rate =  0.01\n",
      "Iteration  3000 \tLoss =  25.62323586049407 \tAccuracy =  2.34375 %\n",
      "Epoch =  19  Average Loss =  24.860085482935702  New Learning Rate =  0.01\n",
      "Iteration  3100 \tLoss =  25.550435546626222 \tAccuracy =  1.953125 %\n",
      "Iteration  3200 \tLoss =  23.09845499171248 \tAccuracy =  2.734375 %\n",
      "Epoch =  20  Average Loss =  24.786779328826988  New Learning Rate =  0.01\n",
      "Iteration  3300 \tLoss =  24.671318712336237 \tAccuracy =  1.7578125 %\n",
      "Epoch =  21  Average Loss =  24.719329756068877  New Learning Rate =  0.01\n",
      "Iteration  3400 \tLoss =  24.452649779028043 \tAccuracy =  3.125 %\n",
      "Iteration  3500 \tLoss =  25.67436035693956 \tAccuracy =  2.734375 %\n",
      "Epoch =  22  Average Loss =  24.656974952760756  New Learning Rate =  0.01\n",
      "Iteration  3600 \tLoss =  23.899886822575844 \tAccuracy =  1.953125 %\n",
      "Epoch =  23  Average Loss =  24.5990836527612  New Learning Rate =  0.01\n",
      "Iteration  3700 \tLoss =  26.258948468054644 \tAccuracy =  2.734375 %\n",
      "Iteration  3800 \tLoss =  25.146781125738784 \tAccuracy =  3.7109375 %\n",
      "Epoch =  24  Average Loss =  24.545129359455288  New Learning Rate =  0.01\n",
      "Iteration  3900 \tLoss =  24.912618375799447 \tAccuracy =  3.125 %\n",
      "Epoch =  25  Average Loss =  24.49466966994934  New Learning Rate =  0.01\n",
      "Iteration  4000 \tLoss =  23.780087637832008 \tAccuracy =  2.34375 %\n",
      "Iteration  4100 \tLoss =  25.99748269943364 \tAccuracy =  3.3203125 %\n",
      "Epoch =  26  Average Loss =  24.447329571933313  New Learning Rate =  0.01\n",
      "Iteration  4200 \tLoss =  23.837113483839758 \tAccuracy =  1.171875 %\n",
      "Epoch =  27  Average Loss =  24.40278812716953  New Learning Rate =  0.01\n",
      "Iteration  4300 \tLoss =  25.11901580856763 \tAccuracy =  2.5390625 %\n",
      "Iteration  4400 \tLoss =  23.65661661784118 \tAccuracy =  3.125 %\n",
      "Epoch =  28  Average Loss =  24.36076814490779  New Learning Rate =  0.01\n",
      "Iteration  4500 \tLoss =  23.24731282160872 \tAccuracy =  0.78125 %\n",
      "Epoch =  29  Average Loss =  24.321028417200782  New Learning Rate =  0.01\n",
      "Iteration  4600 \tLoss =  23.371967426190295 \tAccuracy =  1.7578125 %\n",
      "Iteration  4700 \tLoss =  24.351669571470016 \tAccuracy =  1.171875 %\n",
      "Epoch =  30  Average Loss =  24.283357957730324  New Learning Rate =  0.01\n",
      "Iteration  4800 \tLoss =  24.172509929022013 \tAccuracy =  2.734375 %\n",
      "Epoch =  31  Average Loss =  24.247571556131753  New Learning Rate =  0.01\n",
      "Iteration  4900 \tLoss =  24.72647398881061 \tAccuracy =  2.1484375 %\n",
      "Iteration  5000 \tLoss =  25.553361061442086 \tAccuracy =  1.953125 %\n",
      "Epoch =  32  Average Loss =  24.21350600628024  New Learning Rate =  0.01\n",
      "Iteration  5100 \tLoss =  24.465847275816394 \tAccuracy =  1.953125 %\n",
      "Iteration  5200 \tLoss =  24.638232074635297 \tAccuracy =  2.5390625 %\n",
      "Epoch =  33  Average Loss =  24.18101669527766  New Learning Rate =  0.01\n",
      "Iteration  5300 \tLoss =  24.69816543115806 \tAccuracy =  1.5625 %\n",
      "Epoch =  34  Average Loss =  24.149974635038472  New Learning Rate =  0.01\n",
      "Iteration  5400 \tLoss =  22.86277733718009 \tAccuracy =  2.34375 %\n",
      "Iteration  5500 \tLoss =  23.13754057720742 \tAccuracy =  2.34375 %\n",
      "Epoch =  35  Average Loss =  24.12026411872298  New Learning Rate =  0.01\n",
      "Iteration  5600 \tLoss =  23.59862906054122 \tAccuracy =  2.34375 %\n",
      "Epoch =  36  Average Loss =  24.091780972538036  New Learning Rate =  0.01\n",
      "Iteration  5700 \tLoss =  23.001092149536024 \tAccuracy =  2.34375 %\n",
      "Iteration  5800 \tLoss =  24.58641245619397 \tAccuracy =  1.7578125 %\n",
      "Epoch =  37  Average Loss =  24.064431180369493  New Learning Rate =  0.01\n",
      "Iteration  5900 \tLoss =  24.351491023612606 \tAccuracy =  2.1484375 %\n",
      "Epoch =  38  Average Loss =  24.038129687548103  New Learning Rate =  0.01\n",
      "Iteration  6000 \tLoss =  24.53319050573929 \tAccuracy =  2.34375 %\n",
      "Iteration  6100 \tLoss =  24.746443047486643 \tAccuracy =  1.5625 %\n",
      "Epoch =  39  Average Loss =  24.012799338217015  New Learning Rate =  0.01\n",
      "Iteration  6200 \tLoss =  24.896243303271596 \tAccuracy =  3.515625 %\n",
      "Epoch =  40  Average Loss =  23.988369989842464  New Learning Rate =  0.01\n",
      "Iteration  6300 \tLoss =  23.127095100606823 \tAccuracy =  3.90625 %\n",
      "Iteration  6400 \tLoss =  23.532609802785412 \tAccuracy =  3.3203125 %\n",
      "Epoch =  41  Average Loss =  23.964777823252263  New Learning Rate =  0.01\n",
      "Iteration  6500 \tLoss =  23.68114799693165 \tAccuracy =  2.734375 %\n",
      "Epoch =  42  Average Loss =  23.941964794070632  New Learning Rate =  0.01\n",
      "Iteration  6600 \tLoss =  25.21018933978104 \tAccuracy =  2.5390625 %\n",
      "Iteration  6700 \tLoss =  22.63769857079599 \tAccuracy =  4.296875 %\n",
      "Epoch =  43  Average Loss =  23.919878140983844  New Learning Rate =  0.01\n",
      "Iteration  6800 \tLoss =  25.048608449332228 \tAccuracy =  1.953125 %\n",
      "Epoch =  44  Average Loss =  23.898469905225234  New Learning Rate =  0.01\n",
      "Iteration  6900 \tLoss =  22.925834742799136 \tAccuracy =  1.171875 %\n",
      "Iteration  7000 \tLoss =  23.63907402501232 \tAccuracy =  1.7578125 %\n",
      "Epoch =  45  Average Loss =  23.877696478815256  New Learning Rate =  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  7100 \tLoss =  25.135895192552248 \tAccuracy =  3.90625 %\n",
      "Epoch =  46  Average Loss =  23.857518226825842  New Learning Rate =  0.01\n",
      "Iteration  7200 \tLoss =  22.47239208684573 \tAccuracy =  1.5625 %\n",
      "Iteration  7300 \tLoss =  25.300112831627818 \tAccuracy =  1.7578125 %\n",
      "Epoch =  47  Average Loss =  23.837899205602085  New Learning Rate =  0.01\n",
      "Iteration  7400 \tLoss =  24.018893552691456 \tAccuracy =  1.3671875 %\n",
      "Epoch =  48  Average Loss =  23.81880695399596  New Learning Rate =  0.01\n",
      "Iteration  7500 \tLoss =  23.27858026505802 \tAccuracy =  3.3203125 %\n",
      "Iteration  7600 \tLoss =  22.436052800270716 \tAccuracy =  3.7109375 %\n",
      "Epoch =  49  Average Loss =  23.800212303005107  New Learning Rate =  0.01\n",
      "Iteration  7700 \tLoss =  23.566575145355547 \tAccuracy =  2.34375 %\n",
      "Iteration  7800 \tLoss =  22.723771322000825 \tAccuracy =  2.1484375 %\n",
      "Epoch =  50  Average Loss =  23.782089143711186  New Learning Rate =  0.01\n",
      "Iteration  7900 \tLoss =  23.30911408919691 \tAccuracy =  3.90625 %\n",
      "Epoch =  51  Average Loss =  23.76441410993207  New Learning Rate =  0.01\n",
      "Iteration  8000 \tLoss =  22.18162834289764 \tAccuracy =  2.9296875 %\n",
      "Iteration  8100 \tLoss =  22.982356281975292 \tAccuracy =  1.953125 %\n",
      "Epoch =  52  Average Loss =  23.747166163033228  New Learning Rate =  0.01\n",
      "Iteration  8200 \tLoss =  24.28810863575001 \tAccuracy =  2.734375 %\n",
      "Epoch =  53  Average Loss =  23.730326103769517  New Learning Rate =  0.01\n",
      "Iteration  8300 \tLoss =  23.44720013574856 \tAccuracy =  2.5390625 %\n",
      "Iteration  8400 \tLoss =  23.14559877041662 \tAccuracy =  2.1484375 %\n",
      "Epoch =  54  Average Loss =  23.71387606742758  New Learning Rate =  0.01\n",
      "Iteration  8500 \tLoss =  23.488610552661967 \tAccuracy =  1.953125 %\n",
      "Epoch =  55  Average Loss =  23.697799070159537  New Learning Rate =  0.01\n",
      "Iteration  8600 \tLoss =  22.53543411881432 \tAccuracy =  2.5390625 %\n",
      "Iteration  8700 \tLoss =  24.48525153776859 \tAccuracy =  2.9296875 %\n",
      "Epoch =  56  Average Loss =  23.682078661710136  New Learning Rate =  0.01\n",
      "Iteration  8800 \tLoss =  23.32855561930353 \tAccuracy =  2.34375 %\n",
      "Epoch =  57  Average Loss =  23.666698712681413  New Learning Rate =  0.01\n",
      "Iteration  8900 \tLoss =  24.947515536105406 \tAccuracy =  1.171875 %\n",
      "Iteration  9000 \tLoss =  24.57148366452764 \tAccuracy =  2.734375 %\n",
      "Epoch =  58  Average Loss =  23.651643338492523  New Learning Rate =  0.01\n",
      "Iteration  9100 \tLoss =  22.884277721361627 \tAccuracy =  2.9296875 %\n",
      "Epoch =  59  Average Loss =  23.636896944452843  New Learning Rate =  0.01\n",
      "Iteration  9200 \tLoss =  23.238117775951537 \tAccuracy =  2.734375 %\n",
      "Iteration  9300 \tLoss =  21.73438064539082 \tAccuracy =  2.734375 %\n",
      "Epoch =  60  Average Loss =  23.62244436365008  New Learning Rate =  0.01\n",
      "Iteration  9400 \tLoss =  24.904993460188287 \tAccuracy =  3.125 %\n",
      "Epoch =  61  Average Loss =  23.60827104840707  New Learning Rate =  0.01\n",
      "Iteration  9500 \tLoss =  23.186830940848147 \tAccuracy =  3.7109375 %\n",
      "Iteration  9600 \tLoss =  21.905756734905502 \tAccuracy =  2.1484375 %\n",
      "Epoch =  62  Average Loss =  23.594363271011034  New Learning Rate =  0.01\n",
      "Iteration  9700 \tLoss =  23.297531871608587 \tAccuracy =  3.90625 %\n",
      "Epoch =  63  Average Loss =  23.58070829657922  New Learning Rate =  0.01\n",
      "Iteration  9800 \tLoss =  24.08580162150104 \tAccuracy =  3.3203125 %\n",
      "Iteration  9900 \tLoss =  23.13957810890536 \tAccuracy =  3.125 %\n",
      "Epoch =  64  Average Loss =  23.567294508751907  New Learning Rate =  0.01\n",
      "Iteration  10000 \tLoss =  23.899889329901725 \tAccuracy =  1.3671875 %\n",
      "Epoch =  65  Average Loss =  23.55411148632061  New Learning Rate =  0.01\n",
      "Iteration  10100 \tLoss =  23.169658950214853 \tAccuracy =  3.515625 %\n",
      "Iteration  10200 \tLoss =  23.355808067026373 \tAccuracy =  2.734375 %\n",
      "Epoch =  66  Average Loss =  23.54115003427432  New Learning Rate =  0.01\n",
      "Iteration  10300 \tLoss =  25.055926230289316 \tAccuracy =  2.734375 %\n",
      "Iteration  10400 \tLoss =  23.43617147891127 \tAccuracy =  1.953125 %\n",
      "Epoch =  67  Average Loss =  23.52840216429312  New Learning Rate =  0.01\n",
      "Iteration  10500 \tLoss =  23.531427652464107 \tAccuracy =  2.34375 %\n",
      "Epoch =  68  Average Loss =  23.515861007351877  New Learning Rate =  0.01\n",
      "Iteration  10600 \tLoss =  24.170459555322665 \tAccuracy =  2.5390625 %\n",
      "Iteration  10700 \tLoss =  24.508295248501728 \tAccuracy =  2.734375 %\n",
      "Epoch =  69  Average Loss =  23.50352063867579  New Learning Rate =  0.01\n",
      "Iteration  10800 \tLoss =  22.805540814777128 \tAccuracy =  2.5390625 %\n",
      "Epoch =  70  Average Loss =  23.49137581022817  New Learning Rate =  0.01\n",
      "Iteration  10900 \tLoss =  23.41110601971466 \tAccuracy =  1.953125 %\n",
      "Iteration  11000 \tLoss =  26.498978363222783 \tAccuracy =  1.171875 %\n",
      "Epoch =  71  Average Loss =  23.479421613700552  New Learning Rate =  0.01\n",
      "Iteration  11100 \tLoss =  22.98346137910389 \tAccuracy =  2.34375 %\n",
      "Epoch =  72  Average Loss =  23.467653123724894  New Learning Rate =  0.01\n",
      "Iteration  11200 \tLoss =  24.552421309345622 \tAccuracy =  1.5625 %\n",
      "Iteration  11300 \tLoss =  22.629383646039756 \tAccuracy =  2.1484375 %\n",
      "Epoch =  73  Average Loss =  23.4560650821742  New Learning Rate =  0.01\n",
      "Iteration  11400 \tLoss =  24.043918320242604 \tAccuracy =  3.3203125 %\n",
      "Epoch =  74  Average Loss =  23.44465167415229  New Learning Rate =  0.01\n",
      "Iteration  11500 \tLoss =  22.746449315037097 \tAccuracy =  2.1484375 %\n",
      "Iteration  11600 \tLoss =  23.45436211952569 \tAccuracy =  1.7578125 %\n",
      "Epoch =  75  Average Loss =  23.433406420939914  New Learning Rate =  0.01\n",
      "Iteration  11700 \tLoss =  23.371098758284603 \tAccuracy =  3.125 %\n",
      "Epoch =  76  Average Loss =  23.42232218800344  New Learning Rate =  0.01\n",
      "Iteration  11800 \tLoss =  24.184875720600747 \tAccuracy =  1.7578125 %\n",
      "Iteration  11900 \tLoss =  22.909161359940406 \tAccuracy =  1.7578125 %\n",
      "Epoch =  77  Average Loss =  23.411391288065243  New Learning Rate =  0.01\n",
      "Iteration  12000 \tLoss =  25.09890206351001 \tAccuracy =  2.5390625 %\n",
      "Epoch =  78  Average Loss =  23.40060565304158  New Learning Rate =  0.01\n",
      "Iteration  12100 \tLoss =  22.41620037352945 \tAccuracy =  4.1015625 %\n",
      "Iteration  12200 \tLoss =  23.94768024767356 \tAccuracy =  1.953125 %\n",
      "Epoch =  79  Average Loss =  23.38995705034192  New Learning Rate =  0.01\n",
      "Iteration  12300 \tLoss =  22.590964773656772 \tAccuracy =  3.7109375 %\n",
      "Epoch =  80  Average Loss =  23.37943732261767  New Learning Rate =  0.01\n",
      "Iteration  12400 \tLoss =  23.271861658776853 \tAccuracy =  2.734375 %\n",
      "Iteration  12500 \tLoss =  24.685134954300782 \tAccuracy =  2.5390625 %\n",
      "Epoch =  81  Average Loss =  23.369038631814853  New Learning Rate =  0.01\n",
      "Iteration  12600 \tLoss =  24.981516158865176 \tAccuracy =  2.5390625 %\n",
      "Epoch =  82  Average Loss =  23.358753688074255  New Learning Rate =  0.01\n",
      "Iteration  12700 \tLoss =  24.537800461193967 \tAccuracy =  2.1484375 %\n",
      "Iteration  12800 \tLoss =  23.405588024177284 \tAccuracy =  2.734375 %\n",
      "Epoch =  83  Average Loss =  23.34857594368743  New Learning Rate =  0.01\n",
      "Iteration  12900 \tLoss =  22.952859072136896 \tAccuracy =  1.953125 %\n",
      "Iteration  13000 \tLoss =  23.280638994212044 \tAccuracy =  3.125 %\n",
      "Epoch =  84  Average Loss =  23.338499734306478  New Learning Rate =  0.01\n",
      "Iteration  13100 \tLoss =  23.279490578468288 \tAccuracy =  3.125 %\n",
      "Epoch =  85  Average Loss =  23.328520355119448  New Learning Rate =  0.01\n",
      "Iteration  13200 \tLoss =  21.116671938274326 \tAccuracy =  1.953125 %\n",
      "Iteration  13300 \tLoss =  24.149578894964158 \tAccuracy =  3.125 %\n",
      "Epoch =  86  Average Loss =  23.31863406838005  New Learning Rate =  0.01\n",
      "Iteration  13400 \tLoss =  23.65855620429857 \tAccuracy =  3.125 %\n",
      "Epoch =  87  Average Loss =  23.30883804889832  New Learning Rate =  0.01\n",
      "Iteration  13500 \tLoss =  23.237455106148094 \tAccuracy =  2.34375 %\n",
      "Iteration  13600 \tLoss =  23.994770908384872 \tAccuracy =  2.1484375 %\n",
      "Epoch =  88  Average Loss =  23.299130283401194  New Learning Rate =  0.01\n",
      "Iteration  13700 \tLoss =  23.68852843116821 \tAccuracy =  1.3671875 %\n",
      "Epoch =  89  Average Loss =  23.28950944528035  New Learning Rate =  0.01\n",
      "Iteration  13800 \tLoss =  23.390738086241853 \tAccuracy =  1.953125 %\n",
      "Iteration  13900 \tLoss =  23.38614999003339 \tAccuracy =  2.34375 %\n",
      "Epoch =  90  Average Loss =  23.279974766141123  New Learning Rate =  0.01\n",
      "Iteration  14000 \tLoss =  24.64885578036973 \tAccuracy =  2.34375 %\n",
      "Epoch =  91  Average Loss =  23.270525919936404  New Learning Rate =  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  14100 \tLoss =  22.318556031526512 \tAccuracy =  2.5390625 %\n",
      "Iteration  14200 \tLoss =  24.007385643664218 \tAccuracy =  3.90625 %\n",
      "Epoch =  92  Average Loss =  23.26116292710654  New Learning Rate =  0.01\n",
      "Iteration  14300 \tLoss =  23.425326557741386 \tAccuracy =  3.125 %\n",
      "Epoch =  93  Average Loss =  23.25188607886828  New Learning Rate =  0.01\n",
      "Iteration  14400 \tLoss =  22.44001997848064 \tAccuracy =  2.9296875 %\n",
      "Iteration  14500 \tLoss =  24.478083427976475 \tAccuracy =  0.9765625 %\n",
      "Epoch =  94  Average Loss =  23.242695878555626  New Learning Rate =  0.01\n",
      "Iteration  14600 \tLoss =  23.443098244319867 \tAccuracy =  2.34375 %\n",
      "Epoch =  95  Average Loss =  23.23359299915998  New Learning Rate =  0.01\n",
      "Iteration  14700 \tLoss =  22.13950428142427 \tAccuracy =  1.953125 %\n",
      "Iteration  14800 \tLoss =  24.182682060565664 \tAccuracy =  2.5390625 %\n",
      "Epoch =  96  Average Loss =  23.224578263207825  New Learning Rate =  0.01\n",
      "Iteration  14900 \tLoss =  21.837084947725906 \tAccuracy =  2.5390625 %\n",
      "Epoch =  97  Average Loss =  23.21565265818582  New Learning Rate =  0.01\n",
      "Iteration  15000 \tLoss =  23.61328178808199 \tAccuracy =  3.7109375 %\n",
      "Iteration  15100 \tLoss =  23.611615045596203 \tAccuracy =  2.734375 %\n",
      "Epoch =  98  Average Loss =  23.206817399944104  New Learning Rate =  0.01\n",
      "Iteration  15200 \tLoss =  21.19127964761958 \tAccuracy =  2.9296875 %\n",
      "Epoch =  99  Average Loss =  23.198074042759846  New Learning Rate =  0.01\n",
      "Iteration  15300 \tLoss =  22.098908323845077 \tAccuracy =  2.1484375 %\n",
      "Iteration  15400 \tLoss =  23.982719163699304 \tAccuracy =  2.34375 %\n",
      "Epoch =  100  Average Loss =  23.18942461222467  New Learning Rate =  0.01\n",
      "Iteration  15500 \tLoss =  23.120669213990166 \tAccuracy =  3.7109375 %\n",
      "Iteration  15600 \tLoss =  22.87081219830288 \tAccuracy =  3.515625 %\n",
      "Epoch =  101  Average Loss =  23.180871718009676  New Learning Rate =  0.01\n",
      "Iteration  15700 \tLoss =  22.30589558324263 \tAccuracy =  3.90625 %\n",
      "Epoch =  102  Average Loss =  23.17241859913559  New Learning Rate =  0.01\n",
      "Iteration  15800 \tLoss =  24.43265190043306 \tAccuracy =  1.7578125 %\n",
      "Iteration  15900 \tLoss =  23.86392252419845 \tAccuracy =  3.125 %\n",
      "Epoch =  103  Average Loss =  23.16406906744396  New Learning Rate =  0.01\n",
      "Iteration  16000 \tLoss =  22.385259776279753 \tAccuracy =  2.1484375 %\n",
      "Epoch =  104  Average Loss =  23.155827339692937  New Learning Rate =  0.01\n",
      "Iteration  16100 \tLoss =  22.339722000322812 \tAccuracy =  1.953125 %\n",
      "Iteration  16200 \tLoss =  22.448183053096635 \tAccuracy =  1.5625 %\n",
      "Epoch =  105  Average Loss =  23.14769777556196  New Learning Rate =  0.01\n",
      "Iteration  16300 \tLoss =  21.632114891390355 \tAccuracy =  1.7578125 %\n",
      "Epoch =  106  Average Loss =  23.13968455962491  New Learning Rate =  0.01\n",
      "Iteration  16400 \tLoss =  21.288425297370395 \tAccuracy =  2.1484375 %\n",
      "Iteration  16500 \tLoss =  24.147469261158705 \tAccuracy =  2.34375 %\n",
      "Epoch =  107  Average Loss =  23.131791375602624  New Learning Rate =  0.01\n",
      "Iteration  16600 \tLoss =  22.329171910205787 \tAccuracy =  3.125 %\n",
      "Epoch =  108  Average Loss =  23.12402112039754  New Learning Rate =  0.01\n",
      "Iteration  16700 \tLoss =  23.562969627725707 \tAccuracy =  1.171875 %\n",
      "Iteration  16800 \tLoss =  23.524336310052625 \tAccuracy =  3.3203125 %\n",
      "Epoch =  109  Average Loss =  23.116375695577336  New Learning Rate =  0.01\n",
      "Iteration  16900 \tLoss =  22.507831560885858 \tAccuracy =  2.1484375 %\n",
      "Epoch =  110  Average Loss =  23.10885589802488  New Learning Rate =  0.01\n",
      "Iteration  17000 \tLoss =  22.23068557521951 \tAccuracy =  2.5390625 %\n",
      "Iteration  17100 \tLoss =  23.613582339462177 \tAccuracy =  1.953125 %\n",
      "Epoch =  111  Average Loss =  23.101461412248934  New Learning Rate =  0.01\n",
      "Iteration  17200 \tLoss =  24.20522366460482 \tAccuracy =  3.515625 %\n",
      "Epoch =  112  Average Loss =  23.094190887530637  New Learning Rate =  0.01\n",
      "Iteration  17300 \tLoss =  21.78819777706437 \tAccuracy =  2.734375 %\n",
      "Iteration  17400 \tLoss =  21.344809575560376 \tAccuracy =  3.125 %\n",
      "Epoch =  113  Average Loss =  23.087042068211126  New Learning Rate =  0.01\n",
      "Iteration  17500 \tLoss =  22.93848579815472 \tAccuracy =  3.3203125 %\n",
      "Epoch =  114  Average Loss =  23.080011940463688  New Learning Rate =  0.01\n",
      "Iteration  17600 \tLoss =  21.57841134689336 \tAccuracy =  2.5390625 %\n",
      "Iteration  17700 \tLoss =  23.154606084607575 \tAccuracy =  2.734375 %\n",
      "Epoch =  115  Average Loss =  23.07309686692856  New Learning Rate =  0.01\n",
      "Iteration  17800 \tLoss =  22.144129434074088 \tAccuracy =  2.5390625 %\n",
      "Iteration  17900 \tLoss =  26.13987352753067 \tAccuracy =  1.5957446808510638 %\n",
      "Epoch =  116  Average Loss =  23.066292698435255  New Learning Rate =  0.01\n",
      "Iteration  18000 \tLoss =  23.68082346364993 \tAccuracy =  2.734375 %\n",
      "Epoch =  117  Average Loss =  23.059594870237053  New Learning Rate =  0.01\n",
      "Iteration  18100 \tLoss =  25.230065502190882 \tAccuracy =  3.125 %\n",
      "Iteration  18200 \tLoss =  22.524927415787445 \tAccuracy =  3.125 %\n",
      "Epoch =  118  Average Loss =  23.052998498728865  New Learning Rate =  0.01\n",
      "Iteration  18300 \tLoss =  23.235151373463943 \tAccuracy =  2.1484375 %\n",
      "Epoch =  119  Average Loss =  23.04649849021249  New Learning Rate =  0.01\n",
      "Iteration  18400 \tLoss =  23.630941042675467 \tAccuracy =  1.953125 %\n",
      "Iteration  18500 \tLoss =  21.785106287810216 \tAccuracy =  1.3671875 %\n",
      "Epoch =  120  Average Loss =  23.040089661289603  New Learning Rate =  0.01\n",
      "Iteration  18600 \tLoss =  22.933985607730076 \tAccuracy =  1.7578125 %\n",
      "Epoch =  121  Average Loss =  23.03376685985648  New Learning Rate =  0.01\n",
      "Iteration  18700 \tLoss =  22.71996587622201 \tAccuracy =  1.3671875 %\n",
      "Iteration  18800 \tLoss =  24.392354384116896 \tAccuracy =  2.734375 %\n",
      "Epoch =  122  Average Loss =  23.027525072149636  New Learning Rate =  0.01\n",
      "Iteration  18900 \tLoss =  21.9502799472679 \tAccuracy =  2.1484375 %\n",
      "Epoch =  123  Average Loss =  23.021359504815344  New Learning Rate =  0.01\n",
      "Iteration  19000 \tLoss =  24.410492751484597 \tAccuracy =  2.734375 %\n",
      "Iteration  19100 \tLoss =  23.584615311948756 \tAccuracy =  3.7109375 %\n",
      "Epoch =  124  Average Loss =  23.01526563802959  New Learning Rate =  0.01\n",
      "Iteration  19200 \tLoss =  23.378298754543046 \tAccuracy =  2.9296875 %\n",
      "Epoch =  125  Average Loss =  23.009239252561944  New Learning Rate =  0.01\n",
      "Iteration  19300 \tLoss =  22.25124166299667 \tAccuracy =  2.34375 %\n",
      "Iteration  19400 \tLoss =  24.137919042437982 \tAccuracy =  2.5390625 %\n",
      "Epoch =  126  Average Loss =  23.003276438142134  New Learning Rate =  0.01\n",
      "Iteration  19500 \tLoss =  22.223281595428194 \tAccuracy =  1.171875 %\n",
      "Epoch =  127  Average Loss =  22.99737359192912  New Learning Rate =  0.01\n",
      "Iteration  19600 \tLoss =  23.97668965374062 \tAccuracy =  2.34375 %\n",
      "Iteration  19700 \tLoss =  22.73248005156692 \tAccuracy =  3.125 %\n",
      "Epoch =  128  Average Loss =  22.991527414731404  New Learning Rate =  0.01\n",
      "Iteration  19800 \tLoss =  21.655242104879367 \tAccuracy =  0.78125 %\n",
      "Epoch =  129  Average Loss =  22.985734909851022  New Learning Rate =  0.01\n",
      "Iteration  19900 \tLoss =  22.62899983612002 \tAccuracy =  1.953125 %\n",
      "Iteration  20000 \tLoss =  23.033506922943324 \tAccuracy =  0.9765625 %\n",
      "Epoch =  130  Average Loss =  22.979993386119197  New Learning Rate =  0.01\n",
      "Iteration  20100 \tLoss =  23.20009621198355 \tAccuracy =  2.34375 %\n",
      "Epoch =  131  Average Loss =  22.974300463810458  New Learning Rate =  0.01\n",
      "Iteration  20200 \tLoss =  23.244292367669672 \tAccuracy =  2.1484375 %\n",
      "Iteration  20300 \tLoss =  24.49690493321271 \tAccuracy =  1.7578125 %\n",
      "Epoch =  132  Average Loss =  22.96865408028159  New Learning Rate =  0.01\n",
      "Iteration  20400 \tLoss =  23.300371800010673 \tAccuracy =  1.7578125 %\n",
      "Iteration  20500 \tLoss =  23.746715599115205 \tAccuracy =  2.5390625 %\n",
      "Epoch =  133  Average Loss =  22.963052491620992  New Learning Rate =  0.01\n",
      "Iteration  20600 \tLoss =  23.854903479204214 \tAccuracy =  1.5625 %\n",
      "Epoch =  134  Average Loss =  22.957494267159106  New Learning Rate =  0.01\n",
      "Iteration  20700 \tLoss =  21.580517780441873 \tAccuracy =  2.1484375 %\n",
      "Iteration  20800 \tLoss =  21.965334659441545 \tAccuracy =  2.1484375 %\n",
      "Epoch =  135  Average Loss =  22.95197827495644  New Learning Rate =  0.01\n",
      "Iteration  20900 \tLoss =  22.728672693303917 \tAccuracy =  2.34375 %\n",
      "Epoch =  136  Average Loss =  22.946503657816393  New Learning Rate =  0.01\n",
      "Iteration  21000 \tLoss =  21.657508227309936 \tAccuracy =  2.34375 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  21100 \tLoss =  23.396424175976424 \tAccuracy =  2.34375 %\n",
      "Epoch =  137  Average Loss =  22.941069800543332  New Learning Rate =  0.01\n",
      "Iteration  21200 \tLoss =  23.25852805530915 \tAccuracy =  1.953125 %\n",
      "Epoch =  138  Average Loss =  22.93567628993503  New Learning Rate =  0.01\n",
      "Iteration  21300 \tLoss =  23.089763997239363 \tAccuracy =  1.953125 %\n",
      "Iteration  21400 \tLoss =  24.00337204654811 \tAccuracy =  1.953125 %\n",
      "Epoch =  139  Average Loss =  22.930322869460515  New Learning Rate =  0.01\n",
      "Iteration  21500 \tLoss =  24.28788030075738 \tAccuracy =  3.3203125 %\n",
      "Epoch =  140  Average Loss =  22.925009390907483  New Learning Rate =  0.01\n",
      "Iteration  21600 \tLoss =  22.042893010832515 \tAccuracy =  3.3203125 %\n",
      "Iteration  21700 \tLoss =  22.515462899476205 \tAccuracy =  2.734375 %\n",
      "Epoch =  141  Average Loss =  22.91973576556713  New Learning Rate =  0.01\n",
      "Iteration  21800 \tLoss =  22.77315121911805 \tAccuracy =  2.9296875 %\n",
      "Epoch =  142  Average Loss =  22.91450191769277  New Learning Rate =  0.01\n",
      "Iteration  21900 \tLoss =  23.879159855848588 \tAccuracy =  2.734375 %\n",
      "Iteration  22000 \tLoss =  22.000481383273545 \tAccuracy =  4.296875 %\n",
      "Epoch =  143  Average Loss =  22.90930774292844  New Learning Rate =  0.01\n",
      "Iteration  22100 \tLoss =  24.483504200123235 \tAccuracy =  1.953125 %\n",
      "Epoch =  144  Average Loss =  22.904153074136026  New Learning Rate =  0.01\n",
      "Iteration  22200 \tLoss =  21.96796222629455 \tAccuracy =  1.171875 %\n",
      "Iteration  22300 \tLoss =  22.84390464724636 \tAccuracy =  1.953125 %\n",
      "Epoch =  145  Average Loss =  22.89903765658215  New Learning Rate =  0.01\n",
      "Iteration  22400 \tLoss =  24.227090075240866 \tAccuracy =  4.1015625 %\n",
      "Epoch =  146  Average Loss =  22.893961133735488  New Learning Rate =  0.01\n",
      "Iteration  22500 \tLoss =  21.606126372681818 \tAccuracy =  2.34375 %\n",
      "Iteration  22600 \tLoss =  24.142944479853423 \tAccuracy =  1.5625 %\n",
      "Epoch =  147  Average Loss =  22.888923043827525  New Learning Rate =  0.01\n",
      "Iteration  22700 \tLoss =  22.63105563468297 \tAccuracy =  1.5625 %\n",
      "Epoch =  148  Average Loss =  22.883922825784307  New Learning Rate =  0.01\n",
      "Iteration  22800 \tLoss =  22.434956720886447 \tAccuracy =  3.90625 %\n",
      "Iteration  22900 \tLoss =  21.636150786901887 \tAccuracy =  3.7109375 %\n",
      "Epoch =  149  Average Loss =  22.878959831403527  New Learning Rate =  0.01\n",
      "Iteration  23000 \tLoss =  22.940265836472562 \tAccuracy =  2.34375 %\n",
      "Iteration  23100 \tLoss =  21.930882306373274 \tAccuracy =  1.953125 %\n",
      "Epoch =  150  Average Loss =  22.874033339323287  New Learning Rate =  0.01\n",
      "Iteration  23200 \tLoss =  22.36213045333804 \tAccuracy =  3.515625 %\n",
      "Epoch =  151  Average Loss =  22.86914256602344  New Learning Rate =  0.01\n",
      "Iteration  23300 \tLoss =  21.337245997449887 \tAccuracy =  3.3203125 %\n",
      "Iteration  23400 \tLoss =  21.97385828084154 \tAccuracy =  1.7578125 %\n",
      "Epoch =  152  Average Loss =  22.86428667006416  New Learning Rate =  0.01\n",
      "Iteration  23500 \tLoss =  23.51011827475783 \tAccuracy =  2.734375 %\n",
      "Epoch =  153  Average Loss =  22.859464747720263  New Learning Rate =  0.01\n",
      "Iteration  23600 \tLoss =  22.74735077856348 \tAccuracy =  2.734375 %\n",
      "Iteration  23700 \tLoss =  22.45409859401034 \tAccuracy =  2.1484375 %\n",
      "Epoch =  154  Average Loss =  22.85467582047325  New Learning Rate =  0.01\n",
      "Iteration  23800 \tLoss =  22.871845422373283 \tAccuracy =  2.5390625 %\n",
      "Epoch =  155  Average Loss =  22.84991881679942  New Learning Rate =  0.01\n",
      "Iteration  23900 \tLoss =  21.961357993099725 \tAccuracy =  2.34375 %\n",
      "Iteration  24000 \tLoss =  23.53033629380682 \tAccuracy =  2.9296875 %\n",
      "Epoch =  156  Average Loss =  22.84519255190493  New Learning Rate =  0.01\n",
      "Iteration  24100 \tLoss =  22.3071694553994 \tAccuracy =  2.5390625 %\n",
      "Epoch =  157  Average Loss =  22.840495709390556  New Learning Rate =  0.01\n",
      "Iteration  24200 \tLoss =  23.68463270374548 \tAccuracy =  1.7578125 %\n",
      "Iteration  24300 \tLoss =  23.8583597728473 \tAccuracy =  2.5390625 %\n",
      "Epoch =  158  Average Loss =  22.83582682843565  New Learning Rate =  0.01\n",
      "Iteration  24400 \tLoss =  22.150804498906066 \tAccuracy =  3.3203125 %\n",
      "Epoch =  159  Average Loss =  22.831184299247905  New Learning Rate =  0.01\n",
      "Iteration  24500 \tLoss =  22.57445582351795 \tAccuracy =  2.5390625 %\n",
      "Iteration  24600 \tLoss =  21.125119177920396 \tAccuracy =  3.3203125 %\n",
      "Epoch =  160  Average Loss =  22.826566368492845  New Learning Rate =  0.01\n",
      "Iteration  24700 \tLoss =  23.577988568174117 \tAccuracy =  3.3203125 %\n",
      "Epoch =  161  Average Loss =  22.82197115536167  New Learning Rate =  0.01\n",
      "Iteration  24800 \tLoss =  22.034234827791497 \tAccuracy =  3.515625 %\n",
      "Iteration  24900 \tLoss =  21.12494962946971 \tAccuracy =  2.34375 %\n",
      "Epoch =  162  Average Loss =  22.817396677918634  New Learning Rate =  0.01\n"
     ]
    }
   ],
   "source": [
    "X = inpX.copy()\n",
    "\n",
    "input_size = X.shape[1]\n",
    "output_size = int(np.amax(Y))+1\n",
    "num_examples = X.shape[0]\n",
    "max_iterations = int(40*(num_examples/batch_size))\n",
    "if(max_iterations < 25000):\n",
    "    max_iterations = 25000\n",
    "network = NeuralNetwork(input_size,output_size,hidden_layers_sizes,activation)\n",
    "network.train(X,Y.astype(int),batch_size,n0,max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraccy on Training Data =  82.48337595907928\n"
     ]
    }
   ],
   "source": [
    "predictions = network.predict(X.copy())\n",
    "print(\"Accuraccy on Training Data = \",100 * np.sum(predictions == Y)/Y.shape[0])\n",
    "# print(\"Average of predictions on Training Data = \",np.average(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = load_data(testing_data_path,avg,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = network.predict(testX)\n",
    "np.savetxt(output_path,predictions,fmt=\"%i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
