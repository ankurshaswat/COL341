{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from math import log\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# training_data_path = sys.argv[1]\n",
    "# testing_data_path = sys.argv[2]\n",
    "# output_path = sys.argv[3]\n",
    "\n",
    "training_data_path = \"../data/amazon_train.csv\"\n",
    "testing_data_path = \"../data/amazon_test_public.csv\"\n",
    "output_path = \"../data/cs1160328.txt\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_words(sentence):\n",
    "    words = re.split(';|,|\\*|\\n| ',sentence)\n",
    "#     words = sentence.split(' ')\n",
    "    num = len(words)\n",
    "    for i in range(num-1):\n",
    "        words.append(words[i]+' '+words[i+1])\n",
    "    return words\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    return word_tokens\n",
    "#     words = [w for w in word_tokens if not w in stop_words]\n",
    "#     result = []\n",
    "#     for word in words:\n",
    "#         result.append(ps.stem(word))\n",
    "#     return result\n",
    "\n",
    "def load_train_data(path):\n",
    "    total_reviews = 0\n",
    "    num_reviews = [0,0,0,0,0]\n",
    "    dictionary = {1:{},2:{},3:{},4:{},5:{}}\n",
    "    vocab = {}\n",
    "    total_words_in_vocab = 0\n",
    "    with open(path, 'r') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in spamreader:\n",
    "            total_reviews += 1\n",
    "            rating = int(float(row[0]))\n",
    "            num_reviews[rating - 1] += 1\n",
    "            words = get_list_of_words(row[1])\n",
    "            dict = {}\n",
    "            for word in words:\n",
    "                if word not in dict:\n",
    "                    dict[word] = 1\n",
    "                    if word not in dictionary[rating]:\n",
    "                        dictionary[rating][word] = 1\n",
    "                        if word not in vocab:\n",
    "                            vocab[word] = 1\n",
    "                            total_words_in_vocab += 1\n",
    "                    else:\n",
    "                        dictionary[rating][word] += 1\n",
    "    return num_reviews,total_reviews,dictionary,total_words_in_vocab\n",
    "\n",
    "def load_test_data(path):\n",
    "    data = []\n",
    "    with open(path, 'r') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in spamreader:\n",
    "            data.append(get_list_of_words(row[1]))\n",
    "    return data\n",
    "\n",
    "def predict(testing_examples,dictionary,num_reviews,total,vocab_len):\n",
    "    ratings = []\n",
    "    for list_of_words in testing_examples:\n",
    "        probabs = []\n",
    "        for rating in range(1,6):\n",
    "            probab = log(num_reviews[rating-1]/total)\n",
    "            for word in list_of_words:\n",
    "                if word in dictionary[rating]:\n",
    "                    probab += log((dictionary[rating][word] + 1)/ (num_reviews[rating-1] + vocab_len + 1))\n",
    "                else:\n",
    "                    probab += log(1/ (num_reviews[rating-1] + vocab_len + 1))\n",
    "            probabs.append(probab)\n",
    "        best_rating  = probabs.index(max(probabs))+1\n",
    "        ratings.append(best_rating)\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reviews, tot, dictionary,vocab_len = load_train_data(training_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0, 2: 0, 3: 0, 4: 1, 5: 16175}\n"
     ]
    }
   ],
   "source": [
    "test_data = load_test_data(testing_data_path)\n",
    "predictions = predict(test_data,dictionary,num_reviews,tot,vocab_len)\n",
    "x = {1:0,2:0,3:0,4:0,5:0}\n",
    "for pr in predictions:\n",
    "    x[pr] += 1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path, 'w') as f:\n",
    "    for item in predictions:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
