{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "import scipy.sparse as sp\n",
    "# method = sys.argv[1]\n",
    "# learning_rate = int(sys.argv[2])\n",
    "# iterations = int(sys.argv[3])\n",
    "# batch_size = int(sys.argv[4])\n",
    "# training_data_path = sys.argv[5]\n",
    "# vocabulary_path = sys.argv[6]\n",
    "# testing_data_path = sys.argv[7]\n",
    "# output_path = sys.argv[8]\n",
    "\n",
    "method = 1\n",
    "learning_rate = 0.1\n",
    "iterations = 100\n",
    "batch_size = 128\n",
    "training_data_path = '../data/imdb_train.csv'\n",
    "vocabulary_path = '../data/imdb_vocab'\n",
    "testing_data_path = '../data/imdb_test.csv'\n",
    "output_path = 'out/imdb_output_a.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(data_path,vocab_map,num_features):\n",
    "    Y = []\n",
    "    S = []\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    with open(data_path, 'r') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',')\n",
    "        spamreader = list(spamreader)\n",
    "        n = len(spamreader)\n",
    "        S = sp.dok_matrix((n,num_features), dtype=np.int8)\n",
    "        #X = np.zeros((n,num_features))\n",
    "        Y = np.zeros((n,1))\n",
    "        i = 0\n",
    "        for row in spamreader:\n",
    "            cleaned_text = regex.sub('',row[1])\n",
    "            words = cleaned_text.split(' ')\n",
    "            S[i,0] = 1\n",
    "            for word in words:\n",
    "                if word in vocab_map:\n",
    "                    S[i,vocab_map[word]] += 1\n",
    "            Y[i] = int(row[0])\n",
    "            i += 1\n",
    "    return S.tocsr(),Y\n",
    "\n",
    "def load_data(data_path,vocab_map,num_features):\n",
    "    X_sparse,Y = load(data_path,vocab_map,num_features)\n",
    "    return X_sparse,Y\n",
    "\n",
    "def load_vocab(data_path):\n",
    "    vocab_map = {}\n",
    "    with open(data_path, 'r') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',')        \n",
    "        i = 0\n",
    "        for row in spamreader:\n",
    "            if row[0] not in vocab_map:\n",
    "                vocab_map[row[0]] = i + 1\n",
    "                i += 1\n",
    "    return vocab_map, i+1\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1.0 / (1.0 + np.exp(t * -1))\n",
    "\n",
    "def get_log_likelihood(W,X,Y):\n",
    "    X_W = X.dot(W)\n",
    "    Predictions = sigmoid(X_W)\n",
    "    return np.sum(Y.T.dot(np.log(Predictions)) + (1-Y.T).dot(np.log(1-Predictions)))/Y.shape[0]\n",
    "\n",
    "def get_accuracy(W,X,Y):\n",
    "    X_W = X.dot(W)\n",
    "    Predictions = np.rint(sigmoid(X_W))\n",
    "    return np.sum(Predictions == Y)/Y.shape[0]\n",
    "\n",
    "\n",
    "def get_optimal_learning_rate(W,X,Y):\n",
    "    return 0.01\n",
    "\n",
    "def train_model(X,Y,iterations,learning_rate,lam,learning_rate_mode = 0):\n",
    "    m = X.shape[1]\n",
    "    n= X.shape[0]\n",
    "    W = np.zeros((m,1))\n",
    "    XT = X.transpose()\n",
    "    for i in range(iterations):\n",
    "        g_val = sigmoid(X.dot(W))\n",
    "#         print(get_accuracy(W,X,Y))\n",
    "        if (learning_rate_mode == 0):\n",
    "            W = W + (XT.dot(Y-g_val) - W.dot(lam)).dot(learning_rate).dot(1/n)\n",
    "        elif (learning_rate_mode == 1):\n",
    "            W = W + (XT.dot(Y-g_val) - W.dot(lam)).dot(learning_rate / np.sqrt(i+1)).dot(1/n)\n",
    "        else:\n",
    "            lr = get_optimal_learning_rate(W,X,Y)\n",
    "            W = W + (XT.dot(Y-g_val) - W.dot(lam)).dot(lr).dot(1/n)\n",
    "    return W\n",
    "\n",
    "def kFold_cross_validation(X,Y,lambdas,folds,iterations,learning_rate,learning_rate_mode):\n",
    "    \n",
    "    fold_size = int(X.shape[0]/folds)\n",
    "    \n",
    "    sums = []\n",
    "    for lam in lambdas:\n",
    "        sums.append(0.0)\n",
    "        \n",
    "    for i in range(folds):\n",
    "        X_test = X[i*fold_size:(i+1)*fold_size]\n",
    "        X_train = sp.vstack((X[:i*fold_size],X[(i+1)*fold_size:]))\n",
    "        Y_test = Y[i*fold_size:(i+1)*fold_size]\n",
    "        Y_train = np.vstack((Y[:i*fold_size],Y[(i+1)*fold_size:]))\n",
    "        for i in range(len(lambdas)):\n",
    "            W= train_model(X_train,Y_train,iterations,learning_rate,lambdas[i],learning_rate_mode)\n",
    "#             sums[i] += get_log_likelihood(W,X_test,Y_test)\n",
    "            sums[i] += get_accuracy(W,X_test,Y_test)\n",
    "        print(sums)\n",
    "    for i in range(0,len(sums)):\n",
    "        sums[i] /= folds\n",
    "    return sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map, m = load_vocab(vocabulary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vocab_map)\n",
    "# print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = load_data(testing_data_path,vocab_map,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train_sparse)\n",
    "#print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 89528)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7228, 0.7228, 0.7228, 0.7228, 0.7224, 0.7224, 0.7228, 0.722]\n",
      "[1.4716, 1.4716, 1.4716, 1.4716, 1.4712, 1.4712, 1.4712, 1.4704]\n",
      "[2.2044, 2.2044, 2.2044, 2.2044, 2.204, 2.204, 2.2036000000000002, 2.2032]\n",
      "[2.9468, 2.9468, 2.9468, 2.9468, 2.9464, 2.9464, 2.9464, 2.9452]\n",
      "[3.6932, 3.6932, 3.6932, 3.6932, 3.6928, 3.6928, 3.6928, 3.69]\n",
      "[4.4315999999999995, 4.4315999999999995, 4.4315999999999995, 4.4315999999999995, 4.4312000000000005, 4.4312000000000005, 4.430400000000001, 4.4276]\n",
      "[5.17, 5.17, 5.17, 5.17, 5.169600000000001, 5.170000000000001, 5.1684, 5.1644]\n",
      "[5.9064, 5.9064, 5.9064, 5.9064, 5.906000000000001, 5.9064000000000005, 5.9048, 5.8999999999999995]\n",
      "[6.652799999999999, 6.652799999999999, 6.652799999999999, 6.652799999999999, 6.6524, 6.652800000000001, 6.6508, 6.6464]\n",
      "[7.381599999999999, 7.381599999999999, 7.381599999999999, 7.381999999999999, 7.3816, 7.382000000000001, 7.38, 7.3748]\n"
     ]
    }
   ],
   "source": [
    "# W = train_model(X,Y,iterations,learning_rate,0.01)\n",
    "lambdas = [0,1,5,10,50,100,500,1000]\n",
    "folds = 10\n",
    "# W = train_model(X,Y,iterations,learning_rate,0.01,1)\n",
    "accuracy = kFold_cross_validation(X,Y,lambdas,10,iterations,learning_rate,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "max = accuracy[0]\n",
    "lam = lambdas[0]\n",
    "\n",
    "for i in range (0,len(lambdas)):\n",
    "    if (errors[i] > max):\n",
    "        lam = lambdas[i]\n",
    "        max = accuracy[i]\n",
    "        \n",
    "print(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
